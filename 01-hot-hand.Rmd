# How to Outsmart a Nobel Laureate 

## The Hot Hand

If you've read 2002 Nobel Laureate Daniel Kahneman's best-selling book [*Thinking Fast and Slow*](https://www.google.co.uk/books/edition/Thinking_Fast_and_Slow/AV9x8XakdV0C?hl=en&gbpv=1&dq=thinking+fast+and+slow&printsec=frontcover), you may remember this passage about the *hot hand illusion*, a supposed illustration of the human tendency to see patterns in random noise:

> Amos [Tversky] and his students Tom Gilovich and Robert Vallone caused a stir with their study of misperceptions of randomness in basketball. The "fact" that players occasionally acquire a hot hand is generally accepted by players, coaches, and fans. The inference is irresistible: a player sinks three or four baskets in a row and you cannot help forming the causal judgment that this player is now hot, with a temporarily increased propensity to score. Players on both teams adapt to this judgmentâ€”teammates are more likely to pass to the hot scorer and the defense is more likely to doubleteam. Analysis of thousands of sequences of shots led to a disappointing conclusion: there is no such thing as a hot hand in professional basketball, either in shooting from the field or scoring from the foul line. Of course, some players are more accurate than others, but the sequence of successes and missed shots satisfies all tests of randomness. The hot hand is entirely in the eye of the beholders, who are consistently too quick to perceive order and causality in randomness. The hot hand is a massive and widespread cognitive illusion.

The research that Kahneman mentions was published in a famous paper by [Gilovich, Vallone & Tversky (1985)](https://labs.la.utexas.edu/gilden/files/2016/04/Gilo.Vallone.Tversky.pdf), and later summarized for a general audience in [Gilovich & Tversky (1989)](http://www.medicine.mcgill.ca/epidemiology/hanley/c323/hothand.pdf). The abstract of the original paper says it all:

> Basketball players and fans alike tend to believe that a player's chance of hitting a shot are greater following a hit than following a miss on the previous shot. However, detailed analyses of the shooting records of the Philadelphia 76ers provided no evidence for a positive correlation between the outcomes of successive shots. The same conclusions emerged from free-throw records of the Boston Celtics, and from a controlled shooting experiment with the men and women of Cornell's varsity teams. The outcomes of previous shots influenced Cornell players' predictions but not their performance. The belief in the hot hand and the "detection" of streaks in random sequences is attributed to a general misconception of chance according to which even short random sequences are thought to be highly representative of their generating process.

Between 1985 and 2011, when Kahneman's book was published, this result was replicated numerous times under a variety of different conditions, making it one of the better-documented biases in human decision-making. But it turns out that the hot-hand illusion is *itself* an illusion.  

In a recent issue of *Econometrica*, [Miller \& Sanjurjo (2018)](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA14943) point out a subtle but consequential error in the statistical tests used in the hot hand literature. It turns out that these tests were biased *against* detecting evidence of a hot hand, even if it did in fact exist. Correcting this mistake and re-analyzing Gilovich, Vallone and Tversky's original dataset "reveals significant evidence of streak shooting, with large effect sizes." The hot hand is real.

There are some helpful lessons that we can draw from this episode. First, we all make mistakes--even Nobel Laureates! Second, successful replications tell us less than we might hope: they could easily *reproduce* the bias of the original study. But in my view, the real lesson of the hot hand affair is much simpler: **you should always run a simulation study.** 

The probabilistic error that led so many researchers to draw the wrong conclusion about the hot hand is really quite subtle.^[See [Miller \& Sanjurjo (2019)](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.33.3.144) for a more accessible explanation that connects to several related probability puzzles.] But at the same time, anyone who knows basic programming could have detected the mistake in five minutes if only they had bothered to look. In economics and statistics, simulation is a *superpower*. It helps us to understand our models, check for mistakes, and make unexpected connections, some of which may even lead to new theoretical results. If you'll pardon my continuation of the Swiss Army Knife metaphor, simulation is the knife: arguably the most useful tool in your toolbox. In this lesson, we'll cover some basic tools for carrying out a simulation experiment in R and use them to shed some light on the illusion of the hot hand illusion.



## Update from here down!

**Should I introduce `knitr` and `Rmarkdown` in this lesson or the next one?**

1. Some basic simulation commands in R: `sample()`, `rbinom()`, `rnorm()`, etc. Read the help files.
2. Write a function `draw_sim_data()` that makes 100 Bernoulli(1/2) draws. Optional arguments `p` and `n`? This simulates data when there is *no hot hand*
3. `set.seed()` what does it do?
4. Think about how to calculate the estimator: fraction of times that three ones are followed by another one compared to another zero. Suppose you had a function `is_after_3_ones()` that took a vector of 0 and 1 and returned, for each element, whether it is after three ones. How would you use it? Write this function.
5. Put everything together with `replicate()` to do a simple sim for $p=1/2$ and $n = 100$.
6. How about trying different values of $n$ and $p$? Need to keep results organized: `apply()` family of functions (or maybe the tidy equivalents?)
7. Try doing it in parallel with `mclapply()`. First explain the basic idea of parallel and why this is "embarrassingly parallel." Show them how to time the code, illustrate with `sys.sleep()`. 
8. For the students who finish very quickly, have some extensions: a markov chain DGP, and `is_after_k_ones()`

```{r, cache=TRUE}
dgp <- function(n = 100, p = 0.5) {
  rbinom(n, 1, p)
}

# Maybe have a challenge to write the version for after k ones, but start by
# asking them to do the one for after 3 ones
is_after_k_ones <- function(x, k) {
  out <- rep(NA)
  for(i in (k+1):length(x)) {
    out[i] <- sum(x[(i - k):(i - 1)]) == k
  }
  return(out)
}

get_est <- function(x) {
  #ones <- which(x == 1)
  #mean(x[ones + 1], na.rm = TRUE)
  mean(x[is_after_k_ones(x, 3)], na.rm = TRUE)
}

n_reps <- 1e5
set.seed(1234)
sim_results <- replicate(n_reps, get_est(dgp()))
library(ggplot2)
qplot(sim_results)
mean(sim_results, na.rm = TRUE)

#ones <- which(x == 1)
#x[which(x == 1) + 1]
#runs <- rle(x)
#foo <- rle(x)
#str(foo)
#x
#foo$lengths
#foo$values
```


## Hidden solutions and hints

You can fence off a solution area that will be hidden behind a button using `hide()` before the solution and `unhide()` after, each as inline R code.  Pass the text you want to appear on the button to the `hide()` function.

If the solution is an RMarkdown code chunk, instead of using `hide()` and `unhide()`, simply set the `webex.hide` chunk option to TRUE, or set it to the string you wish to display on the button.

### Example problem

**Recreate the scatterplot below, using the built-in `cars` dataset.**

```{r echo = FALSE}
with(cars, plot(speed, dist))
```


`r hide("I need a hint")`

See the documentation for `plot()` (`?plot`)

`r unhide()`


<!-- note: you could also just set webex.hide to TRUE -->

```{r eval = FALSE, webex.hide="Click here to see the solution"}
plot(cars$speed, cars$dist)
```

<!-- TO CHANGE WIDGET COLOURS:
  move command below out of this HTML comment area
  and then re-compile;
  unfilled becomes yellow, correct becomes pink 
     
`r style_widgets("#FFFF00", "#FF3399")`
-->
