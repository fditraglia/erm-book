[["index.html", "Empirical Research Methods Preface About This Book Pre-requisites Good Coding Style R Markdown Why not Stata? Why not Matlab, Julia, or Python? You can help make this book better!", " Empirical Research Methods Francis J. DiTraglia 2022-04-25 Preface About This Book Supervising undergraduate, masters-level, and doctoral research students has shown me just how many of the skills that I take for granted in my day-to-day work were never taught in a course, but acquired through years of painful trial-and-error. You've probably heard that \"the only way to learn how to do research is by doing research.\" Indeed: classroom exercises are always somewhat artificial, and there is no substitute for getting your hands dirty with a problem that really matters to you. But trial-and-error is a slow and clumsy way to gain proficiency, and throwing students in at the deep end is neither a recipe for academic success nor for mental well-being. The goal of this book is to put some structure around the process through which students learn to do empirical work in economics, building a strong foundation for later self-directed reading and research. The book is divided into a number a number of lessons, each designed to take between between 90 minutes and 3 hours to complete. Broadly speaking, the material is a mix of data science, applied econometrics, and research skills. In keeping with the Swiss Army Knife logo, the idea is to teach you lots of little things that will come in handy later. While the topics covered below are something of a miscellany, there are strong connections between the lessons. For best results, complete them in order. A key theme that runs throughout the lessons is the importance of reproducible research using open-source tools. Reproducible research is about creating a clean and fully-documented path from raw data to final results, making errors less likely to occur and easier to find when they do. It also allows other researchers, or our future selves, to build on past work, expanding the sum total of knowledge. Of course I can only replicate your research if I can run your code, and this is why open-source software is so important. Fortunately there are many fantastic open-source programming languages to choose from. This book uses R, the lingua franca of statistics and an increasingly popular choice among economists. Pre-requisites This book does not assume advanced knowledge of programming, mathematics, or econometrics, but it does have some pre-requisites. My target audience is first-year graduate students and final-year undergraduates in economics. At Oxford, I use this book to teach a first-year master's level course on Empirical Research Methods that comes after students have completed 16 weeks of basic statistics and econometrics. I assume that you've taken an econometrics course that uses matrix notation and that you have basic familiarity with R programming. If you need to brush up on econometrics, I recommend Marno Verbeek's Guide to Modern Econometrics. I've linked to the third edition because it is particularly inexpensive to buy a used copy, but any edition will do. At a more advanced level, Bruce Hansen's two volume series Econometrics is both excellent and free to download online. If you haven't used R before or feel the need for a bit of review, I suggest reading Hands-On Programming with R. It's free, short, and will get you up to speed quickly. Good Coding Style I aim to follow the tidyverse style guide throughout this book and you should too. In theory, any coding style that is clear and consistent is absolutely fine. In practice, it's almost impossible for a novice programmer to figure out what \"clear and consistent\" should mean. Pick a standard, learn it, and after you're more experienced you can always make a conscious decision to deviate from it if you so choose. R Markdown This book is written entirely in R Markdown, a simple markup language that allows you to combine R code and results with formatted text, LaTeX mathematical expressions and more. Writing documents in R Markdown allows you to output your results in a wide variety of formats, including web pages, pdfs, slides, and even Microsoft Word if you absolutely must. R Markdown is a key ingredient in reproducible research. For this reason, I suggest that you use it to take notes as you work through the lessons that follow. This book will not teach you R Markdown directly, but there are many helpful resources online that will. I suggest starting here. Another way to learn about R Markdown is by viewing the underlying source used to generate this book! You can do so on github. Files in the main directory that end in .Rmd are the ones you want. What you're reading right now is called index.Rmd, and the first lesson is 01-hot-hand.Rmd, for example. Why not Stata? Given that much of the material discussed below falls under the broad category of \"applied microeconometrics\" you may wonder why I chose R rather than Stata. Indeed, Stata is easy-to-use, and makes it relatively painless to implement \"textbook\" microeconometric methods.1 So why don't I like Stata? Before beginning my polemic I should be absolutely clear that Stata users are not bad people: hate the sin, love the sinner. Here begins the sermon. First, Stata is expensive. The price for a Business single-user Stata license is $765 per year.2 If you want support for multicore computing, the price is even higher: an 8-core version of Stata costs $1,395 annually. There is no discount for Government or nonprofits, but as an Oxford faculty member, I can obtain an 8-core version of Stata for the low price of $595 per year, or around 9% of my annual research allowance. In contrast, the tools that we will learn in this book, mainly R and C++, are completely free. This is particularly important in the modern world of high-performance cluster computing. If you're considering running your code on a multicore machine on Amazon, Google, or Microsoft cloud servers, you don't want to pay a software license fee for every core that you use. Second, Stata is almost comically behind the times. Let's see what's new in Stata version 16, released in February 2020.3 At the top of the list is the LASSO, a wildly popular technique for high-dimensional regression. Rob Tibshirani developed this method in a seminal paper from 1996, so it only took 24 years for it to be incorporated into Stata.4 Fortunately, Tibshirani and his co-authors made it easy for Stata, by releasing open-source software to implement the LASSO and related methods in R over a decade ago.5 Next on the list of new Stata features is linear programming, a technique that came to prominence in the late 1940s.6 Stata 16 also has the ability to call \"any Python package\"--something you can do for free in R using reticulate or in Python itself for that matter--and \"truly reproducible reporting.\" Reproducible reporting is incredibly valuable, and it's something that we'll cover in detail below. It's also been available in R, completely free of charge, since at least 2002.7 I suppose we shouldn't expect too much of a statistical computing package that only added support for matrix programming in 2005, a full 20 years after Stata version 1.0.8 Third, Stata is a black box. Because the underlying source code is kept secret, there's no way for a Stata user to know for certain what's happening under the hood. A few years ago I tried to determine precisely what instrument set Stata was using in its implementation of a well-known dynamic panel estimator. The documentation was vague, so I resorted to reverse-engineering the Stata results by trial-and-error in R. I never did get the results to match perfectly. In contrast, if you're not sure what a particular R function or package is doing, you can simply read the source code and find out. Fourth, and most importantly, Stata makes it hard to share with others. If I don't own a copy of Stata, I can't replicate your work. Even if I do own a copy of Stata, I still may not be able to so do: Stata's proprietary binary data formats are updated fairly regularly and do not maintain backwards compatibility. Datafiles created in Stata version 16, for example, cannot be opened in Stata 13. Indeed, depending on the number of variables included in your dataset, Stata 16 files cannot necessarily be opened even in Stata 15. Fortunately, as we'll see below, intrepid open-source programmers have developed free software to unlock data from Stata's proprietary and ever-changing binary formats. Why not Matlab, Julia, or Python? Unlike Stata, Matlab is a bona fide programming language and a fairly capable one at that. Nevertheless, my other critiques of Stata from above still apply: Matlab is extremely expensive, and it's not open source. In contrast, I have nothing bad to say about Python and Julia: they're great languages and you should consider learning one or both of them!9 In the end I decided to choose one language and R struck me as the best choice at this moment in time for the particular set of topics I wanted to cover. In five or ten years time who knows: I may re-write this book in Julia! But for the moment, R has a decided advantage in terms of maturity, a huge collection of useful packages, and a large and extremely supportive user community. You can help make this book better! I'm writing this book \"in the open\" for two reasons. First, I want to make it easy for others to borrow and adapt my material if they find it useful. Second, I want you to help me make this book better! Do you have comments or suggestions? Have you spotted typos or other errors? Is there a topic that I don't cover but you think I should? If so then please send me feedback! The best way to point out a typo or suggest a correction is by sending me a pull request. For other comments, you can either email me by removing any references to giant extinct lizards from erm@TYRRANOSAURUSuser.sent.VELOCIRAPTORas or contact me on twitter @economictricks. Now let's get started! Arguably, Stata is too easy to learn precisely because of the incentives faced by a software developer with monopoly power: see Hal Varian's paper: Economic Incentives in Software Design.↩︎ These figures were accurate as of March 2021. For the latest prices, see https://www.stata.com/order/dl/.↩︎ https://www.stata.com/new-in-stata/↩︎ Tibshirani (1996) - Regression Shrinkage and Selection via the Lasso↩︎ Friedman et al (2010) - Regularization Paths for Generalized Linear Models via Coordinate Descent↩︎ For a history of linear programming, see Dantzig (1983). To be completely fair, the linear programming algorithm implemented in Stata 16 was only developed in 1992, a lag of merely 28 years.↩︎ Reproducible reporting in R started with sweave. These days we have a fantastic successor package called knitr, which I cover below.↩︎ The \"Mata\" programming language was added in Stata 9: https://www.stata.com/stata9/. For a timeline of Stata versions, see https://www.stata.com/support/faqs/resources/history-of-stata/.↩︎ A good resources aimed at economists is https://quantecon.org, available in both Python and Julia versions.↩︎ "],["running-a-simulation-study.html", "Lesson 1 Running a Simulation Study 1.1 Is the Hot Hand an Illusion? 1.2 Drawing Random Data in R 1.3 The Skeleton of a Simulation Study 1.4 Exercise - The Hot Hand", " Lesson 1 Running a Simulation Study It’s nearly impossible to overstate the value that economists ascribe to cleverness. Like most obsessions, this one is not altogether healthy. --David Autor My general philosophy in life is never to rely on being clever; instead I want to rely on being thorough and having a justifiable workflow. -- Richard McElreath In economics and statistics, simulation is a superpower. It helps us to understand our models, check for mistakes, and make unexpected connections. If you'll pardon my continuation of the Swiss Army Knife metaphor, simulation is the knife: arguably the most useful tool in your toolbox. Riffing on the quotes from above, simulation is both a substitute for and a complement to cleverness. In this lesson, we'll learn the basics of carrying out a simulation study in R using the following key commands: sample() rbinom() rnorm() set.seed() replicate() expand.grid() Map() We'll then apply what we've learned to shed some light on a simple but surprisingly subtle statistical error that led an entire literature astray for nearly two decades. Before we dive into the code, let me set the stage for the example that appears at the end of this lesson. 1.1 Is the Hot Hand an Illusion? If you've read 2002 Nobel Laureate Daniel Kahneman's best-selling book Thinking Fast and Slow, you may remember this passage about the hot hand illusion, a supposed illustration of the human tendency to see patterns in random noise: Amos [Tversky] and his students Tom Gilovich and Robert Vallone caused a stir with their study of misperceptions of randomness in basketball. The \"fact\" that players occasionally acquire a hot hand is generally accepted by players, coaches, and fans. The inference is irresistible: a player sinks three or four baskets in a row and you cannot help forming the causal judgment that this player is now hot, with a temporarily increased propensity to score. Players on both teams adapt to this judgment—teammates are more likely to pass to the hot scorer and the defense is more likely to doubleteam. Analysis of thousands of sequences of shots led to a disappointing conclusion: there is no such thing as a hot hand in professional basketball, either in shooting from the field or scoring from the foul line. Of course, some players are more accurate than others, but the sequence of successes and missed shots satisfies all tests of randomness. The hot hand is entirely in the eye of the beholders, who are consistently too quick to perceive order and causality in randomness. The hot hand is a massive and widespread cognitive illusion. The research that Kahneman mentions was published in a famous paper by Gilovich, Vallone &amp; Tversky (1985), and later summarized for a general audience in Gilovich &amp; Tversky (1989). The abstract of the original paper says it all: Basketball players and fans alike tend to believe that a player's chance of hitting a shot are greater following a hit than following a miss on the previous shot. However, detailed analyses of the shooting records of the Philadelphia 76ers provided no evidence for a positive correlation between the outcomes of successive shots. The same conclusions emerged from free-throw records of the Boston Celtics, and from a controlled shooting experiment with the men and women of Cornell's varsity teams. The outcomes of previous shots influenced Cornell players' predictions but not their performance. The belief in the hot hand and the \"detection\" of streaks in random sequences is attributed to a general misconception of chance according to which even short random sequences are thought to be highly representative of their generating process. Between 1985 and 2011, when Kahneman's book was published, this result was replicated numerous times under a variety of different conditions, making it one of the better-documented biases in human decision-making. But it turns out that another source of bias was at play here, one of the statistical variety. In a recent issue of Econometrica, Miller &amp; Sanjurjo (2018) point out a subtle but consequential error in the statistical tests used in the hot hand literature. It turns out that these tests were biased against detecting evidence of a hot hand, even if it did in fact exist. Correcting this mistake and re-analyzing Gilovich, Vallone and Tversky's original dataset \"reveals significant evidence of streak shooting, with large effect sizes.\" In response to this paper, the literature has now shifted to trying to estimate the size of the effect.10 As Benjamin (2019) points out it's still an open question whether sports fans over-estimate the importance of the hot hand phenomenon, but it's certainly not a cognitive illusion.11 There are some helpful lessons that we can draw from this episode. First, we all make mistakes--even Nobel Laureates! Second, successful replications tell us less than we might hope: they could easily reproduce the bias of the original study. But in my view, the real lesson of the hot hand affair is much simpler: you should always run a simulation study. The probabilistic error that led so many researchers to draw the wrong conclusion about the hot hand is really quite subtle.12 But at the same time, anyone who knows basic programming could have detected the mistake in half an hour if they had only bothered to look. 1.2 Drawing Random Data in R Before we can use simulations to study the illusion of the hot hand illusion, we need to review the basics of drawing random data in R. We'll examine the functions sample(), rbinom() and set.seed() in detail. I'll also point you to a large number functions for simulating from well-known probability distributions in R. Finally, you'll have a chance to practice what you've learned by solving a few short exercises. 1.2.1 sample() R has many helpful built-in functions for making simulated random draws. The simplest is sample(), which makes size random draws without replacement from a vector x. To test this out, I'll create a very simple vector my_vector &lt;- c(&#39;there&#39;, &#39;is&#39;, &#39;no&#39;, &#39;largest&#39;, &#39;integer&#39;) The following line of code makes two draws without replacement from my_vector sample(x = my_vector, size = 2) # without replacement ## [1] &quot;is&quot; &quot;no&quot; If I run the same line of code again, I may not get the same result: it's random!13 sample(x = my_vector, size = 2) # without replacement ## [1] &quot;is&quot; &quot;integer&quot; To draw with replacement, set replace = TRUE sample(x = my_vector, size = 7, replace = TRUE) # with replacement ## [1] &quot;there&quot; &quot;integer&quot; &quot;integer&quot; &quot;no&quot; &quot;there&quot; &quot;largest&quot; &quot;integer&quot; As usual in R, the argument names x, size, and replace are optional. But it is considered good coding style to explicitly supply an argument name whenever we're overriding a function's default behavior. This makes it easier for anyone reading our code to understand what's happening. Since sample() defaults to making draws without replacement, it's a good idea to write replace = TRUE rather then simply TRUE. But even without writing replace =, the code will still work as long as we supply all of the arguments in the correct order: sample(my_vector, 7, TRUE) # bad style ## [1] &quot;is&quot; &quot;is&quot; &quot;no&quot; &quot;no&quot; &quot;integer&quot; &quot;no&quot; &quot;is&quot; sample(my_vector, 7, replace = TRUE) # good style ## [1] &quot;integer&quot; &quot;no&quot; &quot;integer&quot; &quot;is&quot; &quot;is&quot; &quot;is&quot; &quot;there&quot; 1.2.2 Probability Distributions in R As a programming language targeted at statistical applications, R supplies built-in functions for all of the most common probability distributions.14 These functions follow a consistent naming convention. They being with either d, p, q, or r and are followed by an abbreviated name for a particular probability distribution. The prefix d denotes a density function (or mass function for a discrete distribution); p denotes a cumulative distribution function (CDF), q denotes a quantile function, and r denotes a function for making random draws from a particular distribution. For example: dunif() gives the probability density function of a uniform random variable, pnorm() gives the CDF of a normal random variable, qchisq() gives the quantile function of a Chi-squared, and rbinom allows us to make random draws from a Binomial distribution. The following table gives a full list of the relevant commands. R commands Distribution d/p/q/rbeta Beta d/p/q/rbinom Binomial d/p/q/rcauchy Cauchy d/p/q/rchisq Chi-Squared d/p/q/rexp Exponential d/p/q/rf F d/p/q/rgamma Gamma d/p/q/rgeom Geometric d/q/p/rhyper Hypergeometric d/p/q/rlogis Logistic d/p/q/rlnorm Log Normal d/p/q/rnbinom Negative Binomial d/p/q/rnorm Normal d/p/q/rpois Poisson d/p/q/rt Student's t d/p/q/runif Uniform d/p/q/rweibull Weibull There's a single help file for all of the d/p/q/r functions for a particular distribution. For example, if you enter ?dbeta at the console you'll be shown the help files for dbeta(), pbeta(), qbeta(), and rbeta(). To get a feel for how these functions work, let's take a look at rbinom(), the function for drawing from a Binomial distribution. Recall that a Binomial\\((m,p)\\) random variable equals the number of heads in \\(m\\) independent tosses of a coin with \\(\\mathbb{P}(\\text{Heads})=p\\). Or to use a bit of probability jargon, it equals the number of successes in \\(m\\) independent Bernoulli trials, each with probability of success \\(p\\).15 If \\(X\\) is a Binomial random variable with parameters \\(m\\) and \\(p\\), traditionally written as \\(X \\sim \\text{Binomial}(m, p)\\) then \\(X\\) must take on a value in the set \\(\\{0, 1, 2, ..., m\\}\\) and the probability that it takes on a particular value \\(x\\) in this set is \\[ \\mathbb{P}(X = x) = \\binom{m}{x} p^x (1 - p)^{m-x} \\] The function rbinom() makes random draws with the probabilities given by this formula. Its takes three arguments: size is the number of trials, \\(m\\) in the formula, prob is the probability of success, \\(p\\) in the formula, and n is the desired number of Binomial draws. For example, we can make a single draw from a Binomial\\((m = 10, p =1 /2)\\) distribution as follows rbinom(n = 1, size = 10, prob = 0.5) ## [1] 4 and fifteen draws from the same distribution by changing n to 10 rbinom(n = 15, size = 10, prob = 0.5) ## [1] 4 5 7 3 5 2 6 4 9 4 6 3 5 3 6 It's important not to confuse n with size. The former tells R how many Binomial draws to make. The latter tells R the value of the parameter \\(m\\) of the Binomial\\((m, p)\\) distribution. Perhaps you remember that if \\(X \\sim \\text{Binomial}(m, p)\\) then \\(\\mathbb{E}[X] = mp\\) and \\(\\text{Var}= mp(1-p)\\). We can approximate these results numerically by simulating a large number of draws, say 5000, from a Binomial distribution: m &lt;- 20 p &lt;- 0.25 n_sims &lt;- 5000 sim_draws &lt;- rbinom(n_sims, m, p) and then comparing the theoretical value for \\(\\mathbb{E}(X)\\) to a simulation-based approximation: c(EV_Theoretical = m * p, EV_Simulation = mean(sim_draws)) ## EV_Theoretical EV_Simulation ## 5.0000 5.0098 and similarly for \\(\\text{Var}(X)\\) c(Var_Theoretical = m * p * (1 - p), Var_Simulation = var(sim_draws)) ## Var_Theoretical Var_Simulation ## 3.750000 3.700044 Reassuringly, our simulation results are very close to the theoretical values. They would be even closer if we used a larger value for n_sims. 1.2.3 set.seed() A key theme of this book is the importance of reproducible research. Anyone else who wants to check your work should be able to obtain exactly the same results as you did by running your code. But this seems to be at odds with the idea of simulating random data. For example, if I run rbinom(10, 4, 0.6) repeatedly, I'll most likely get different results each time: rbinom(10, 4, 0.6) ## [1] 3 1 2 4 2 3 2 0 3 0 rbinom(10, 4, 0.6) ## [1] 2 3 2 1 1 4 3 1 1 3 rbinom(10, 4, 0.6) ## [1] 3 1 0 2 3 4 3 4 4 4 The function set.seed() allows us to ensure that we obtain the same simulation draws whenever we re-run the same simulation code. To use it, we simply choose a seed, any integer between negative and positive \\((2^{31} - 1)\\), and supply it as an argument to set.seed(). Simulation draws made on a computer aren't really random: they're only pseudo-random. This means that they \"look\" random and pass statistical tests for randomness but are in fact generated by a completely deterministic algorithm. Setting the seed sets the initial condition of the pseudorandom number generator. Because the algorithm is deterministic, the same initial condition always leads to the same results. This is what allows us to replicate our simulation draws. Each time we make another draw, the seed changes. But we can always return it to its previous state using set.seed(). For example, suppose I set the seed to 1. and re-run my code from above as follows set.seed(1) x1 &lt;- rbinom(10, 4, 0.6) x1 ## [1] 3 3 2 1 3 1 1 2 2 4 If I run rbinom(10, 4, 0.6) again, I will most likely not get the same result, because the state of the pseudorandom number generator has changed: x2 &lt;- rbinom(10, 4, 0.6) x2 ## [1] 3 3 2 3 2 2 2 0 3 2 identical(x1, x2) # safe/reliable way to test if two objects are exactly equal ## [1] FALSE but if I reset the seed to 1 I'll obtain exactly the same result as before: set.seed(1) x3 &lt;- rbinom(10, 4, 0.6) x3 ## [1] 3 3 2 1 3 1 1 2 2 4 identical(x3, x1) ## [1] TRUE Whenever you write simulation code, start by choosing a seed and adding the line set.seed(MY-SEED-GOES-HERE) to the top of your R script. You'll often see people use set.seed(12345) or set.seed(54321). When I'm not feeling lazy, I like to generate a truly random number to use as my seed. The website random.org provides free access to bona fide random numbers generated from atmospheric noise. The \"True Random Number Generator\" on the top right of their main page allows you to make uniform integer draws on a range from \"Min\" to \"Max.\" Using the widest possible range, \\(\\pm1\\times 10^9\\), I generated the seed 420508570 which I'll use in the following exercises. 1.2.4 Exercises Set your seed to 420508570 at the start of your solution code for each of these exercises to obtain results that match the solutions. Run sample(x = my_vector, size = 10). What happens and why? Show Solution R will throw an error. You can't make ten draws without replacement from a set of five objects: set.seed(420508570) sample(x = my_vector, size = 10) ## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39; Write a line of code that makes five draws without replacement from the set of numbers \\(\\{1, 2, 3, ..., 100\\}\\). Show Solution set.seed(420508570) sample(1:100, 5) ## [1] 100 67 51 44 12 Create a vector of thirty elements called urn that represents an urn containing ten blue balls and twenty red balls. Draw five balls with replacement from urn and store the draws in a vector called draws. Then write a line of code to count up the number of blue balls in draws. Show Hint Use rep() and c() to construct urn. Use == and sum() to count up the number of blue balls in draws. See the relevant help files for details, e.g. ?rep. Show Solution set.seed(420508570) urn &lt;- c(rep(&#39;blue&#39;, 10), rep(&#39;red&#39;, 20)) draws &lt;- sample(urn, 5, replace = TRUE) draws ## [1] &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; sum(draws == &#39;blue&#39;) ## [1] 2 Make 1000 draws from a normal distribution with mean 5 and variance 4 and store them in a vector called normal_sims. Calculate the mean and variance of your draws, and plot a histogram. Show Hint Consult the help file for rnorm() paying close attention to the fact that R specifies the normal distribution in terms of a mean and standard deviation rather than a mean and variance. You can plot a histogram with any number of functions: e.g. the base R function hist() or qplot() from the ggplot2 package. Show Solution set.seed(420508570) normal_sims &lt;- rnorm(1000, 5, 2) # Variance = 4; Standard Dev. = 2 mean(normal_sims) ## [1] 4.895722 var(normal_sims) ## [1] 3.912262 ggplot2::qplot(normal_sims, bins = 25) There is no built-in R function called rbern() for simulating draws from the Bernoulli Distribution with probability of success \\(p\\). Write one of your own and use it to make ten Bernoulli(0.8) draws. Your function rbern() should take two arguments: the number of draws n and the probability of success p. Show Hint There are various ways to do this. The simplest is by setting the arguments of rbinom() appropriately. Show Solution rbern &lt;- function(n, p) { # Make n random draws from a Bernoulli(p) distribution rbinom(n, size = 1, prob = p) } set.seed(420508570) rbern(10, 0.8) ## [1] 1 1 1 1 0 1 1 1 1 1 1.3 The Skeleton of a Simulation Study While the specific details will vary, nearly every simulation study has the same basic structure: Generate simulated data. Calculate an estimate from the simulated data. Repeat steps 1 and 2 many times, saving each of the estimates. Summarize the results. Thinking in terms of this structure helps us to write code that is easier to understand, easier to generalize, and faster to run. The key is to break these steps down into functions that carry out a single, well-defined task. Generally these will include: A function to generate simulated data. A function to calculate an estimate from the data. A function that repeatedly calls i. and ii. and summarizes the results. This may sound a bit abstract, so in the remainder of this section we'll walk through the details in a simple example: estimating the bias of the maximum likelihood estimator for the variance of a normal distribution. Along the way we'll explore three extremely helpful R functions for carrying simulation studies: replicate(), expand.grid(), and Map(). In the next section you'll apply what you've learned to the hot hand example. 1.3.1 A Biased Estimator of \\(\\sigma^2\\) My introductory statistics students often ask me why the sample variance, \\(S^2\\), divides by \\((n-1)\\) rather than the sample size \\(n\\): \\[ S^2 \\equiv \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 \\] The answer is that dividing by \\((n-1)\\) yields an unbiased estimator: if \\(X_1, ..., X_n\\) are a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then \\(\\mathbb{E}[S^2] = \\sigma^2\\). So what would happen if we divided by \\(n\\) instead? Consider the estimator \\(\\widehat{\\sigma}^2\\) defined by \\[ \\widehat{\\sigma}^2 \\equiv \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2. \\] If \\(X_i \\sim \\text{Normal}(\\mu, \\sigma^2)\\) then \\(\\widehat{\\sigma}^2\\) is in fact the maximum likelihood estimator for \\(\\sigma^2\\). With a bit of algebra, we can show that \\(\\mathbb{E}[\\widehat{\\sigma}^2] = (n-1)\\sigma^2/n\\) which clearly does not equal the population variance.16 It follows that \\[ \\text{Bias}(\\widehat{\\sigma}^2) \\equiv \\mathbb{E}[\\widehat{\\sigma}^2 - \\sigma^2] = -\\sigma^2/n \\] so \\(\\widehat{\\sigma}^2\\) is biased downwards. Because the bias goes to zero as the sample size grows, however, it is still a consistent estimator of \\(\\sigma^2\\). Another way to see that \\(\\widehat{\\sigma}^2\\) is biased is by carrying out a simulation study. To do this, we generate data from a distribution with a known variance and calculate \\(\\widehat{\\sigma}^2\\). Then we generate a new dataset from the same distribution and again calculate the corresponding value of \\(\\widehat{\\sigma}^2\\). Repeating this a large number of times, we end up with many estimates \\(\\widehat{\\sigma}^2\\), each based on a dataset of the same size drawn independently from the same population. This collection of estimates gives us an approximation to the sampling distribution of \\(\\widehat{\\sigma}^2\\). Using this approximation, we can get a good estimate of \\(\\text{Bias}(\\widehat{\\sigma}^2)\\) by comparing the sample mean of our simulated estimates \\(\\widehat{\\sigma}^2\\) to the true variance \\(\\sigma^2\\). 1.3.2 draw_sim_data() The first thing we need is a function to generate simulated data. Let's draw the \\(X_1, ..., X_n\\) from a normal distribution with mean zero and variance s_sq. To do this, we write a simple R function as follows: draw_sim_data &lt;- function(n, s_sq) { rnorm(n, sd = sqrt(s_sq)) } The nice thing about writing such a function is that we can test that it's working correctly. For example, suppose you were worried that draw_sim_data does not in fact generate n draws from a normal distribution with mean zero and variance s_sq. Then you could simply draw a large sample and check! Here I'll verify that draw_sim_data() returns a vector of the expected length, with the desired mean and variance, drawn from normal distribution.17 Everything works as expected: set.seed(420508570) test_sims &lt;- draw_sim_data(5000, 9) length(test_sims) ## [1] 5000 mean(test_sims) ## [1] -0.04620928 var(test_sims) ## [1] 8.832994 qqnorm(test_sims) qqline(test_sims) 1.3.3 get_estimate() The next step is to write a function that calculates \\(\\widehat{\\sigma}^2\\). We can do this as follows: get_estimate &lt;- function(x) { sum((x - mean(x))^2) / length(x) # divides by n not (n-1) } Again it's a good idea to test your code before proceeding. There are several tests we could consider running. First, if all the elements of x are the same then get_estimate() should return zero because (x - mean(x)) will simply be a vector of zeros. Everything looks good: get_estimate(rep(5, 25)) ## [1] 0 get_estimate(rep(0, 10)) ## [1] 0 Second, get_estimate() should not in general give the same result as var(), R's built-in function for the sample variance. This is because the latter divides by \\(n\\) rather than \\((n-1)\\). But if \\(n\\) is very large, this difference should become negligible. Again, everything works as expected: set.seed(420508570) sim_small &lt;- draw_sim_data(5, 1) c(sigma_hat_sq = get_estimate(sim_small), Sample_Var = var(sim_small)) ## sigma_hat_sq Sample_Var ## 0.1710749 0.2138436 sim_big &lt;- draw_sim_data(5000, 1) c(sigma_hat_sq = get_estimate(sim_big), Sample_Var = var(sim_big)) ## sigma_hat_sq Sample_Var ## 0.9814408 0.9816371 1.3.4 get_bias() Now we're ready to actually carry out our simulation study. The final step is to write a function called get_bias() that repeatedly calls draw_sim_data() and get_estimate(), stores the resulting estimates \\(\\widehat{\\sigma}^2\\) and calculates a simulation estimate of the bias. Compared to the functions from above, this one will be more complicated, so I'll explain it in steps. First the code: get_bias &lt;- function(n, s_sq, n_reps = 5000) { draw_sim_replication &lt;- function() { sim_data &lt;- draw_sim_data(n, s_sq) get_estimate(sim_data) } sim_replications &lt;- replicate(n_reps, draw_sim_replication()) mean(sim_replications - s_sq) } The function get_bias() takes three arguments: n is the sample size for each replication of the simulation experiment, s_sq is the true population variance from which we will simulate normal data, and n_reps is the number of simulation replications, i.e. the number of times that we want to repeat the simulation. The final argument, n_reps is optional: if you call get_bias() and supply only the first two arguments, R will set n_reps equal to the default value of 5000. The first step inside of get_bias() constructs a function called draw_sim_replication() that doesn't take any input arguments. This may seem strange: I'll explain it in a moment. For now, focus on the steps that draw_sim_replication() carries out. It first runs draw_sim_data(n, s_q) and stores the result in a vector called sim_data. Next it feeds sim_data as an input to get_estimate(). In other words, it carries out one replication of our simulation experiment. But how does the call to draw_sim_data() \"know\" which values to use for n and s_sq given that draw_sim_replication() doesn't take any input arguments? The key is that draw_sim_replication() is created inside of another function: get_bias(). When draw_sim_replication() encounters a reference to n and s_sq, it substitutes the values that were supplied as arguments to get_bias(). Here's another way of looking at draw_sim_replication(). We want to be able to run our simulation study for different values of n and s_sq. After we tell get_bias() our desired values of n and s_sq, it constructs a function for us called draw_sim_replication() that hard codes these particular parameter values. From this point on, calling draw_sim_replication() does \"the right thing\" without our having to explicitly specify n and s_sq. The next step of get_bias() uses the function replicate() to repeatedly call the function draw_sim_replication() a total of n_reps times. The results are stored in a vector called sim_replications. In essence, replicate() is shorthand for a common way of using a for loop. In the following example, x and y will be identical. But constructing x requires much more work: we first need to set up an empty vector, and then explicitly loop over it. In contrast, replicate() does all of this behind the scenes to construct y: do_something &lt;- function() { return(42) } x &lt;- rep(NA, 50) for(i in 1:50) { x[i] &lt;- do_something() } y &lt;- replicate(50, do_something()) identical(x, y) ## [1] TRUE Finally, get_bias() uses the simulation replications stored in the vector sim_replications to approximate the bias of \\(\\widehat{\\sigma}^2\\) by comparing them to the true value of \\(\\sigma^2\\), namely s_sq. It does this by computing the simulation analogue of \\(\\mathbb{E}[\\widehat{\\sigma}^2 - \\sigma^2]\\), which is simply mean(sim_replications - s_sq). 1.3.5 Running the Simulation Study Now we're ready to run our simulation study: we simply need to call get_bias() with our desired values of n and s_sq, for example: set.seed(420508570) get_bias(n = 5, s_sq = 1) ## [1] -0.2007312 It works! Up to simulation error, this result agrees with the theoretical bias of \\(-\\sigma^2/n = -1/5\\). To see that this isn't simply a fluke, we could try different values of n and s_sq. Again, the results agree with the theoretical values: set.seed(420508570) c(theoretical = -1/3, simulation = get_bias(3, 1)) ## theoretical simulation ## -0.3333333 -0.3412176 1.3.6 expand.grid() and Map() Now we have a function get_bias() that can approximate the bias of \\(\\widehat{\\sigma}^2\\) for any values of n and s_sq that we care to specify. But what if we want to carry out a simulation study over a range of values for n and s_sq? One way to do this is with a pair of nested for loops: one that iterates over different values of n and another that iterates over different values of s_sq. But this isn't a great strategy for two reasons. First, loops within loops tend to be slow in R. Second, the book-keeping required to implement this strategy is a bit involved. Fortunately there's a much better way: use expand.grid() and Map(). First we'll set up a grid of values for n and s_sq: n_grid &lt;- 3:5 n_grid ## [1] 3 4 5 s_sq_grid &lt;- seq(from = 1, to = 3, by = 0.5) s_sq_grid ## [1] 1.0 1.5 2.0 2.5 3.0 Now suppose that we want to run get_bias() for every combination of values in n_grid and s_sq_grid. Using the built-in R function expand.grid() we can easily construct a data frame whose rows contain all of these combinations: parameters &lt;- expand.grid(n = n_grid, s_sq = s_sq_grid) parameters ## n s_sq ## 1 3 1.0 ## 2 4 1.0 ## 3 5 1.0 ## 4 3 1.5 ## 5 4 1.5 ## 6 5 1.5 ## 7 3 2.0 ## 8 4 2.0 ## 9 5 2.0 ## 10 3 2.5 ## 11 4 2.5 ## 12 5 2.5 ## 13 3 3.0 ## 14 4 3.0 ## 15 5 3.0 The next step is to evaluated get_bias() repeatedly, once for every combination of parameter values stored in the rows of parameters. The Map() function makes this easy: set.seed(420508570) bias &lt;- Map(get_bias, n = parameters$n, s_sq = parameters$s_sq) Much like replicate(), Map() is shorthand for a common kind of for loop. In this case we loop over the rows of parameters. The first argument to Map() is the name of the function that we want to call repeatedly, in our case get_bias(). The remaining arguments are vectors of values. These are the arguments that Map() passes to get_bias(). The result of running the above code is a list of value, one for each row of parameters. head(bias) ## [[1]] ## [1] -0.3412176 ## ## [[2]] ## [1] -0.2546259 ## ## [[3]] ## [1] -0.2005197 ## ## [[4]] ## [1] -0.5007384 ## ## [[5]] ## [1] -0.3816447 ## ## [[6]] ## [1] -0.292554 length(bias) ## [1] 15 For example, the first element of bias corresponds to get_bias(3, 1). By setting the same seed and running this command \"manually\" we can verify that everything works as expected: set.seed(420508570) identical(get_bias(3, 1), bias[[1]]) ## [1] TRUE This pattern using expand.grid() and Map() is extremely flexible. In our example, get_bias() returns a scalar so bias is just a list of numbers. But more generally Map() can return a list that contains any kind of object at all. Here's a slightly more interesting example. The function get_bias_and_var() is a very slight modification of get_bias() from above that returns a list of two named elements: bias and variance get_bias_and_var &lt;- function(n, s_sq, n_reps = 5000) { draw_sim_replication &lt;- function() { sim_data &lt;- draw_sim_data(n, s_sq) get_estimate(sim_data) } sim_replications &lt;- replicate(n_reps, draw_sim_replication()) list(bias = mean(sim_replications - s_sq), variance = var(sim_replications)) } We can use this function to calculate both the bias and variance of the MLE \\(\\widehat{\\sigma}^2\\) as follows set.seed(420508570) bias_and_variance &lt;- Map(get_bias_and_var, n = parameters$n, s_sq = parameters$s_sq) bias_and_variance[1:3] ## [[1]] ## [[1]]$bias ## [1] -0.3412176 ## ## [[1]]$variance ## [1] 0.4554343 ## ## ## [[2]] ## [[2]]$bias ## [1] -0.2546259 ## ## [[2]]$variance ## [1] 0.3643993 ## ## ## [[3]] ## [[3]]$bias ## [1] -0.2005197 ## ## [[3]]$variance ## [1] 0.3243315 1.3.7 Formatting the Results We've carried out our simulation experiment, but the results are a bit messy. The parameter values are stored in a data frame called parameters and the biases and variances are stored in a list called bias_and_var. Let's format things a bit more nicely. If you have a list my_list whose elements are \"rows\" and you want to bind them together into a data frame, you can use the somewhat inscrutable command do.call(rbind, my_list). For example: do.call(rbind, bias_and_variance) ## bias variance ## [1,] -0.3412176 0.4554343 ## [2,] -0.2546259 0.3643993 ## [3,] -0.2005197 0.3243315 ## [4,] -0.5007384 0.9496033 ## [5,] -0.3816447 0.8616167 ## [6,] -0.292554 0.7173205 ## [7,] -0.6609821 1.781236 ## [8,] -0.5061243 1.538961 ## [9,] -0.3953514 1.326648 ## [10,] -0.8513285 2.705087 ## [11,] -0.6122964 2.467789 ## [12,] -0.4727481 2.014822 ## [13,] -0.9570253 4.124553 ## [14,] -0.7513177 3.201657 ## [15,] -0.567449 2.916767 Now we'll overwrite bias_and_var with the above and bind its columns, cbind(), with those of parameters bias_and_variance &lt;- do.call(rbind, bias_and_variance) sim_results &lt;- cbind(parameters, bias_and_variance) sim_results ## n s_sq bias variance ## 1 3 1.0 -0.3412176 0.4554343 ## 2 4 1.0 -0.2546259 0.3643993 ## 3 5 1.0 -0.2005197 0.3243315 ## 4 3 1.5 -0.5007384 0.9496033 ## 5 4 1.5 -0.3816447 0.8616167 ## 6 5 1.5 -0.292554 0.7173205 ## 7 3 2.0 -0.6609821 1.781236 ## 8 4 2.0 -0.5061243 1.538961 ## 9 5 2.0 -0.3953514 1.326648 ## 10 3 2.5 -0.8513285 2.705087 ## 11 4 2.5 -0.6122964 2.467789 ## 12 5 2.5 -0.4727481 2.014822 ## 13 3 3.0 -0.9570253 4.124553 ## 14 4 3.0 -0.7513177 3.201657 ## 15 5 3.0 -0.567449 2.916767 1.4 Exercise - The Hot Hand Now that you know the basics of running a simulation study in R, it's time to re-visit the Hot Hand example introduced above. Let shots be a sequence of shots made by a basketball player, Liz. In R, shots is stored as a vector of zeros and ones. If Liz has a \"hot hand\" then her chance of making her next shot should be higher when her last few shots were a success. This means we should expect ones to become more likely after a streak of ones in the vector shots. If Liz doesn't have a hot hand, then she should be no more likely to make her next shot after a streak of successes on her last few shots. Before Miller &amp; Sanjurjo (2018), researchers studying the hot hand operationalized this intuition as follows. Define a \"streak\" as \\(k\\) successes in a row. For each streak in shots, look at the shot that immediately follows it. Then collect each of these shots following a streak into another vector after_streak. Finally, compare the proportion of successes in shots to that in after_streak. Since each vector contains zeros and ones, we can make this comparison using mean(). If there's no hot hand, it seems reasonable to expect that mean(after_streak) will approximately equal to mean(shots). If there is a hot hand, on the other hand, then it seems reasonable to expect that mean(after_streak) should be larger than mean(shots). In this simulation study, rather than using a vector shots of a real basketball player, we'll draw a random sequence of zeros and ones. Since we control how the sequence is generated, this gives us a way of checking whether the procedure described in the preceding paragraph works as expected. To keep things simple, we'll set \\(k = 3\\) and generate shots as an iid sequence of Bernoulli(p) draws. Such a sequence clearly does not exhibit a hot hand. Moreover, there's no need to calculate mean(shots) because we know that the chance of making each simulated shot is p. This means that the exercise reduces to comparing mean(after_streak) to p. For example, if p is 0.5, then we compare mean(after_streak) to 0.5. Write a function called get_avg_after_streak() that takes a single input argument: shots a vector of zeros and ones. Your function should work as follows. First, create an empty vector of the same length as shots called after_streak. Second, set each element of after_streak to TRUE or FALSE depending on whether the corresponding element of shots follows a streak of three ones (TRUE) or not (FALSE). Third, extract all the elements of shots for which after_streak is true and calculate the proportion of these elements that equal one. Test your function on the vector c(0, 1, 1, 1, 1, 0, 0, 0) to make sure that it's working correctly. Show Hint For each element of shots we need to decide whether to set the corresponding element of after_streak to TRUE or FALSE. Regardless of the values that they take, the first three elements of shots cannot possibly follow a streak: there aren't three shots that precede them! This means that the first three elements of after_streak should always be set to FALSE. All that remains is to examine elements 4, 5, ..., n of shots. The easiest way to do this is with a for loop. For a given element i of shots, where i is between 4 and n, think about how to extract the three elements of shots that precede element i. Once you've extracted them, there are various ways to test whether all three equal one. The easiest way is to use the R function all(). If you find yourself getting confused, try working through the logic of the for loop by hand in the example scores &lt;- c(0, 1, 1, 1, 1, 0, 0, 0). Show Solution get_avg_after_streak &lt;- function(shots) { # shots should be a vector of 0 and 1; if not STOP! stopifnot(all(shots %in% c(0, 1))) n &lt;- length(shots) after_streak &lt;- rep(NA, n) # Empty vector of length n # The first 3 elements of shots by definition cannot # follow a streak after_streak[1:3] &lt;- FALSE # Loop over the remaining elements of shots for(i in 4:n) { # Extract the 3 shots that precede shot i prev_three_shots &lt;- shots[(i - 3):(i - 1)] # Are all three of the preceding shots equal to 1? # (TRUE/FALSE) after_streak[i] &lt;- all(prev_three_shots == 1) } # shots[after_streak] extracts all elements of shots # for which after_streak is TRUE. Taking the mean of # these is the same as calculating the prop. of ones mean(shots[after_streak]) } get_avg_after_streak(c(0, 1, 1, 1, 1, 0, 0, 0)) ## [1] 0.5 Write a function called draw_shots() that simulates an iid sequence of Bernoulli draws representing basketball shots, where 1 denotes a success and 0 denotes a miss. Your function should take two arguments: n_shots is the number of shots, and prob_success is the probability that any given shot will be a success. It should return a vector of length n_shots containing the simulation draws. Test your function by drawing a large sequence of shots with prob_success equal to 0.5 and calculating the sample mean. Show Solution draw_shots &lt;- function(n_shots, prob_success) { rbinom(n_shots, 1, prob_success) } set.seed(420508570) mean(draw_shots(1e4, 0.5)) ## [1] 0.4993 Write a function called get_hot_handedness() that combines draw_shots() and get_avg_after_streak() to calculated \"hot handedness\" of a randomly-drawn sequence. In words: get_est() should draw a sequence shots of simulated basketball shots and then calculate and return the fraction of successes in sim_shots among the shots that immediately follow a streak of three successes. Think carefully about what arguments get_est() needs to take if we want to be able to vary the length and success probability of our simulated sequence of basketball shots. Show Solution get_hot_handedness &lt;- function(n_shots, prob_success) { sim_shots &lt;- draw_shots(n_shots, prob_success) get_avg_after_streak(sim_shots) } Amos watches Liz take 100 basketball shots and calculates a test statistic \\(T\\) as follows: he identifies the shots that immediately follow a \"streak\" of three successful shots, and calculates Liz's success rate on this subset of shots. Amos argues as follows: \"Suppose that Liz does not have a hot hand: each of her shots is independent of the others and her probability of success on a given shot is 1/2. Then the expected value of my \\(T\\) will be one half.\" Use replicate() and get_hot_handedness() to carry out a simulation study with 10,000 replications to check whether Amos is correct. Show Hint It's possible, though unlikely, that get_hot_handedness() will return an NaN. This happens when the sequence of 100 simulated basketball shots contains no streaks of length three. We need to ignore any such simulation draws. The easiest way to do this is by using the na.rm of mean() to TRUE. Show Solution Amos is wrong: the expected value of his test statistic is approximately 0.46 rather than 0.5. On average, he will conclude that Liz has a cold hand when in fact her shots are iid. This approach is biased against finding evidence of a hot hand, if it exists: set.seed(420508570) sims &lt;- replicate(1e4, get_hot_handedness(100, 0.5)) mean(sims, na.rm = TRUE) ## [1] 0.4651983 Use Map() and expand.grid() to repeat the preceding exercise over a grid of values for the number of shots that Liz takes, n_shots, and her probability of success, prob_success. In particular, consider n_shots in \\(\\{50, 100, 200\\}\\) and prob_success in \\(\\{0.4, 0.5, 0.6\\}\\). Use do.call() and cbind() to summarize your results in a simple table. What conclusions do you draw? Show Hint Before using Map() and expand.grid() you'll need to create a function that can carry out the simulation study for fixed values of n_shots and prob_success. Call it get_test_stat_avg(). In broad strokes, you can base it on the function get_bias() that I explained earlier in the lesson. Just remember that our goal in this example is to calculate the mean of Amos' test statistic over simulation replications rather than its bias. Show Solution # Function to carry out the simulation study for a fixed # combination of values for n_shots and prob_success get_test_stat_avg &lt;- function(n_shots, prob_success, n_reps = 5e3) { draw_sim_replication &lt;- function() { sim_test_stat &lt;- get_hot_handedness(n_shots, prob_success) } sim_replications &lt;- replicate(n_reps, draw_sim_replication()) mean(sim_replications, na.rm = TRUE) } # Construct a grid of values for n_shots and # prob_success n_shots_grid &lt;- c(50, 100, 200) prob_success_grid &lt;- c(0.4, 0.5, 0.6) parameters &lt;- expand.grid(n_shots = n_shots_grid, prob_success = prob_success_grid) # Carry out the simulation study at each combination # of parameter values set.seed(420508570) test_stat_avgs &lt;- Map(get_test_stat_avg, n_shots = parameters$n_shots, prob_success = parameters$prob_success) # Store the results in a simple table and # display them test_stat_avgs &lt;- do.call(rbind, test_stat_avgs) results &lt;- cbind(parameters, test_stat_avgs) results ## n_shots prob_success test_stat_avgs ## 1 50 0.4 0.2934501 ## 2 100 0.4 0.3376670 ## 3 200 0.4 0.3705452 ## 4 50 0.5 0.4256395 ## 5 100 0.5 0.4582441 ## 6 200 0.5 0.4810223 ## 7 50 0.6 0.5491750 ## 8 100 0.6 0.5804618 ## 9 200 0.6 0.5892968 For every combination of n_shots and prob_success, Amos' test statistic appears to be biased downwards: test_stat_avgs is always smaller than prob_success. For a fixed value of prob_success, the bias appears to be more severe when n_shots is smaller. Bonus Question: Repeat the preceding exercise for different definitions of a streak, e.g. two successes in a row or four successes in a row. Think about which functions you'll need to change and how. How do your results change? Bonus Question: The exercises above consider a simulated basketball player who does not have a hot hand. Design a new simulation with a player who does exhibit a hot hand and see how this affects the results. (There are many ways to set up a simulation with dependence between trials, so there are many possible answers to this question.) See for example this write-up of Lantis &amp; Nessen (2021) in the February 2022 NBER digest.↩︎ An ungated version of Benjamin (2019) is available here.↩︎ See Miller &amp; Sanjurjo (2019) for a more accessible explanation that connects to several related probability puzzles.↩︎ Technically, \"random\" draws made on a computer are only pseudorandom. We'll discuss this further below.↩︎ For less common distributions, see CRAN Task View: Probability Distributions↩︎ A Bernoulli trial is a model for a possibly biased coin flip: if \\(X \\sim \\text{Bernoulli}(p)\\) then \\(\\mathbb{P}(X=1) = p\\) and \\(\\mathbb{P}(X=0) = 1-p\\).↩︎ To see this, first rewrite \\(\\sum_{i=1}^n (X_i - \\bar{X}_n)^2\\) as \\(\\sum_{i=1}^n (X_i - \\mu)^2 - n(\\bar{X}_n - \\mu)^2\\). This step is just algebra. Then take expectations, using the fact that the \\(X_i\\) are independent and identically distributed.↩︎ If you're unfamiliar with the Normal Q-Q plot that I used to check the normality of test_sims, you can read about it this blog post.↩︎ "],["getting-started-with-dplyr.html", "Lesson 2 Getting Started with dplyr 2.1 Package Installation 2.2 What is a tibble? 2.3 Filter Rows with filter 2.4 Sort data with arrange 2.5 Choose columns with select 2.6 The summarize verb 2.7 The group_by verb 2.8 Understanding the pipe: %&gt;% 2.9 Chaining commands 2.10 Change an existing variable or create a new one with mutate", " Lesson 2 Getting Started with dplyr This lesson and the next one will introduce you to some powerful tools for exploratory data analysis, using the gapminder dataset as an example. Simple descriptive statistics can be extremely powerful. When you read a newspaper columnist arguing that \\(X\\) is to blame for the massive change in \\(Y\\), how often do you take a look at the data to see if \\(Y\\) is really changing at all? Or if it's actually changing in the opposite direction? These are easy wins and we should make the most of them, particularly given that many common misconceptions are predictable. Today we'll focus on basic data manipulation and summary statistics using the dplyr package, part of the Tidyverse family of R packages. We'll make heavy use of dplyr and other Tidyverse packages throughout these lessons. This lesson will only scratch the surface of dplyr, although we'll also pick up a few more dplyr tricks in later lessons as well. A good resource for learning more is R for Data Science. The dplyr cheat sheet is also extremely helpful. So what is dplyr and why should you use it? There are three popular toolkits for data manipulation in R. The first is so-called \"base R.\" This simply means the built-in commands like subset() and operators like [], $, and so on. You need to know base R and you'll end up using it frequently, but it's simply not a great tool for complicated data manipulation. If you wrangle your data using base R commands, your code will be complicated, hard to read, and hard to maintain. The two main alternatives are dplyr and data.table. As you might expect, opinions differ on which of these is the \"better\" choice. Having used and taught both packages, here's my summary of the pros and cons. The data.table package is extremely fast. If you routinely work with very large datasets (a million rows or more), it's worth learning data.table. But data.table syntax is a bit arcane, and can off-putting to newcomers. The key advantage of dplyr is that it's easy to learn and its syntax is extremely intuitive and easy to read. It also has many helpful features that data.table lacks. Given the importance of replicability and open science, this tips the balance in favor of dplyr for this book. 2.1 Package Installation Before we can get started, you'll need to install two packages: dplyr, and gapminder. To do this, you can either click on the \"Packages\" tab in RStudio or use the command install.packages() at the R console, e.g. install.packages(&#39;dplyr&#39;) install.packages(&#39;gapminder&#39;) You only need to install a package once, but you need to load it every time you want to use it. To load a package, use the library() command, e.g. library(dplyr) library(ggplot2) library(gapminder) Now we're ready to go! I could tell you all about the data contained in gapminder, but an important part of this book is helping you to become self-sufficient. So instead I will leave this as an exercise for the reader! 2.1.1 Exercise After loading gapminder enter the command ?gapminder in the R console to view the R help file for this dataset. Read the documentation you find there to answer the following questions: How many rows and columns does gapminder contain? What information is contained in each row and column? What is the source of the data? 2.2 What is a tibble? The dplyr package uses a special operator called the pipe, written %&gt;% to chain together commands called verbs that act on objects called tibbles. This probably sounds complicated, but it's easier that it looks. Over the next few sections we'll slowly unpack the preceding sentence by looking at a number of simple examples. But first things first: what is a tibble? Let's see what happens if we display the gapminder dataset: gapminder ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows If you're used to working with dataframes in R, this may surprise you. Rather than trying to print all nrow(gapminder) rows on the screen, R helpfully shows us a useful summary of the information contained in gapminder. This is because gapminder is not a dataframe; it's a tibble, often abbreviated tbl. For our purposes, all you really need to know about tibbles is that they are souped up versions of R dataframes that are designed to work seamlessly with dplyr. To learn more see the chapter \"Tibbles\" in R for Data Science. 2.3 Filter Rows with filter We're ready to learn our first dplyr verb: filter selects rows. Here's an example: gapminder %&gt;% filter(year == 2007) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 2007 43.8 31889923 975. ## 2 Albania Europe 2007 76.4 3600523 5937. ## 3 Algeria Africa 2007 72.3 33333216 6223. ## 4 Angola Africa 2007 42.7 12420476 4797. ## 5 Argentina Americas 2007 75.3 40301927 12779. ## 6 Australia Oceania 2007 81.2 20434176 34435. ## 7 Austria Europe 2007 79.8 8199783 36126. ## 8 Bahrain Asia 2007 75.6 708573 29796. ## 9 Bangladesh Asia 2007 64.1 150448339 1391. ## 10 Belgium Europe 2007 79.4 10392226 33693. ## # … with 132 more rows Compare the results of running this command to what we got when we typed gapminder into the console above. Rather than displaying the whole dataset, now R is only showing us the 142 rows for which the column year has a value of 2007. So how does this work? The pipe operator %&gt;% \"pipes\" the tibble gapminder into the function filter(). The argument year == 2007 tells filter() that it should find all the rows such that the logical condition year == 2007 is TRUE. Oh no! Have we accidentally deleted all of the other rows of gapminder? No: we haven't made any changes to gapminder at all. If you don't believe me try entering gapminder at the console. All that this command does is display a subset of gapminder. If we wanted to store the result of running this command, we'd need to assign it to a variable, for example gapminder2007 &lt;- gapminder %&gt;% filter(year == 2007) gapminder2007 ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 2007 43.8 31889923 975. ## 2 Albania Europe 2007 76.4 3600523 5937. ## 3 Algeria Africa 2007 72.3 33333216 6223. ## 4 Angola Africa 2007 42.7 12420476 4797. ## 5 Argentina Americas 2007 75.3 40301927 12779. ## 6 Australia Oceania 2007 81.2 20434176 34435. ## 7 Austria Europe 2007 79.8 8199783 36126. ## 8 Bahrain Asia 2007 75.6 708573 29796. ## 9 Bangladesh Asia 2007 64.1 150448339 1391. ## 10 Belgium Europe 2007 79.4 10392226 33693. ## # … with 132 more rows We can also use filter to subset on two or more variables. For example, here we display data for the US in 2007: gapminder %&gt;% filter(year == 2007, country == &#39;United States&#39;) ## # A tibble: 1 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 United States Americas 2007 78.2 301139947 42952. Notice that I always put a linebreak after the pipe operator %&gt;% in my code examples. This isn't required to make the code run, but it's a good habit. As we start to write longer and more complicated dplyr commands, linebreaks will make it much easier to understand how our code works. 2.3.1 Exercises What is the difference between x = 3 and x == 3 in R? Show Solution The first assigns the value 3 to the variable x; the second tests whether x is equal to 3 and returns either TRUE or FALSE. Write code that uses filter to choose the subset of gapminder for which year is 2002. Show Solution gapminder %&gt;% filter(year == 2002) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 2002 42.1 25268405 727. ## 2 Albania Europe 2002 75.7 3508512 4604. ## 3 Algeria Africa 2002 71.0 31287142 5288. ## 4 Angola Africa 2002 41.0 10866106 2773. ## 5 Argentina Americas 2002 74.3 38331121 8798. ## 6 Australia Oceania 2002 80.4 19546792 30688. ## 7 Austria Europe 2002 79.0 8148312 32418. ## 8 Bahrain Asia 2002 74.8 656397 23404. ## 9 Bangladesh Asia 2002 62.0 135656790 1136. ## 10 Belgium Europe 2002 78.3 10311970 30486. ## # … with 132 more rows When I displayed data for the US in 2007, I put quotes around United States but not around year. Why? Show Solution This is because year contains numeric data while country contains character data, aka string data. If you instead try to choose the subset with year equal to 2005, something will go wrong. Try it. What happens and why? Show Solution If you go back to the help file for gapminder you'll see that it only contains data for every fifth year. The year 2005 isn't in our dataset so dplyr will display an empty tibble: gapminder %&gt;% filter(year == 2005) ## # A tibble: 0 × 6 ## # … with 6 variables: country &lt;fct&gt;, continent &lt;fct&gt;, year &lt;int&gt;, ## # lifeExp &lt;dbl&gt;, pop &lt;int&gt;, gdpPercap &lt;dbl&gt; Write code that stores the data for Asian countries in a tibble called gapminder_asia. Then display this tibble. Show Solution gapminder_asia &lt;- gapminder %&gt;% filter(continent == &#39;Asia&#39;) gapminder_asia ## # A tibble: 396 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 386 more rows Which country had the higher life expectancy in 1977: Ireland or Brazil? Which had the higher GDP per capita? Show Solution gapminder %&gt;% filter(year == 1977, country %in% c(&#39;Ireland&#39;, &#39;Brazil&#39;)) ## # A tibble: 2 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Brazil Americas 1977 61.5 114313951 6660. ## 2 Ireland Europe 1977 72.0 3271900 11151. 2.4 Sort data with arrange Suppose we wanted to sort gapminder by gdpPercap. To do this we can use the arrange command along with the pipe %&gt;% as follows: gapminder %&gt;% arrange(gdpPercap) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Congo, Dem. Rep. Africa 2002 45.0 55379852 241. ## 2 Congo, Dem. Rep. Africa 2007 46.5 64606759 278. ## 3 Lesotho Africa 1952 42.1 748747 299. ## 4 Guinea-Bissau Africa 1952 32.5 580653 300. ## 5 Congo, Dem. Rep. Africa 1997 42.6 47798986 312. ## 6 Eritrea Africa 1952 35.9 1438760 329. ## 7 Myanmar Asia 1952 36.3 20092996 331 ## 8 Lesotho Africa 1957 45.0 813338 336. ## 9 Burundi Africa 1952 39.0 2445618 339. ## 10 Eritrea Africa 1957 38.0 1542611 344. ## # … with 1,694 more rows The logic is very similar to what we saw above for filter. We pipe the tibble gapminder into the function arrange(). The argument gdpPercap tells arrange() that we want to sort by GDP per capita. Note that by default arrange() sorts in ascending order. If we want to sort in descending order, we use the function desc() as follows: gapminder %&gt;% arrange(desc(gdpPercap)) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Kuwait Asia 1957 58.0 212846 113523. ## 2 Kuwait Asia 1972 67.7 841934 109348. ## 3 Kuwait Asia 1952 55.6 160000 108382. ## 4 Kuwait Asia 1962 60.5 358266 95458. ## 5 Kuwait Asia 1967 64.6 575003 80895. ## 6 Kuwait Asia 1977 69.3 1140357 59265. ## 7 Norway Europe 2007 80.2 4627926 49357. ## 8 Kuwait Asia 2007 77.6 2505559 47307. ## 9 Singapore Asia 2007 80.0 4553009 47143. ## 10 Norway Europe 2002 79.0 4535591 44684. ## # … with 1,694 more rows 2.4.1 Exercises What is the lowest life expectancy in the gapminder dataset? Which country and year does it correspond to? Show Solution gapminder %&gt;% arrange(lifeExp) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda Africa 1992 23.6 7290203 737. ## 2 Afghanistan Asia 1952 28.8 8425333 779. ## 3 Gambia Africa 1952 30 284320 485. ## 4 Angola Africa 1952 30.0 4232095 3521. ## 5 Sierra Leone Africa 1952 30.3 2143249 880. ## 6 Afghanistan Asia 1957 30.3 9240934 821. ## 7 Cambodia Asia 1977 31.2 6978607 525. ## 8 Mozambique Africa 1952 31.3 6446316 469. ## 9 Sierra Leone Africa 1957 31.6 2295678 1004. ## 10 Burkina Faso Africa 1952 32.0 4469979 543. ## # … with 1,694 more rows What is the highest life expectancy in the gapminder dataset? Which country and year does it correspond to? Show Solution gapminder %&gt;% arrange(desc(lifeExp)) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. ## 3 Japan Asia 2002 82 127065841 28605. ## 4 Iceland Europe 2007 81.8 301931 36181. ## 5 Switzerland Europe 2007 81.7 7554661 37506. ## 6 Hong Kong, China Asia 2002 81.5 6762476 30209. ## 7 Australia Oceania 2007 81.2 20434176 34435. ## 8 Spain Europe 2007 80.9 40448191 28821. ## 9 Sweden Europe 2007 80.9 9031088 33860. ## 10 Israel Asia 2007 80.7 6426679 25523. ## # … with 1,694 more rows 2.5 Choose columns with select We use the select verb to choose a subset of columns. For example, to display only pop, country, and year, we would write gapminder %&gt;% select(pop, country, year) ## # A tibble: 1,704 × 3 ## pop country year ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 8425333 Afghanistan 1952 ## 2 9240934 Afghanistan 1957 ## 3 10267083 Afghanistan 1962 ## 4 11537966 Afghanistan 1967 ## 5 13079460 Afghanistan 1972 ## 6 14880372 Afghanistan 1977 ## 7 12881816 Afghanistan 1982 ## 8 13867957 Afghanistan 1987 ## 9 16317921 Afghanistan 1992 ## 10 22227415 Afghanistan 1997 ## # … with 1,694 more rows Now suppose that we wanted to select every column except pop. Here's one way to do it: gapminder %&gt;% select(country, continent, year, lifeExp, gdpPercap) ## # A tibble: 1,704 × 5 ## country continent year lifeExp gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 779. ## 2 Afghanistan Asia 1957 30.3 821. ## 3 Afghanistan Asia 1962 32.0 853. ## 4 Afghanistan Asia 1967 34.0 836. ## 5 Afghanistan Asia 1972 36.1 740. ## 6 Afghanistan Asia 1977 38.4 786. ## 7 Afghanistan Asia 1982 39.9 978. ## 8 Afghanistan Asia 1987 40.8 852. ## 9 Afghanistan Asia 1992 41.7 649. ## 10 Afghanistan Asia 1997 41.8 635. ## # … with 1,694 more rows but that takes a lot of typing! If there were more than a handful of columns in our tibble it would be very difficult to deselect a column in this way. Fortunately there's a shortcut: use the minus sign gapminder %&gt;% select(-pop) ## # A tibble: 1,704 × 5 ## country continent year lifeExp gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 779. ## 2 Afghanistan Asia 1957 30.3 821. ## 3 Afghanistan Asia 1962 32.0 853. ## 4 Afghanistan Asia 1967 34.0 836. ## 5 Afghanistan Asia 1972 36.1 740. ## 6 Afghanistan Asia 1977 38.4 786. ## 7 Afghanistan Asia 1982 39.9 978. ## 8 Afghanistan Asia 1987 40.8 852. ## 9 Afghanistan Asia 1992 41.7 649. ## 10 Afghanistan Asia 1997 41.8 635. ## # … with 1,694 more rows Just as we could when selecting, we can deselect multiple columns by separating their names with a comma: gapminder %&gt;% select(-pop, -year) ## # A tibble: 1,704 × 4 ## country continent lifeExp gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 28.8 779. ## 2 Afghanistan Asia 30.3 821. ## 3 Afghanistan Asia 32.0 853. ## 4 Afghanistan Asia 34.0 836. ## 5 Afghanistan Asia 36.1 740. ## 6 Afghanistan Asia 38.4 786. ## 7 Afghanistan Asia 39.9 978. ## 8 Afghanistan Asia 40.8 852. ## 9 Afghanistan Asia 41.7 649. ## 10 Afghanistan Asia 41.8 635. ## # … with 1,694 more rows It's easy to mix up the dplyr verbs select and filter. Here's a handy mnemonic: filteR filters Rows while seleCt selects Columns. Suppose we wanted to select only the column pop from gapminder. 2.5.1 Exercise Select only the columns year, lifeExp, and country in gapminder. Show Solution gapminder %&gt;% select(year, lifeExp, country) ## # A tibble: 1,704 × 3 ## year lifeExp country ## &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1952 28.8 Afghanistan ## 2 1957 30.3 Afghanistan ## 3 1962 32.0 Afghanistan ## 4 1967 34.0 Afghanistan ## 5 1972 36.1 Afghanistan ## 6 1977 38.4 Afghanistan ## 7 1982 39.9 Afghanistan ## 8 1987 40.8 Afghanistan ## 9 1992 41.7 Afghanistan ## 10 1997 41.8 Afghanistan ## # … with 1,694 more rows Select all the columns except year, lifeExp, and country in gapminder. Show Solution gapminder %&gt;% select(-year, -lifeExp, -country) ## # A tibble: 1,704 × 3 ## continent pop gdpPercap ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Asia 8425333 779. ## 2 Asia 9240934 821. ## 3 Asia 10267083 853. ## 4 Asia 11537966 836. ## 5 Asia 13079460 740. ## 6 Asia 14880372 786. ## 7 Asia 12881816 978. ## 8 Asia 13867957 852. ## 9 Asia 16317921 649. ## 10 Asia 22227415 635. ## # … with 1,694 more rows 2.6 The summarize verb Suppose we want to calculate the sample mean of the column lifeExp in gapminder. We can do this using the summarize verb as follows: gapminder %&gt;% summarize(mean_lifeExp = mean(lifeExp)) ## # A tibble: 1 × 1 ## mean_lifeExp ## &lt;dbl&gt; ## 1 59.5 Note the syntax: within summarize we have an assignment statement. In particular, we assign mean(lifeExp) to the variable mean_lifeExp. The key thing to know about summarize is that it always returns collapses a tibble with many rows into a single row. When we think about computing a sample mean, this makes sense: we want to summarize the column lifeExp as a single number. It doesn't actually make much sense to compute the mean of lifeExp because this involves averaging over different countries and different years. Instead let's compute the mean for a single year: 1952: gapminder %&gt;% filter(year == 1952) %&gt;% summarize(mean_lifeExp = mean(lifeExp)) ## # A tibble: 1 × 1 ## mean_lifeExp ## &lt;dbl&gt; ## 1 49.1 We can use summarize to compute multiple summary statistics for a single variable, the same summary statistic for multiple variables, or both: gapminder %&gt;% filter(year == 1952) %&gt;% summarize(mean_lifeExp = mean(lifeExp), sd_lifeExp = sd(lifeExp), mean_pop = mean(pop)) ## # A tibble: 1 × 3 ## mean_lifeExp sd_lifeExp mean_pop ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 49.1 12.2 16950402. Note that if we don't explicitly use an assignment statement, R will make up names for us based on the commands that we used: gapminder %&gt;% filter(year == 1952) %&gt;% summarize(mean(lifeExp), median(lifeExp), max(lifeExp)) ## # A tibble: 1 × 3 ## `mean(lifeExp)` `median(lifeExp)` `max(lifeExp)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 49.1 45.1 72.7 2.6.1 Exercise Use summarize to compute the 75th percentile of life expectancy in 1977. Show Solution gapminder %&gt;% filter(year == 1977) %&gt;% summarize(quantile(lifeExp, 0.75)) ## # A tibble: 1 × 1 ## `quantile(lifeExp, 0.75)` ## &lt;dbl&gt; ## 1 70.4 Use summarize to compute the 75th percentile of life expectancy among Asian countries in 1977. Show Solution gapminder %&gt;% filter(year == 1977, continent == &#39;Asia&#39;) %&gt;% summarize(quantile(lifeExp, 0.75)) ## # A tibble: 1 × 1 ## `quantile(lifeExp, 0.75)` ## &lt;dbl&gt; ## 1 65.9 2.7 The group_by verb The true power of summarize is its ability to compute grouped summary statistics in combination with another dplyr verb: group_by. In essence, group_by allows us to tell dplyr that we don't want to work with the whole dataset at once; rather we want to work with particular subsets or groups. The basic idea is similar to what we've done using filter in the past. For example, to calculate mean population (in millions) and mean life expectancy in the year 2007, we could use the following code: gapminder %&gt;% filter(year == 2007) %&gt;% summarize(meanPop = mean(pop) / 1000000, meanLifeExp = mean(lifeExp)) ## # A tibble: 1 × 2 ## meanPop meanLifeExp ## &lt;dbl&gt; &lt;dbl&gt; ## 1 44.0 67.0 Using group_by we could do the same thing for all years in the dataset at once: gapminder %&gt;% group_by(year) %&gt;% summarize(meanPop = mean(pop) / 1000000, meanLifeExp = mean(lifeExp)) ## # A tibble: 12 × 3 ## year meanPop meanLifeExp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 17.0 49.1 ## 2 1957 18.8 51.5 ## 3 1962 20.4 53.6 ## 4 1967 22.7 55.7 ## 5 1972 25.2 57.6 ## 6 1977 27.7 59.6 ## 7 1982 30.2 61.5 ## 8 1987 33.0 63.2 ## 9 1992 36.0 64.2 ## 10 1997 38.8 65.0 ## 11 2002 41.5 65.7 ## 12 2007 44.0 67.0 Notice what has changed in the second code block: we replaced filter(year == 2007) with group_by(year). This tells dplyr that, rather than simply restricting attention to data from 2007, we want to form subsets (groups) of the dataset that correspond to the values of the year variable. Whatever comes after group_by will then be calculated for these subsets. Here's another example. Suppose we wanted to calculate mean life expectancy and total population in each continent during the year 2007. To accomplish this, we can chain together the filter, group_by and summarize verbs as follows: gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarize(meanPop = mean(pop) / 1000000, meanLifeExp = mean(lifeExp)) ## # A tibble: 5 × 3 ## continent meanPop meanLifeExp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 17.9 54.8 ## 2 Americas 36.0 73.6 ## 3 Asia 116. 70.7 ## 4 Europe 19.5 77.6 ## 5 Oceania 12.3 80.7 We can also use group_by to subset over multiple variables at once. For example, to calculate mean life expectancy and total population in each continent separately for every year, we can use the following code: gapminder %&gt;% group_by(year, continent) %&gt;% summarize(meanPop = mean(pop) / 1000000, meanLifeExp = mean(lifeExp)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. ## # A tibble: 60 × 4 ## # Groups: year [12] ## year continent meanPop meanLifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 Africa 4.57 39.1 ## 2 1952 Americas 13.8 53.3 ## 3 1952 Asia 42.3 46.3 ## 4 1952 Europe 13.9 64.4 ## 5 1952 Oceania 5.34 69.3 ## 6 1957 Africa 5.09 41.3 ## 7 1957 Americas 15.5 56.0 ## 8 1957 Asia 47.4 49.3 ## 9 1957 Europe 14.6 66.7 ## 10 1957 Oceania 5.97 70.3 ## # … with 50 more rows 2.7.1 Exercise Why doesn't the following code work as expected? gapminder %&gt;% summarize(meanLifeExp = mean(lifeExp)) %&gt;% group_by(year) Show Solution The steps are carried out in the wrong order: we need to form groups first and then calculate our desired summaries. Calculate the median GDP per capita in each continent in 1977. Show Solution gapminder %&gt;% filter(year == 1977) %&gt;% group_by(continent) %&gt;% summarize(medGDPc = median(gdpPercap)) ## # A tibble: 5 × 2 ## continent medGDPc ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 1400. ## 2 Americas 6281. ## 3 Asia 3195. ## 4 Europe 14226. ## 5 Oceania 17284. Repeat 2. but sort your results in descending order. Show Solution gapminder %&gt;% filter(year == 1977) %&gt;% group_by(continent) %&gt;% summarize(medGDPc = median(gdpPercap)) %&gt;% arrange(desc(medGDPc)) ## # A tibble: 5 × 2 ## continent medGDPc ## &lt;fct&gt; &lt;dbl&gt; ## 1 Oceania 17284. ## 2 Europe 14226. ## 3 Americas 6281. ## 4 Asia 3195. ## 5 Africa 1400. Calculate the mean and standard deviation of life expectancy for separately for each continent in every year after 1977. Sort your results in ascending order by the standard deviation of life expectancy. Show Solution Show Solution gapminder %&gt;% filter(year &gt; 1977) %&gt;% group_by(continent, year) %&gt;% summarize(meanGDPc = mean(gdpPercap), sdGDPc = sd(gdpPercap)) %&gt;% arrange(sdGDPc) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the ## `.groups` argument. ## # A tibble: 30 × 4 ## # Groups: continent [5] ## continent year meanGDPc sdGDPc ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Oceania 1982 18555. 1304. ## 2 Oceania 1987 20448. 2038. ## 3 Africa 1987 2283. 2567. ## 4 Africa 1992 2282. 2644. ## 5 Africa 1997 2379. 2821. ## 6 Africa 2002 2599. 2973. ## 7 Africa 1982 2482. 3243. ## 8 Oceania 1992 20894. 3579. ## 9 Africa 2007 3089. 3618. ## 10 Oceania 1997 24024. 4206. ## # … with 20 more rows 2.8 Understanding the pipe: %&gt;% Let's revisit the pipe, %&gt;%, that we've used in the code examples above. I told you that the command gapminder %&gt;% filter(year == 2007) \"pipes\" the tibble gapminder into the function filter(). But what exactly does this mean? Take a look at the R help file for the dplyr function filter. We see that filter() takes something called .data as its first argument. Moving on to the \"Arguments\" section of the help file, we see that .data is \"A tbl\" i.e. a tibble. To better understand what this means, let's try using filter without the pipe: filter(gapminder, year == 2007, country == &#39;United States&#39;) ## # A tibble: 1 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 United States Americas 2007 78.2 301139947 42952. Notice that this gives us exactly the same result as gapminder %&gt;% filter(year == 2007, country == &#39;United States&#39;) ## # A tibble: 1 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 United States Americas 2007 78.2 301139947 42952. In other words The pipe is gives us an alternative way of supplying the first argument to a function. Let's try this with a more familiar R function: mean. The first argument of mean is a vector x. So let's try using the pipe to compute the mean of some data: x &lt;- c(1, 5, 2, 7, 2) x %&gt;% mean ## [1] 3.4 The pipe supplies a function with its first argument. If we want to specify additional arguments, we need to do so within the function call itself. For example, here's how we could use the pipe to compute the mean after dropping missing observations: y &lt;- c(1, 5, NA, 7, 2) y %&gt;% mean(na.rm = TRUE) ## [1] 3.75 One important note about the pipe: it's not a base R command. Instead it's a command provided by the package Magrittr. (If you're familiar with the Belgian painter Magritte, you may realize that the name of this package is quite witty!) This package is installed automatically along with dplyr. 2.8.1 Exercise Write R code that uses the pipe to calculate the sample variance of z &lt;- c(4, 1, 5, NA, 3) excluding the missing observation from the calculation. Show Solution z &lt;- c(4, 1, 5, NA, 3) z %&gt;% var(na.rm = TRUE) ## [1] 2.916667 Re-write the code from your solution to Exercise #4 without using the pipe. Show Solution arrange(gapminder,lifeExp) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda Africa 1992 23.6 7290203 737. ## 2 Afghanistan Asia 1952 28.8 8425333 779. ## 3 Gambia Africa 1952 30 284320 485. ## 4 Angola Africa 1952 30.0 4232095 3521. ## 5 Sierra Leone Africa 1952 30.3 2143249 880. ## 6 Afghanistan Asia 1957 30.3 9240934 821. ## 7 Cambodia Asia 1977 31.2 6978607 525. ## 8 Mozambique Africa 1952 31.3 6446316 469. ## 9 Sierra Leone Africa 1957 31.6 2295678 1004. ## 10 Burkina Faso Africa 1952 32.0 4469979 543. ## # … with 1,694 more rows arrange(gapminder, desc(lifeExp)) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. ## 3 Japan Asia 2002 82 127065841 28605. ## 4 Iceland Europe 2007 81.8 301931 36181. ## 5 Switzerland Europe 2007 81.7 7554661 37506. ## 6 Hong Kong, China Asia 2002 81.5 6762476 30209. ## 7 Australia Oceania 2007 81.2 20434176 34435. ## 8 Spain Europe 2007 80.9 40448191 28821. ## 9 Sweden Europe 2007 80.9 9031088 33860. ## 10 Israel Asia 2007 80.7 6426679 25523. ## # … with 1,694 more rows 2.9 Chaining commands In the examples we've looked at so far, the pipe doesn't seem all that useful: it's just an alternative way of specifying the first argument to a function. The true power and convenience of the pipe only becomes apparent we need to chain a series of commands together. For example, suppose we wanted to display the 1952 data from gapminder sorted by gdpPercap in descending order. Using the pipe, this is easy: gapminder %&gt;% filter(year == 1952) %&gt;% arrange(desc(gdpPercap)) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Kuwait Asia 1952 55.6 160000 108382. ## 2 Switzerland Europe 1952 69.6 4815000 14734. ## 3 United States Americas 1952 68.4 157553000 13990. ## 4 Canada Americas 1952 68.8 14785584 11367. ## 5 New Zealand Oceania 1952 69.4 1994794 10557. ## 6 Norway Europe 1952 72.7 3327728 10095. ## 7 Australia Oceania 1952 69.1 8691212 10040. ## 8 United Kingdom Europe 1952 69.2 50430000 9980. ## 9 Bahrain Asia 1952 50.9 120447 9867. ## 10 Denmark Europe 1952 70.8 4334000 9692. ## # … with 132 more rows Notice how I split the commands across multiple lines. This is good practice: it makes your code much easier to read. So what's happening when we chain commands in this way? The first step in the chain gapminder %&gt;% filter(year == 1952) returns a tibble: the subset of gapminder for which year is 1952. The next step %&gt;% arrange(gdpPercap) pipes this new tibble into the function arrange(), giving us the desired result. I hope you agree with me that this is pretty intuitive: even if we didn't know anything about dplyr we could almost figure out what this code is supposed to do. In stark contrast, let's look at the code we'd have to use if we wanted to accomplish the same task without using the pipe: arrange(filter(gapminder, year == 1952), desc(gdpPercap)) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Kuwait Asia 1952 55.6 160000 108382. ## 2 Switzerland Europe 1952 69.6 4815000 14734. ## 3 United States Americas 1952 68.4 157553000 13990. ## 4 Canada Americas 1952 68.8 14785584 11367. ## 5 New Zealand Oceania 1952 69.4 1994794 10557. ## 6 Norway Europe 1952 72.7 3327728 10095. ## 7 Australia Oceania 1952 69.1 8691212 10040. ## 8 United Kingdom Europe 1952 69.2 50430000 9980. ## 9 Bahrain Asia 1952 50.9 120447 9867. ## 10 Denmark Europe 1952 70.8 4334000 9692. ## # … with 132 more rows There are may reasons why this code is harder to read, but the most important one is that the commands arrange and filter have to appear in the code in the opposite of the order in which they are actually being carried out. This is because parentheses are evaluated from inside to outside. This is what's great about the pipe: it lets us write our code in a way that accords with the actual order of the steps we want to carry out. 2.9.1 Exercise What was the most populous European country in 1992? Write appropriate dplyr code using the pipe to display the information you need to answer this question. Show Solution gapminder %&gt;% filter(year == 1992, continent == &#39;Europe&#39;) %&gt;% arrange(desc(pop)) ## # A tibble: 30 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Germany Europe 1992 76.1 80597764 26505. ## 2 Turkey Europe 1992 66.1 58179144 5678. ## 3 United Kingdom Europe 1992 76.4 57866349 22705. ## 4 France Europe 1992 77.5 57374179 24704. ## 5 Italy Europe 1992 77.4 56840847 22014. ## 6 Spain Europe 1992 77.6 39549438 18603. ## 7 Poland Europe 1992 71.0 38370697 7739. ## 8 Romania Europe 1992 69.4 22797027 6598. ## 9 Netherlands Europe 1992 77.4 15174244 26791. ## 10 Hungary Europe 1992 69.2 10348684 10536. ## # … with 20 more rows Re-write your code from part 1. without using the pipe. Show Solution arrange(filter(gapminder, year == 1992, continent == &#39;Europe&#39;), desc(pop)) ## # A tibble: 30 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Germany Europe 1992 76.1 80597764 26505. ## 2 Turkey Europe 1992 66.1 58179144 5678. ## 3 United Kingdom Europe 1992 76.4 57866349 22705. ## 4 France Europe 1992 77.5 57374179 24704. ## 5 Italy Europe 1992 77.4 56840847 22014. ## 6 Spain Europe 1992 77.6 39549438 18603. ## 7 Poland Europe 1992 71.0 38370697 7739. ## 8 Romania Europe 1992 69.4 22797027 6598. ## 9 Netherlands Europe 1992 77.4 15174244 26791. ## 10 Hungary Europe 1992 69.2 10348684 10536. ## # … with 20 more rows 2.10 Change an existing variable or create a new one with mutate It's a little hard to read the column pop in gapminder since there are so many digits. Suppose that, instead of raw population, we wanted to display population in millions. This requires us to pop by 1000000, which we can do using the function mutate() from dplyr as follows: gapminder %&gt;% mutate(pop = pop / 1000000) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8.43 779. ## 2 Afghanistan Asia 1957 30.3 9.24 821. ## 3 Afghanistan Asia 1962 32.0 10.3 853. ## 4 Afghanistan Asia 1967 34.0 11.5 836. ## 5 Afghanistan Asia 1972 36.1 13.1 740. ## 6 Afghanistan Asia 1977 38.4 14.9 786. ## 7 Afghanistan Asia 1982 39.9 12.9 978. ## 8 Afghanistan Asia 1987 40.8 13.9 852. ## 9 Afghanistan Asia 1992 41.7 16.3 649. ## 10 Afghanistan Asia 1997 41.8 22.2 635. ## # … with 1,694 more rows Note the syntax here: within mutate() we have an assignment statement, namely pop = pop / 1000000. This tells R to calculate pop / 1000000 and assign the result to pop, in place of the original variable. We can also use mutate() to create a new variable. The gapminder dataset doesn't contain overall GDP, only GDP per capita. To calculate GDP, we need to multiply gdpPercap by pop. But wait! Didn't we just change pop so it's expressed in millions? No: we never stored the results of our previous command, we simply displayed them. Just as I discussed above, unless you overwrite it, the original gapminder dataset will be unchanged. With this in mind, we can create the gdp variable as follows: gapminder %&gt;% mutate(gdp = pop * gdpPercap) ## # A tibble: 1,704 × 7 ## country continent year lifeExp pop gdpPercap gdp ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. ## # … with 1,694 more rows 2.10.1 Exercise Explain why we used = rather than == in the mutate() examples above. Show Solution We used = because we're carrying out an assignment operation. In contrast == tests for equality, returning TRUE or FALSE. Calculate life expectancy in months and use it to answer the following question: \"which country in the Americas had the shortest life expectancy in months in the year 1962?\" Show Solution gapminder %&gt;% mutate(lifeExpMonths = 12 * lifeExp) %&gt;% filter(year == 1962, continent == &#39;Americas&#39;) %&gt;% arrange(lifeExpMonths) ## # A tibble: 25 × 7 ## country continent year lifeExp pop gdpPercap lifeExpMonths ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bolivia Americas 1962 43.4 3593918 2181. 521. ## 2 Haiti Americas 1962 43.6 3880130 1797. 523. ## 3 Guatemala Americas 1962 47.0 4208858 2750. 563. ## 4 Honduras Americas 1962 48.0 2090162 2291. 576. ## 5 Nicaragua Americas 1962 48.6 1590597 3634. 584. ## 6 Peru Americas 1962 49.1 10516500 4957. 589. ## 7 El Salvador Americas 1962 52.3 2747687 3777. 628. ## 8 Dominican Republic Americas 1962 53.5 3453434 1662. 642. ## 9 Ecuador Americas 1962 54.6 4681707 4086. 656. ## 10 Brazil Americas 1962 55.7 76039390 3337. 668. ## # … with 15 more rows "],["getting-started-with-ggplot2.html", "Lesson 3 Getting Started with ggplot2 3.1 A simple scatterplot using ggplot2 3.2 Plotting on the log scale 3.3 The color and size aesthetics 3.4 Faceting - Plots for Multiple Subsets 3.5 Plotting summarized data 3.6 Line plots 3.7 Bar plots 3.8 Cleveland Dot Charts 3.9 Histograms 3.10 Boxplots", " Lesson 3 Getting Started with ggplot2 In this lesson we'll build on your knowledge of dplyr and the gapminder dataset and introduce ggplot2, the R graphics package par excellence. Like dplyr, ggplot2 is also a part of the Tidyverse family of packages. To install the whole family of packages, use install.packages('tidyverse'). To load all of them at once, use library(tidyverse). (Of course you can also install and load ggplot2 on its own if you prefer.) This lesson is only the tip of the iceberg when it comes to ggplot2. We'll pick up a few more ggplot2 tricks in future lessons. For a more comprehensive treatment, see the free online draft of Data Visualization: A Practical Introduction. Another good reference is R for Data Science, and don't forget the ggplot2 cheat sheet! 3.1 A simple scatterplot using ggplot2 We'll start off by constructing a subset of the gapminder dataset that contains information from the year 2007 that we'll use for our plots below. library(gapminder) library(tidyverse) gapminder_2007 &lt;- gapminder %&gt;% filter(year == 2007) It takes some time to grow accustomed to ggplot2 syntax, so rather than giving you a lot of detail, we'll examine a series of examples that start off simple and become more complex. The first of these is a simple scatterplot using gapminder_2007. Each point will correspond to a single country in 2007. Its x-coordinate will be GDP per capita and its y-coordinate will be life expectancy. Here's the code: ggplot(gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp)) We see that GDP per capita is a very strong predictor of life expectancy, although the relationship is non-linear. Notice how I add a linebreak after the +. This is analogous to how I always add a linebreak after the pipe %&gt;%. While it isn't necessary for the code to run correctly, it improves readability. 3.1.1 Exercise Using my code example as a template, make a scatterplot with pop on the x-axis and lifeExp on the y-axis using gapminder_2007. Does there appear to be a relationship between population and life expectancy? Show Solution There is no clear relationship between population and life expectancy based on the 2007 data: ggplot(gapminder_2007) + geom_point(mapping = aes(x = pop, y = lifeExp)) Repeat 1. with gdpPercap on the y-axis. Show Solution There is no clear relationship between population and GDP per capita based on the 2007 data: ggplot(gapminder_2007) + geom_point(mapping = aes(x = pop, y = gdpPercap)) 3.2 Plotting on the log scale It's fairly common to transform data onto a log scale before carrying out further analysis or plotting. To transform the x-axis to the log base 10 scale, it's as easy as adding a + scale_x_log10() to the end of our command from above: ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp)) + scale_x_log10() Again: notice how I split the code across multiple lines and ended each of the intermediate lines with the +. This makes things much easier to read. 3.2.1 Exercise Using my code example as a template, make a scatterplot with the log base 10 of pop on the x-axis and lifeExp on the y-axis using the gapminder_2007 dataset. Show Solution ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = pop, y = lifeExp)) + scale_x_log10() Suppose that rather than putting the x-axis on the log scale, we wanted to put the y-axis on the log scale. Figure out how to do this, either by clever guesswork or a google search, and then repeat my example with gdpPercap and lifeExp with gdpPercap in levels and lifeExp in logs base 10. Show Solution ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp)) + scale_y_log10() Repeat 2. but with both axes on the log scale. Show Solution ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp)) + scale_x_log10() + scale_y_log10() 3.3 The color and size aesthetics It's time to start unraveling the somewhat mysterious-looking syntax of ggplot. To make a graph using ggplot we use the following template: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) replacing &lt;DATA&gt;, &lt;GEOM_FUNCTION&gt;, and &lt;MAPPINGS&gt; to specify what we want to plot and how it should appear. The first part is easy: we replace &lt;DATA&gt; with the dataset we want to plot, for example gapminder_2007 in the example from above. The second part is also fairly straightforward: we replace &lt;GEOM_FUNCTION&gt; with the name of a function that specifies the kind of plot we want to make. So far we've only seen one example: geom_point() which tells ggplot that we want to make a scatterplot. We'll see more examples in later lessons. For now, I want to focus on the somewhat more complicated-looking mapping = aes(&lt;MAPPINGS&gt;). The abbreviation aes is short for aesthetic and the code mapping = aes(&lt;MAPPINGS&gt;) defines what is called an aesthetic mapping. This is just a fancy way of saying that it tells R how we want our plot to look. The information we need to put in place of &lt;MAPPINGS&gt; depends on what kind of plot we're making. Thus far we've only examined geom_point() which produces a scatterplot. For this kind of plot, the minimum information we need to provide is the location of each point. For example, in our example above we wrote aes(x = gdpPercap, y = lifeExp) to tell R that gdpPercap gives the x-axis location of each point, and lifeExp gives the y-axis location. When making a scatterplot with geom_point we are not limited to specifying the x and y coordinates of each point; we can also specify the size and color of each point. This gives us a useful way of displaying more than two variables in a two-dimensional plot. We do this using aes. For example, let's use the color of each point to indicate continent ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + scale_x_log10() Notice how ggplot automatically generates a helpful legend. This plot makes it easy to see at a glance that the European countries in 2007 tend to have high GDP per capita and high life expectancy, while the African countries have the opposite. We can also use the size of each point to encode information, e.g. population: ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() 3.3.1 Exercise Would it make sense to set size = continent? What about setting col = pop? Explain briefly. Show Solution Neither of these makes sense since continent is categorical and pop is continuous: color is useful for categorical variables and size for continuous ones. The following code is slightly different from what I've written above. What is different. Try running it. What happens? Explain briefly. ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp)) + scale_x_log10() Show Solution It still works! You don't have to explicitly write data or mapping when using ggplot. I only included these above for clarity. In the future I'll leave them out to make my code more succinct. ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp)) + scale_x_log10() Create a tibble called gapminder_1952 that contains data from gapminder from 1952. Show Solution gapminder_1952 &lt;- gapminder %&gt;% filter(year == 1952) Use gapminder_1952 from the previous part to create a scatter plot with population on the x-axis, life expectancy on the y-axis, and continent represented by the color of the points. Plot population on the log scale (base 10). Show Solution ggplot(gapminder_1952) + geom_point(aes(x = pop, y = lifeExp, color = continent)) + scale_x_log10() Suppose that instead of indicating continent using color, you wanted all the points in the plot from 3. to be blue. Consult the chapter \"Visualising Data\" from R for Data Science to find out how to do this. Show Solution When you want color to be a variable from your dataset, put color = &lt;VARIABLE&gt; inside of aes; when you simply want to set the colors of all the points, put color = '&lt;COLOR&gt;' outside of aes, for example ggplot(gapminder_1952) + geom_point(aes(x = pop, y = lifeExp), color = &#39;blue&#39;) + scale_x_log10() 3.4 Faceting - Plots for Multiple Subsets Recall our plot of GDP per capita and life expectancy in 2007 from above: gapminder_2007 &lt;- gapminder %&gt;% filter(year == 2007) ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() This is an easy way to make a plot for a single year. But what if you wanted to make the same plot for every year in the gapminder dataset? It would take a lot of copying-and-pasting of the preceding code chunk to accomplish this. Fortunately there's a much easier way: faceting. In ggplot2 a facet is a subplot that corresponds to a subset of your dataset, for example the year 2007. We'll now use faceting to reproduce the plot from above for all the years in gapminder simultaneously: ggplot(gapminder) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() + facet_wrap(~ year) Note the syntax here: in a similar way to how we added scale_x_log10() to plot on the log scale, we add facet_wrap(~ year) to facet by year. The tilde ~ is important: this has to precede the variable by which you want to facet. Now that we understand how to produce it, let's take a closer look at this plot. Notice how this plot allows us to visualize five variables simultaneously. By looking at how the plots change over time, we see a pattern of increasing GDP per capita and life expectancy throughout the world between 1952 and 2007. Notice in particular the dramatic improvements in both variables in the Asian economies. 3.4.1 Exercise What would happen if I were to run the following code? Explain briefly. ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() + facet_wrap(~ year) Show Solution We'll only get one facet since the tibble gapminder_2007 only has data for 2007: ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() + facet_wrap(~ year) Make a scatterplot with data from gapminder for the year 1977. Your plot should be faceted by continent with GDP per capita on the log scale on the x-axis, life expectancy on the y-axis, and population indicated by the size of each point. Show Solution gapminder_1977 &lt;- gapminder %&gt;% filter(year == 1977) ggplot(gapminder_1977) + geom_point(aes(x = gdpPercap, y = lifeExp, size = pop)) + scale_x_log10() + facet_wrap(~ continent) What would happen if you tried to facet by pop? Explain briefly. Show Solution You'll get something crazy if you try this. Population is continuous rather than categorical so every country has a different value for this variable. You'll end up with one plot for every country, containing a single point: # Not run: it takes a long time and looks nasty! gapminder_1977 &lt;- gapminder %&gt;% filter(year == 1977) ggplot(gapminder_1977) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent)) + scale_x_log10() + facet_wrap(~ pop) 3.5 Plotting summarized data By combining summarize and group_by with ggplot, it's easy to make plots of grouped data. For example, here's how we could plot total world population in millions from 1952 to 2007. First we construct a tibble which I'll name by_year containing the desired summary statistic grouped by year and display it: by_year &lt;- gapminder %&gt;% mutate(popMil = pop / 1000000) %&gt;% group_by(year) %&gt;% summarize(totalpopMil = sum(popMil)) by_year ## # A tibble: 12 × 2 ## year totalpopMil ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 2407. ## 2 1957 2664. ## 3 1962 2900. ## 4 1967 3217. ## 5 1972 3577. ## 6 1977 3930. ## 7 1982 4289. ## 8 1987 4691. ## 9 1992 5111. ## 10 1997 5515. ## 11 2002 5887. ## 12 2007 6251. Then we make a scatterplot using ggplot: ggplot(by_year) + geom_point(aes(x = year, y = totalpopMil)) Here's a more complicated example where we additionally use color to plot each continent separately: by_year_continent &lt;- gapminder %&gt;% mutate(popMil = pop / 1000000) %&gt;% group_by(year, continent) %&gt;% summarize(totalpopMil = sum(popMil)) by_year ## # A tibble: 12 × 2 ## year totalpopMil ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 2407. ## 2 1957 2664. ## 3 1962 2900. ## 4 1967 3217. ## 5 1972 3577. ## 6 1977 3930. ## 7 1982 4289. ## 8 1987 4691. ## 9 1992 5111. ## 10 1997 5515. ## 11 2002 5887. ## 12 2007 6251. ggplot(by_year_continent) + geom_point(aes(x = year, y = totalpopMil, color = continent)) Make sure you understand how the preceding example works before attempting the exercise. 3.5.1 Exercise What happens if you append + expand_limits(y = 0) to the preceding ggplot code? Why might this be helpful in some cases? Show Solution The function expand_limits() lets us tweak the limits of our x or y-axis in a ggplot. In this particular example expand_limits(y = 0) ensures that the y-axis begins at zero. Without using this command, ggplot will choose the y-axis on its own so that there is no \"empty space\" in the plot. Sometimes we may want to override this behavior. Make a scatter with average GDP per capita across all countries in gapminder in the y-axis and year on the x-axis. Show Solution by_year &lt;- gapminder %&gt;% group_by(year) %&gt;% summarize(meanGDPc = mean(gdpPercap)) ggplot(by_year) + geom_point(aes(x = year, y = meanGDPc)) Repeat 2. broken down by continent, using color to distinguish the points. Put mean GDP per capita on the log scale. Show Solution by_year &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarize(meanGDPc = mean(gdpPercap)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. ggplot(by_year) + geom_point(aes(x = year, y = meanGDPc, color = continent)) + scale_y_log10() 3.6 Line plots Thus far we've only learned how to make one kind of plot with ggplot: a scatterplot, which we constructed using geom_scatter(). Sometimes we want to connect the dots in a scatterplot, for example when we're interested in visualizing a trend over time. The resulting plot is called a line plot. To make one, simply replace geom_scatter() with geom_line(). For example: by_year_continent &lt;- gapminder %&gt;% mutate(popMil = pop / 1000000) %&gt;% group_by(year, continent) %&gt;% summarize(totalpopMil = sum(popMil)) by_year ## # A tibble: 60 × 3 ## # Groups: year [12] ## year continent meanGDPc ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Africa 1253. ## 2 1952 Americas 4079. ## 3 1952 Asia 5195. ## 4 1952 Europe 5661. ## 5 1952 Oceania 10298. ## 6 1957 Africa 1385. ## 7 1957 Americas 4616. ## 8 1957 Asia 5788. ## 9 1957 Europe 6963. ## 10 1957 Oceania 11599. ## # … with 50 more rows ggplot(by_year_continent) + geom_line(aes(x = year, y = totalpopMil, color = continent)) 3.6.1 Exercise Repeat exercise 5-3 with a line plot rather than a scatterplot. Show Solution by_year &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarize(meanGDPc = mean(gdpPercap)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. ggplot(by_year) + geom_line(aes(x = year, y = meanGDPc, color = continent)) + scale_y_log10() 3.7 Bar plots To make a bar plot, we use geom_col(). Note that the x argument of aes needs to be a categorical variable for a bar plot to make sense. Here's a simple example: by_continent &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarize(meanLifeExp = mean(lifeExp)) ggplot(by_continent) + geom_col(aes(x = continent, y = meanLifeExp)) Sometimes we want to turn a bar plot, or some other kind of plot, on its side. This can be particularly helpful if the x-axis labels are very long. To do this, simply add + coord_flip() to your ggplot command, for example: ggplot(by_continent) + geom_col(aes(x = continent, y = meanLifeExp)) + coord_flip() 3.7.1 Exercise Make a collection of bar plots faceted by year that compare mean GDP per capita across countries in a given year. Orient your plots so it's easy to read the continent labels. Show Solution ggplot(by_year) + geom_col(aes(x = continent, y = meanGDPc)) + facet_wrap(~ year) + coord_flip() 3.8 Cleveland Dot Charts Now that you know how to make a barchart don't bother; dot charts as described by Cleveland (1984), are a simpler, cleaner and more flexible alternative. ggplot(by_continent) + geom_point(aes(x = meanLifeExp, y = continent)) Unlike the equivalent bar chart from above, this dot chart restricts the meanLifeExp axis rather than extending it all the way to zero. This makes sense given that our interest in making this plot is to compare average life expectancy across continents. Dot charts are typically most informative when sorted by the continuous variable, meanLifeExp in our case. A convenient way to achieve this is by using the fct_reorder() function from the forcats package, a member of the Tidyverse. Here's a simple pipeline that does the trick: library(forcats) by_continent %&gt;% mutate(continent = fct_reorder(continent, meanLifeExp)) %&gt;% ggplot() + geom_point(aes(x = meanLifeExp, y = continent)) The first argument of fct_reorder() is the factor whose levels we want to re-order. The second argument is the variable that we'll use to determine the order. In our case, we reorder continent according to meanLifeExp. Another thing worth noticing in the preceding code chunk is the way that I modified by_continent in place and piped the result directly into ggplot(). I didn't bother to store this modified version of by_continent or give it a new name, because I knew that I wouldn't need to use it again. Because dots take up less space than bars, dot charts provide a cleaner way of making comparisons within and between groups simultaneously. Here's a more complicated example that shows how life expectancy has changed in each continent between 1987 and 2007: gapminder %&gt;% filter(year %in% c(1987, 2007)) %&gt;% mutate(year = factor(year)) %&gt;% group_by(continent, year) %&gt;% summarize(meanLifeExp = mean(lifeExp)) %&gt;% ggplot(aes(x = meanLifeExp, y = continent)) + geom_line(aes(group = continent)) + geom_point(aes(color = year)) 3.8.1 Exercise Make a dot chart of GDP per capita in all European countries in the year 2007. Sort the dots so that the country with the highest GDP per capita appears a the top and the country with the lowest appears at the bottom. Show Solution gapminder %&gt;% filter(continent == &#39;Europe&#39;, year == 2007) %&gt;% mutate(country = fct_reorder(country, gdpPercap)) %&gt;% ggplot() + geom_point(aes(x = gdpPercap, y = country)) 3.9 Histograms To make a ggplot2 histogram, we use the function geom_histogram(). Recall that a histogram summarizes a single variable at a time by forming non-overlapping bins of equal width and calculating the fraction of observations in each bin. If we choose a different width for the bins, we'll get a different histogram. Here's an example of two different bin widths: gapminder_2007 &lt;- gapminder %&gt;% filter(year == 2007) ggplot(gapminder_2007) + geom_histogram(aes(x = lifeExp), binwidth = 5) ggplot(gapminder_2007) + geom_histogram(aes(x = lifeExp), binwidth = 1) 3.9.1 Exercise All of the examples we've seen that use ggplot besides histograms have involved specifying both x and y within aes(). Why are histograms different? Show Solution This is because histograms only depict a single variable while the other plots we've made show two variables at once. What happens if you don't specify a bin width in either of my two examples? Show Solution If you don't specify a bin width, ggplot2 will pick one for you and possibly give you a warning suggesting that you pick a better bin width manually. Make a histogram of GDP per capita in 1977. Play around with different bin widths until you find one that gives a good summary of the data. Show Solution There's no obvious right answer for the bin width, but here's one possibility: gapminder1977 &lt;- gapminder %&gt;% filter(year == 1977) ggplot(gapminder_1977) + geom_histogram(aes(x = gdpPercap), binwidth = 5000) Repeat 3. but put GDP per capita on the log scale. Show Solution You'll need a much smaller bin width when using the log scale, for example: ggplot(gapminder_1977) + geom_histogram(aes(x = gdpPercap), binwidth = 0.2) + scale_x_log10() Compare and contrast the two different histograms you've made. Show Solution No right answer: it's a discussion question! But the idea is to see how taking logs gets rid of the huge positive skewness in GDP per capita. 3.10 Boxplots The final kind of ggplot we'll learn about in this lesson is a boxplot, a visualization of the five-number summary of a variable: minimum, 25th percentile, median, 75th percentile, and maximum. To make a boxplot in ggplot we use the function geom_boxplot(), for example: ggplot(gapminder_2007) + geom_boxplot(aes(x = continent, y = lifeExp)) Compared to histograms, boxplots provide less detail but allow us to easily compare across groups. 3.10.1 Exercise What is the meaning of the little \"dots\" that appear in the boxplot above? Use a Google search to find out what they are and how they are computed. Show Solution They are outliers: ggplot considers any observation that is more than 1.5 times the interquartile range away from the \"box\" to be an outlier, and adds a point to indicate it. Use faceting to construct a collection of boxplots, each of which compares log GDP per capita across continents in a given year. Turn your boxplots sideways to make it easier to read the continent labels. Show Solution ggplot(gapminder) + geom_boxplot(aes(x = continent, y = gdpPercap)) + facet_wrap(~ year) + scale_y_log10() + coord_flip() + ggtitle(&#39;GDP per Capita by Continent: 1952-2007&#39;) Use a Google search to find out how to add a title to a ggplot. Use it to add a title to the plot you created in 2. Show Solution Use ggtitle('YOUR TITLE HERE') as I did in my solution to 2. above. "],["predictive-regression-part-i.html", "Lesson 4 Predictive Regression Part I 4.1 Introduction 4.2 The Least Squares Problem 4.3 Linear Regression with lm() 4.4 Plotting the Regression Line 4.5 Getting More from lm() 4.6 Summarizing The Ouput of lm() 4.7 Tidying up with broom 4.8 Dummy Variables with lm() 4.9 Fun with R Formulas", " Lesson 4 Predictive Regression Part I 4.1 Introduction This is the first of two lessons that will teach you how to implement and interpret predictive linear regression in R. For the moment we won't worry about causality and we won't talk about heteroskedasticity or autocorrelation. In this first lesson, we'll introduce the basics using a simple dataset that you can download from my website and display as follows: library(readr) kids &lt;- read_csv(&quot;http://ditraglia.com/data/child_test_data.csv&quot;) kids ## # A tibble: 434 × 4 ## kid.score mom.hs mom.iq mom.age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 65 1 121. 27 ## 2 98 1 89.4 25 ## 3 85 1 115. 27 ## 4 83 1 99.4 25 ## 5 115 1 92.7 27 ## 6 98 0 108. 18 ## 7 69 1 139. 20 ## 8 106 1 125. 23 ## 9 102 1 81.6 24 ## 10 95 1 95.1 19 ## # … with 424 more rows Each row of the tibble kids contains information on a three-year old child. The first column gives the child's test score at age three, while the remaining columns provide information about each child's mother: kid.score child test score at age 3 mom.age age of mother at birth of child mom.hs mother completed high school? (1 = yes) mom.iq mother's IQ score The columns kid.score gives the child's test score at age three. The remaining columns describe the child's mother: mom.age is mother's age at the birth of the child, mom.hs is a dummy variable that equals one the mother completed high school, and mom.iq is the mother's IQ score. Our main goal will be to predict a child's test score based on mother characteristics. But stay alert: in some of the exercises I may be a bit devious and ask you to predict something else! 4.1.1 Exercise Using a dot . to separate words in a variable name isn't great coding style: it's better to use an underscore _. Search the dplyr help files for the command rename() and then use this command to replace each instance of a . in the column names of kids with an underscore _. Show Solution library(dplyr) kids &lt;- kids %&gt;% rename(kid_score = kid.score, mom_hs = mom.hs, mom_iq = mom.iq, mom_age = mom.age) 4.2 The Least Squares Problem Suppose we observe a dataset with \\(n\\) observations \\((Y_i, X_i)\\) where \\(Y_i\\) is an outcome variable for person \\(i\\)--the thing we want to predict--and \\(X_i\\) is a vector of \\(p\\) predictor variables--the things we'll use to make our prediction. In the kids dataset, our outcome is kid_score and our predictors are mom_hs, mom_age, and mom_iq. Our goal is to build a model of the form \\(X&#39;\\beta = \\sum_{j=1}^p \\beta_j X_{j}\\) that we can use to predict \\(Y\\) for a person who is not in our dataset. The constants \\(\\beta_j\\) are called coefficients and a model of this form is called a linear model because the \\(\\beta_j\\) enter linearly: they're not raised to any powers etc. Ordinary least squares (OLS) uses the observed data to find the coefficients \\(\\widehat{\\beta}\\) that solve the least squares problem \\[ \\underset{\\beta}{\\text{minimize}} \\sum_{i=1}^n (Y_i - X_i&#39;\\beta)^2. \\] In case you were wondering \"but wait, where's the intercept?\" I should point out that some people prefer to write \\((Y_i - \\beta_0 - X_i&#39; \\beta)\\) rather than \\((Y_i - X_i&#39;\\beta)\\). To allow an intercept using my notation, simply treat the first element of my \\(X_i\\) vector as a \\(1\\) and the first element of my \\(\\beta\\) vector as the intercept. 4.2.1 Exercise Suppose we want to regress the outcome \\(Y_i\\) on an intercept only, in other words we want to minimize \\(\\sum_{i=1}^n (Y_i - \\beta)^2\\) over \\(\\beta\\). What is the solution? Does this make sense? Show Solution Differentiating with respect to \\(\\beta\\), the first order condition is \\(-2 \\sum_{i=1}^n (Y_i - \\widehat{\\beta}) = 0\\). Because the objective function is convex, this characterizes the global minimum. Re-arranging and solving for \\(\\widehat{\\beta}\\) gives \\(\\widehat{\\beta} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\). In other words \\(\\widehat{\\beta} = \\bar{Y}\\), the sample mean. This makes sense: the sample mean is a reasonable prediction of the next \\(Y\\)-observation if you have no other information to work with. Here we've shown that it is also the least squares solution. 4.3 Linear Regression with lm() The R function lm(), short for linear model, solves the least squares problem. Its basic syntax is lm([formula], [dataframe]) where [formula] is an R formula--an object that describes the regression we want to run--and [dataframe] is the name of a data frame containing our \\(X\\) and \\(Y\\) observations, e.g. kids. R formulas can be a bit confusing when you first encounter them, so I'll explain the details in stages. For the moment, there are two symbols you need to learn: ~ and + The tilde symbol ~ is used to separate the \"left hand side\" and \"right hand side\" of a formula: the outcome goes on the left of the ~ and the predictors go on the right. For example, to regress kid_score on mom_iq we use the command lm(kid_score ~ mom_iq, kids) ## ## Call: ## lm(formula = kid_score ~ mom_iq, data = kids) ## ## Coefficients: ## (Intercept) mom_iq ## 25.80 0.61 This tells R: \"please solve the least squares problem to predict kid_score using mom_iq based on the data contained in kids.\" Notice that R includes an intercept in the regression automatically. This is a good default, because it seldom makes sense to run a regression without an intercept. When you want to run a regression with multiple right-hand side predictors, use the plus sign + to separate them. For example, to regress kid_score on mom_iq and mom_ageuse the command lm(kid_score ~ mom_iq + mom_age, kids) ## ## Call: ## lm(formula = kid_score ~ mom_iq + mom_age, data = kids) ## ## Coefficients: ## (Intercept) mom_iq mom_age ## 17.5962 0.6036 0.3881 4.3.1 Exercise Interpret the regression coefficients from lm(kid_score ~ mom_iq, kids). Show Solution Consider two kids whose mothers differ by 1 point in mom_iq. We would predict that the kid whose mom has the higher value of mom_iq will score about 0.6 points higher in kid_score. lm(kid_score ~ mom_iq, kids) ## ## Call: ## lm(formula = kid_score ~ mom_iq, data = kids) ## ## Coefficients: ## (Intercept) mom_iq ## 25.80 0.61 Run a linear regression to predict mom_hs using kid_score and mom_iq. Show Solution lm(mom_hs ~ kid_score + mom_iq, kids) ## ## Call: ## lm(formula = mom_hs ~ kid_score + mom_iq, data = kids) ## ## Coefficients: ## (Intercept) kid_score mom_iq ## -0.060135 0.002775 0.006050 4.4 Plotting the Regression Line The ggplot2 package makes it easy to produce an attractive and informative plot of the results of a simple linear regression. Using what we learned in our last lesson, we know how to make a scatter plot of mom_iq and kid_score: library(ggplot2) ggplot(kids) + geom_point(aes(x = mom_iq, y = kid_score)) To add the regression line, we'll use geom_smooth(), a function for plotting smoothed conditional means. We can use geom_smooth() in exactly the same way as geom_point() by specifying aes() to set the x and y variables. By default, geom_smooth() plots a non-parametric regression curve rather than a linear regression: ggplot(kids) + geom_point(aes(x = mom_iq, y = kid_score)) + geom_smooth(aes(x = mom_iq, y = kid_score)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Notice that we had to type aes(x = mom_iq, y = kid_score) twice in the preceding code chunk. This is tedious and error-prone. In more complicated plots that contain multiple geom functions, life is much simpler if we specify our desired aes() once. To do this, pass it as an argument to ggplot rather than to the geom, for example ggplot(kids, aes(x = mom_iq, y = kid_score)) + geom_point() + geom_smooth() To plot a regression line rather than this non-parametric regression function, we merely need to set method = 'lm in geom_smooth(), for example ggplot(kids, aes(x = mom_iq, y = kid_score)) + geom_point() + geom_smooth(method = &#39;lm&#39;) By default this plots the regression line from the regression y ~ x. 4.4.1 Exercise Use ggplot() to make a scatter plot with mom_age on the horizontal axis and kid_score on the vertical axis. Show Solution ggplot(kids, aes(x = mom_age, y = kid_score)) + geom_point() Use geom_smooth() to including the non-parametric regression function on the preceding plot. Show Solution ggplot(kids, aes(x = mom_age, y = kid_score)) + geom_point() + geom_smooth() Modify the preceding to include the regression line corresponding to kid_score ~ mom_age on the scatter plot rather than the non-parametric regression function. Show Solution ggplot(kids, aes(x = mom_age, y = kid_score)) + geom_point() + geom_smooth(method = &#39;lm&#39;) 4.5 Getting More from lm() If we simply run lm as above, R will display only the estimated regression coefficients and the command that we used to run the regression: Call. To get more information, we need to store the results of our regression using the assignment operator &lt;- for example: reg1 &lt;- lm(kid_score ~ mom_iq, kids) If you run the preceding line of code in the R console, it won't produce any output. But if you check your R environment after running it, you'll see a new List object: reg1. To see what's inside this list, we can use the command str: str(reg1) ## List of 12 ## $ coefficients : Named num [1:2] 25.8 0.61 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;mom_iq&quot; ## $ residuals : Named num [1:434] -34.68 17.69 -11.22 -3.46 32.63 ... ## ..- attr(*, &quot;names&quot;)= chr [1:434] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ effects : Named num [1:434] -1808.22 190.39 -8.77 -1.96 33.73 ... ## ..- attr(*, &quot;names&quot;)= chr [1:434] &quot;(Intercept)&quot; &quot;mom_iq&quot; &quot;&quot; &quot;&quot; ... ## $ rank : int 2 ## $ fitted.values: Named num [1:434] 99.7 80.3 96.2 86.5 82.4 ... ## ..- attr(*, &quot;names&quot;)= chr [1:434] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .... Don't panic: you don't need to know what all of these list elements are. The important thing to understand is that lm returns a list from which we can extract important information about the regression we have run. To extract the regression coefficient estimates, we use the function coefficients() or coef() for short coef(reg1) ## (Intercept) mom_iq ## 25.7997778 0.6099746 To extract the regression residuals, we use the function residuals() or resid() for short resid(reg1) ## 1 2 3 4 5 6 ## -34.67839049 17.69174662 -11.21717291 -3.46152907 32.62769741 6.38284487 ## 7 8 9 10 11 12 ## -41.52104074 3.86488149 26.41438662 11.20806784 11.17050590 -25.66178773 ## 13 14 15 16 17 18 ## 3.93517580 -17.40659730 14.87699469 10.74760539 6.40273692 5.13172163 ## 19 20 21 22 23 24 ## -2.44440289 15.87116678 9.04396998 11.90180808 14.47664178 0.24807074 ## 25 26 27 28 29 30 ## 13.66974891 -4.06297022 -14.03359486 -7.52559318 2.18354609 2.47533381 .... To extract the fitted values i.e. \\(\\hat{Y}_i \\equiv X_i&#39;\\hat{\\beta}\\), the predicted values of, we use fitted.values fitted.values(reg1) ## 1 2 3 4 5 6 7 8 ## 99.67839 80.30825 96.21717 86.46153 82.37230 91.61716 110.52104 102.13512 ## 9 10 11 12 13 14 15 16 ## 75.58561 83.79193 79.82949 83.66179 80.06482 95.40660 87.12301 99.25239 ## 17 18 19 20 21 22 23 24 ## 95.59726 93.86828 107.44440 85.12883 92.95603 103.09819 85.52336 86.75193 ## 25 26 27 28 29 30 31 32 ## 85.33025 100.06297 86.03359 85.52559 74.81645 95.52467 92.37138 87.90567 ## 33 34 35 36 37 38 39 40 ## 97.75549 92.06345 84.67978 82.44896 84.29520 91.07649 78.98774 80.30825 .... 4.5.1 Exercise Plot a histogram of the residuals from reg1 using ggplot with a bin width of 5. Is there anything noteworthy about this plot? Show Solution There seems to be a bit of left skewness in the residuals. library(ggplot2) ggplot() + geom_histogram(aes(x = resid(reg1)), binwidth = 5) Calculate the residuals \"by hand\" by subtracting the fitted values from reg1 from the column kid_score in kids. Use the R function all.equal to check that this gives the same result as resid(). Show Solution They give exactly the same result: all.equal(resid(reg1), kids$kid_score - fitted.values(reg1)) ## [1] TRUE As long as you include an intercept in your regression, the residuals will sum to zero. Verify that this is true (up to machine precision!) of the residuals from reg1 Show Solution Close enough! sum(resid(reg1)) ## [1] 1.056155e-12 By construction, the regression residuals are uncorrelated with any predictors included in the regression. Verify that this holds (up to machine precision!) for reg1. Show Solution Again, close enough! cor(resid(reg1), kids$mom_iq) ## [1] 3.121525e-16 4.6 Summarizing The Ouput of lm() To view the \"usual\" summary of regression output, we use the summary() function: summary(reg1) ## ## Call: ## lm(formula = kid_score ~ mom_iq, data = kids) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56.753 -12.074 2.217 11.710 47.691 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.79978 5.91741 4.36 1.63e-05 *** ## mom_iq 0.60997 0.05852 10.42 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.27 on 432 degrees of freedom ## Multiple R-squared: 0.201, Adjusted R-squared: 0.1991 ## F-statistic: 108.6 on 1 and 432 DF, p-value: &lt; 2.2e-16 Among other things, summary shows us the coefficient estimates and associated standard errors for each regressor. It also displays the t-value (Estimate / SE) and associated p-value for a test of the null hypothesis \\(H_0\\colon \\beta = 0\\) versus \\(H_1\\colon \\beta \\neq 0\\). Farther down in the output, summary provides the residual standard error, the R-squared, and the F-statistic and associated p-value for a test of the null hypothesis that all regression coefficients except for the intercept are zero.18 Health warning: by default, lm() computes standard errors and p-values under the classical regression assumptions. In particular, unless you explicitly tell R to do otherwise, it will assume that the regression errors \\(\\varepsilon_i \\equiv Y_i - X_i&#39; \\beta\\) are homoskedastic, and iid. If you're not quite sure what this means, or if you're worried that I'm sweeping important details under the rug, fear not: we'll revisit this in a later lesson. For the moment, let me offer you the following mantra, paraphrasing the wisdom of my favorite professor from grad school: You can always run a [predictive] linear regression; it's inference that requires assumptions. 4.6.1 Exercise Use the kids tibble to run a regression that uses kid_score and mom_hs to predict mom_iq. Store your results in an object called reg_reverse and then display a summary of the regression results. Show Solution reg_reverse &lt;- lm(mom_iq ~ mom_hs + kid_score, kids) summary(reg_reverse) ## ## Call: ## lm(formula = mom_iq ~ mom_hs + kid_score, data = kids) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.046 -10.412 -1.762 8.839 42.714 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 68.86638 2.82454 24.381 &lt; 2e-16 *** ## mom_hs 6.82821 1.58450 4.309 2.03e-05 *** ## kid_score 0.29688 0.03189 9.309 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.16 on 431 degrees of freedom ## Multiple R-squared: 0.234, Adjusted R-squared: 0.2304 ## F-statistic: 65.82 on 2 and 431 DF, p-value: &lt; 2.2e-16 4.7 Tidying up with broom We saw above that lm() returns a list. It turns out that summary(), when applied to an lm() object, also returns a list: str(summary(reg1)) ## List of 11 ## $ call : language lm(formula = kid_score ~ mom_iq, data = kids) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language kid_score ~ mom_iq ## .. ..- attr(*, &quot;variables&quot;)= language list(kid_score, mom_iq) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;kid_score&quot; &quot;mom_iq&quot; ## .. .. .. ..$ : chr &quot;mom_iq&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;mom_iq&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 .... In principle, this gives us a way of extracting particular pieces of information from a table of regression output generated by summary(). For example, if you carefully examine the output of str(summary(reg1)) you'll find a named list element called r.squared. By accessing this element, you can pluck out the R-squared from summary(reg1) as follows: summary(reg1)$r.squared ## [1] 0.2009512 Similarly, you could extract F-statistics and associated degrees of freedom by accessing You could extract the information That wasn't so bad! But now suppose you wanted to extract the estimates, standard errors, and p-values from reg1. While it's possible to do this by poring over the output of str(summary(reg1)), there's a much easier way. The broom package provides some extremely useful functions for extracting regression output. Best of all, the same tools apply to models that we'll meet in later lessons. Use tidy() to create a tibble containing regression estimates, standard errors, t-statistics, and p-values e.g. library(broom) tidy(reg1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 25.8 5.92 4.36 1.63e- 5 ## 2 mom_iq 0.610 0.0585 10.4 7.66e-23 Use glance() to create a tibble that summarizes various measures of model fit: glance(reg1) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.201 0.199 18.3 109. 7.66e-23 1 -1876. 3757. 3769. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Finally, use augment() to create a tibble that merges the tibble you used to run your regression with the corresponding regression fitted values, residuals, etc. augment(reg1, kids) ## # A tibble: 434 × 10 ## kid_score mom_hs mom_iq mom_age .fitted .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 65 1 121. 27 99.7 -34.7 0.00688 18.2 0.0126 ## 2 98 1 89.4 25 80.3 17.7 0.00347 18.3 0.00164 ## 3 85 1 115. 27 96.2 -11.2 0.00475 18.3 0.000905 ## 4 83 1 99.4 25 86.5 -3.46 0.00231 18.3 0.0000416 ## 5 115 1 92.7 27 82.4 32.6 0.00284 18.2 0.00456 ## 6 98 0 108. 18 91.6 6.38 0.00295 18.3 0.000181 ## 7 69 1 139. 20 111. -41.5 0.0178 18.2 0.0478 ## 8 106 1 125. 23 102. 3.86 0.00879 18.3 0.000200 ## 9 102 1 81.6 24 75.6 26.4 0.00577 18.2 0.00611 ## 10 95 1 95.1 19 83.8 11.2 0.00255 18.3 0.000483 ## # … with 424 more rows, and 1 more variable: .std.resid &lt;dbl&gt; Notice that augment() uses a dot \".\" to begin the name of any column that it merges. This avoids potential clashes with columns you already have in your dataset. After all, you'd never start a column name with a dot would you? 4.7.1 Exercise To answer the following, you may need to consult the help files for tidy.lm(), glance.lm(), and augment.lm() from the broom package. Use dplyr and tidy() to display the regression estimate, standard error, t-statistic, and p-value for the predictor kid_score in reg_reverse from above. Show Solution reg_reverse %&gt;% tidy() %&gt;% filter(term == &#39;kid_score&#39;) ## # A tibble: 1 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 kid_score 0.297 0.0319 9.31 6.61e-19 Use ggplot() and augment() to make a scatterplot with the fitted values from reg_reverse on the horizontal axis and mom_iq on the vertical axis. Use geom_abline() to add a 45-degree line to your plot. (You may need to read the help file for this function.) Show Solution augment(reg_reverse, kids) %&gt;% ggplot(aes(x = .fitted, y = mom_iq)) + geom_point() + geom_abline(intercept = 0, slope = 1) Continuing from the preceding exercise, run a regression of mom_iq on the fitted values from reg_reverse and display the estimated regression coefficients. Compare the R-squared of this regression to that of reg_reverse. Explain your results. Show Solution When we regress \\(Y_i\\) on \\(\\widehat{Y}_i\\), the fitted values from a regression of \\(Y_i\\) on \\(X_i\\), we get an intercept of zero and a slope of one: kids_augmented &lt;- augment(reg_reverse, kids) reg_y_vs_fitted &lt;- lm(mom_iq ~ .fitted, kids_augmented) tidy(reg_y_vs_fitted) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.96e-13 8.73 2.25e-14 1.00e+ 0 ## 2 .fitted 1.00e+ 0 0.0871 1.15e+ 1 7.85e-27 This makes sense. Suppose we wanted to choose \\(\\alpha_0\\) and \\(\\alpha_1\\) to minimize \\(\\sum_{i=1}^n (Y_i - \\alpha_0 - \\alpha_1 \\widehat{Y}_i)^2\\) where \\(\\widehat{Y}_i = \\widehat{\\beta}_0 + X_i&#39;\\widehat{\\beta}_1\\). This is equivalent to minimizing \\[ \\sum_{i=1}^n \\left[Y_i - (\\alpha_0 + \\widehat{\\beta}_0) - X_i&#39;(\\alpha_1\\widehat{\\beta}_1)\\right]^2. \\] By construction \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) minimize \\(\\sum_{i=1}^n (Y_i - \\beta_0 - X_i&#39;\\beta_1)^2\\), so unless \\(\\widehat{\\alpha_0} = 0\\) and \\(\\widehat{\\alpha_1} = 1\\) we'd have a contradiction! Similar reasoning explains why the R-squared values for the two regressions are the same: c(glance(reg_reverse)$r.squared, glance(reg_y_vs_fitted)$r.squared) ## [1] 0.2339581 0.2339581 The R-squared of a regression equals \\(1 - \\text{SS}_{\\text{residual}} / \\text{SS}_{\\text{total}}\\) \\[ \\text{SS}_{\\text{total}} = \\sum_{i=1}^n (Y_i - \\bar{Y})^2,\\quad \\text{SS}_{\\text{residual}} = \\sum_{i=1}^n (Y_i - \\widehat{Y}_i^2) \\] The total sum of squares is the same for both regressions because they have the same outcome variable. The residual sum of squares is the same because \\(\\widehat{\\alpha}_0 = 0\\) and \\(\\widehat{\\alpha}_1 = 1\\) together imply that both regressions have the same fitted values. 4.8 Dummy Variables with lm() The column mom_hs in kids is a dummy variable, also known as a binary variable. It equals 1 if a child's mother graduated from college and 0 otherwise. For this reason, the coefficient on mom_hs in the following regression tells us the difference of mean test scores between kids whose mothers graduated from college and those whose mothers did not, while the intercept tells us the mean of kid_score for children whose mothers didn't graduate from high school: lm(kid_score ~ mom_hs, kids) ## ## Call: ## lm(formula = kid_score ~ mom_hs, data = kids) ## ## Coefficients: ## (Intercept) mom_hs ## 77.55 11.77 Although it's represented using the numerical values 0 and 1, mom_hs doesn't actually encode quantitative information. The numerical values are just shorthand for two different categories: mom_hs is a categorical variable. To keep from getting confused, it's good practice to make categorical variables obvious by storing them as character or factor data. Here I create a new column, mom_education, that stores the same information as mom_hs as a factor: kids &lt;- kids %&gt;% mutate(mom_education = if_else(mom_hs == 1, &#39;High School&#39;, &#39;No High School&#39;)) %&gt;% mutate(mom_education = factor(mom_education, levels = unique(mom_education))) The column mom_education is a factor, R's built-in representation of a categorical variable. So what happens if we include mom_education in our regression in place of mom_hs? lm(kid_score ~ mom_education, kids) ## ## Call: ## lm(formula = kid_score ~ mom_education, data = kids) ## ## Coefficients: ## (Intercept) mom_educationNo High School ## 89.32 -11.77 Wait a minute; now the estimate is negative! We can't run a regression that includes an intercept and a coefficient for each level of a dummy variable--this is the dummy variable trap!--so R has excluded one of them. Rather capriciously, lm() has chosen to treat High School as the omitted category. We can override this behavior by using fct_relevel() from the forcats package. The following code tells R that we want 'No High School' to be the first ordered factor level, the level that lm() treats as the omitted category by default: library(forcats) kids &lt;- kids %&gt;% mutate(mom_education = fct_relevel(mom_education, &#39;No High School&#39;)) lm(kid_score ~ mom_education, kids) ## ## Call: ## lm(formula = kid_score ~ mom_education, data = kids) ## ## Coefficients: ## (Intercept) mom_educationHigh School ## 77.55 11.77 In the exercise you'll explore how lm() handles categorical variables that take on more than two values. In your econometrics class you probably learned that we can use two dummy variables to encode a three-valued categorical variable, four dummy variables to encode a four-valued categorical variable, and so on. You may wonder if we need to explicitly construct these dummy variables in R. The answer is no: lm() handles things for us automatically, as you're about to discover. This is worth putting in bold: you don't have to explicitly construct dummy variables in R. The lm() function will construct them for you. 4.8.1 Exercise Read the help file for the base R function cut(). Setting the argument breaks to c(16, 19, 23, 26, 29), use this function in concert with dplyr() to create a factor variable called mom_age_bins to kids. Use the base R function levels() to display the factor levels of mom_age_bins. Show Solution kids &lt;- kids %&gt;% mutate(mom_age_bins = cut(mom_age, c(16, 19, 23, 26, 29))) levels(kids$mom_age_bins) ## [1] &quot;(16,19]&quot; &quot;(19,23]&quot; &quot;(23,26]&quot; &quot;(26,29]&quot; Run a linear regression of kid_score on mom_age_bins and display the coefficients. Explain the results. Show Solution R \"expands\" the factor variable mom_age_bins into a collection of dummy variables before running the regression. It names these dummies in an obvious way based on the levels that mom_age_bins takes on: lm(kid_score ~ mom_age_bins, kids) ## ## Call: ## lm(formula = kid_score ~ mom_age_bins, data = kids) ## ## Coefficients: ## (Intercept) mom_age_bins(19,23] mom_age_bins(23,26] ## 87.054 -2.308 2.293 ## mom_age_bins(26,29] ## 2.232 In words: R has run a regression of kid_score on a constant (the intercept), a dummy for mother's age in the range (19,23], a dummy for mother's age in the range (23,26] and a dummy for mother's age in the range (26,29]. From running levels(kids$mom_age_bins) above, we know this means the omitted category is (16,19]. This corresponds to the intercept in the regression, so the average value of kid_score for a child whose mother's age lies in the range (16,19] is around 87 points. All the other coefficients are differences of means relative to this category of \"teen moms.\" For example, kids whose mothers' age is in the range (19,23] score about two points lower, on average, then kids whose mothers' age is in the range (16,19]. Re-run the preceding regression, but use fct_relevel() from the forcats package make (26,29] the omitted category for mom_age_bins. Show Solution kids &lt;- kids %&gt;% mutate(mom_age_bins = fct_relevel(mom_age_bins, &#39;(26,29]&#39;)) lm(kid_score ~ mom_age_bins, kids) ## ## Call: ## lm(formula = kid_score ~ mom_age_bins, data = kids) ## ## Coefficients: ## (Intercept) mom_age_bins(16,19] mom_age_bins(19,23] ## 89.28571 -2.23214 -4.54043 ## mom_age_bins(23,26] ## 0.06106 4.9 Fun with R Formulas It's time to learn some more about R formulas. But before we do, you may ask \"why bother?\" It's true that you run just about any regression you need using nothing more complicated than + and ~ as introduced above. I know, because I did this for the better part of a decade! But a key goal of this book is showing you how to work smarter rather than harder, both to make your own life easier and help others replicate your work. If you ever plan to fit more than a handful of models with more than a handful of variables, it's worth your time to learn about formulas. You've already met the special symbols ~ and + explained in the following table. In the next few sub-sections, I'll walk you through the others: ., -, 1, :, *, ^, and I(). Symbol Purpose Example In Words ~ separate LHS and RHS of formula y ~ x regress y on x + add variable to a formula y ~ x + z regress y on x and z . denotes \"everything else\" y ~ . regress y on all other variables in a data frame - remove variable from a formula y ~ . - x regress y on all other variables except z 1 denotes intercept y ~ x - 1 regress y on x without an intercept : construct interaction term y ~ x + z + x:z regress y on x, z, and the product x times z * shorthand for levels plus interaction y ~ x * z regress y on x, z, and the product x times z ^ higher order interactions y ~ (x + z + w)^3 regress y on x, z, w, all two-way interactions, and the three-way interactions I() \"as-is\" - override special meanings of other symbols from this table y ~ x + I(x^2) regress y on x and x squared 4.9.1 \"Everything Else\" - The Dot . Sometimes all you want to do is run a regression of one variable on everything else. If you have lots of predictors, typing out all of their names, each separated by a + sign, is painful and error-prone. Fortunately there's a shortcut: the dot . lm(kid_score ~ ., kids) ## ## Call: ## lm(formula = kid_score ~ ., data = kids) ## ## Coefficients: ## (Intercept) mom_hs mom_iq ## -8.571 5.638 0.571 ## mom_age mom_educationHigh School mom_age_bins(16,19] ## 1.155 NA 14.048 ## mom_age_bins(19,23] mom_age_bins(23,26] ## 7.205 7.631 This command tells R to regress kid_score on everything else in kids. Notice that the coefficient mom_educationHigh School is NA, in other words missing. This is because you can't run a regression on mom_hs and mom_education at the same time: they're two versions of exactly the same information and hence are perfectly co-linear, so R drops one of them. We'll encounter the dot in many guises later in this lesson and elsewhere. Wherever you see it, replace it mentally with the word \"everything\" and you'll never be confused. The rest will be clear from context. 4.9.2 Removing Predictors with - The regression we ran above using the dot . was very silly: it included both mom_hs and mom_education, and it also included both mom_age_bins and mom_age. Suppose we wanted to run a regression of kid_score on everything except mom_age_bins and mom_education. This is easy to achieve using the minus sign - as follows: lm(kid_score ~ . - mom_age_bins - mom_education, kids) ## ## Call: ## lm(formula = kid_score ~ . - mom_age_bins - mom_education, data = kids) ## ## Coefficients: ## (Intercept) mom_hs mom_iq mom_age ## 20.9847 5.6472 0.5625 0.2248 Think of + as saying \"add me to the regression\" and - as saying \"remove me from the regression.\" This use of - is very similar to what you've seen in the select() function from dplyr. And as in dplyr, we can use it to remove one variable or more than one. 4.9.3 The Intercept: 1 It almost always makes sense to include an intercept when you run a linear regression. Without one, we're forced to predict that \\(Y\\) will be zero when \\(X\\) is zero. Because this is usually a bad idea, lm() includes an intercept by default: lm(kid_score ~ mom_iq, kids) ## ## Call: ## lm(formula = kid_score ~ mom_iq, data = kids) ## ## Coefficients: ## (Intercept) mom_iq ## 25.80 0.61 In some special cases, however, we may have a reason to run a regression without an intercept. R's formula syntax denotes the intercept by 1. Armed with this knowledge, we can remove it from our regression using - as introduced above: lm(kid_score ~ mom_iq - 1, kids) ## ## Call: ## lm(formula = kid_score ~ mom_iq - 1, data = kids) ## ## Coefficients: ## mom_iq ## 0.8623 Another situation in which we may wish to remove the intercept is when running a regression with a categorical variable. We can't include an intercept and a coefficient for each value of a categorical variable in our regression: this is the dummy variable trap. We either have to drop one level of the categorical variable (the baseline or omitted category) or drop the intercept. Above we saw how to choose which category to omit. But another option is to drop the intercept. In the first regression, the intercept equals the mean of kid_score for the omitted category mom_education == \"No High School\" while the intercept gives the difference of means: lm(kid_score ~ mom_education, kids) ## ## Call: ## lm(formula = kid_score ~ mom_education, data = kids) ## ## Coefficients: ## (Intercept) mom_educationHigh School ## 77.55 11.77 In the second, we obtain the mean of kid_score for each group: lm(kid_score ~ mom_education - 1, kids) ## ## Call: ## lm(formula = kid_score ~ mom_education - 1, data = kids) ## ## Coefficients: ## mom_educationNo High School mom_educationHigh School ## 77.55 89.32 4.9.4 Exercise Run a regression of mom_hs on everything in kids except mom_age and mom_hs. Show Solution lm(kid_score ~ . - mom_age - mom_hs, kids) ## ## Call: ## lm(formula = kid_score ~ . - mom_age - mom_hs, data = kids) ## ## Coefficients: ## (Intercept) mom_iq mom_educationHigh School ## 23.2831 0.5688 6.0147 ## mom_age_bins(16,19] mom_age_bins(19,23] mom_age_bins(23,26] ## 3.7181 0.3138 4.4481 Write dplyr code to verify that lm(kid_score ~ mom_education - 1, kids) does indeed calculate mean of kid_score for each group, as asserted. Show Solution kids %&gt;% group_by(mom_education) %&gt;% summarize(mean(kid_score)) ## # A tibble: 2 × 2 ## mom_education `mean(kid_score)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 No High School 77.5 ## 2 High School 89.3 What do you get if you run the regression lm(kid_score ~ 1, kids)? Explain. Show Solution This is a regression with only an intercept, so it calculates the sample mean of kid_score lm(kid_score ~ 1, kids) ## ## Call: ## lm(formula = kid_score ~ 1, data = kids) ## ## Coefficients: ## (Intercept) ## 86.8 kids %&gt;% summarize(mean(kid_score)) ## # A tibble: 1 × 1 ## `mean(kid_score)` ## &lt;dbl&gt; ## 1 86.8 See the exercise earlier in this lesson for a mathematical proof that regression with only an intercept is equivalent to computing the sample mean of \\(Y\\). Run a regression of kid_score on mom_age_bins and mom_iq, but rather than including an intercept and an omitted category fit a separate coefficient for each level of mom_age_bins. Show Solution lm(kid_score ~ mom_age_bins + mom_iq - 1, kids) ## ## Call: ## lm(formula = kid_score ~ mom_age_bins + mom_iq - 1, data = kids) ## ## Coefficients: ## mom_age_bins(26,29] mom_age_bins(16,19] mom_age_bins(19,23] ## 23.9233 26.1812 23.8011 ## mom_age_bins(23,26] mom_iq ## 28.2905 0.6139 4.9.5 Transforming Outcomes and Predictors What if you wanted to regress the logarithm of kid_score on mom_age and mom_age^2? One way to do this is by creating a new data frame: new_kids &lt;- kids %&gt;% mutate(log_kid_score = log(kid_score), mom_age_sq = mom_age^2) lm(log_kid_score ~ mom_age + mom_age_sq, new_kids) ## ## Call: ## lm(formula = log_kid_score ~ mom_age + mom_age_sq, data = new_kids) ## ## Coefficients: ## (Intercept) mom_age mom_age_sq ## 4.4586761 -0.0109575 0.0004214 It worked! But that required an awful lot of typing. What's more, I had to clutter up my R environment with another data frame: new_kids. A more elegant approach uses R's formula syntax to do all the heavy lifting. First I'll show you the syntax and then I'll explain it: lm(log(kid_score) ~ mom_age + I(mom_age^2), kids) ## ## Call: ## lm(formula = log(kid_score) ~ mom_age + I(mom_age^2), data = kids) ## ## Coefficients: ## (Intercept) mom_age I(mom_age^2) ## 4.4586761 -0.0109575 0.0004214 The key point here is that we can use functions within an R formula. When lm() encounters log(kid_score) ~ mom_age + I(mom_age^2) it looks at the data frame kids, and then parses the formula to construct all the variables that it needs to run the regression. There's no need for us to construct and store these in advance: R does everything for us. Remember how lm() automatically constructed all the dummy variables required for a regression with a categorical predictor in the exercise from above? Roughly the same thing is going on here. The only awkward part is the function I(). What on earth is this mean?! Formulas have their own special syntax: a + inside a formula doesn't denote addition and a . doesn't indicate a decimal point. The full set of characters that have a \"special meaning\" within an R formula is as follows: ~ + . - : * ^ |. We've already met the first four of these; we'll encounter the next three in the following section. If you want to use any of these characters in their ordinary meaning you need to \"wrap them\" in an I(). In words, I() means \"as-is,\" in other words \"don't treat the things inside these parentheses as formula syntax; treat them as you would a plain vanilla R expression. Since ^ has a special meaning within a formula, we need to wrap mom_age^2 inside of I() to include the square of mom_age. We don't have to wrap log(kid_score) inside of I() because log() isn't one of the special characters ~ + . - : * ^ |. 4.9.6 Adding Interactions With :, *, and ^ An interaction is a predictor that is constructed by taking the product of two \"basic\" predictors: for example \\(X_1 \\times X_2\\). In R formula syntax we use a colon : to denote an interaction, for example: lm(kid_score ~ mom_age:mom_iq, kids) ## ## Call: ## lm(formula = kid_score ~ mom_age:mom_iq, data = kids) ## ## Coefficients: ## (Intercept) mom_age:mom_iq ## 48.04678 0.01698 runs a regression of kid_score on the product of mom_age and mom_iq. This is a rather strange regression to run. Unless you have a very good reason for doing so, it's strange to use the product of mom_age and mom_iq to predict kid_score without using the individual variables as well. In statistical parlance, the coefficient on a predictor that is not an interaction, is often called a main effect. And as a rule of thumb, just as it rarely makes sense to run a regression without an intercept, it rarely makes sense to include an interaction term without including the associated main effects. Taking this to heart, we can run a regression that includes mom_age and mom_iq in addition to their interaction as follows: coef(lm(kid_score ~ mom_age + mom_iq + mom_age:mom_iq, kids)) ## (Intercept) mom_age mom_iq mom_age:mom_iq ## -32.69120826 2.55287614 1.09946961 -0.02130665 Because including two variables as main effects along with their interaction is such a common pattern in practice, R's formula syntax include a special shorthand symbol for this: *. Somewhat confusingly, the expression x1 * x2 within an R formula does not refer to the product of x1 and x2. Instead it denotes x1 + x2 + x1:x2. For example, we could run exactly the same regression as above with less typing as follows: coef(lm(kid_score ~ mom_age * mom_iq, kids)) ## (Intercept) mom_age mom_iq mom_age:mom_iq ## -32.69120826 2.55287614 1.09946961 -0.02130665 Both : and * can be chained together to create higher-order interactions. For example, the following command regresses kid_score on the three-way interaction of mom_hs, mom_iq, and mom_age coef(lm(kid_score ~ mom_hs:mom_iq:mom_age, kids)) ## (Intercept) mom_hs:mom_iq:mom_age ## 74.860699531 0.006427933 If we want to include the corresponding levels, and two-way interactions alongside the three-way interaction, we can achieve this by chaining the * symbol as follows: coef(lm(kid_score ~ mom_hs * mom_iq * mom_age, kids)) ## (Intercept) mom_hs mom_iq ## -123.19702677 128.77032321 2.29825977 ## mom_age mom_hs:mom_iq mom_hs:mom_age ## 5.23979255 -1.62184546 -3.72860045 ## mom_iq:mom_age mom_hs:mom_iq:mom_age ## -0.06249009 0.05390860 The caret ^ is another shorthand symbol for running regressions with interactions. I rarely use it myself, but you may come across it occasionally. The syntax (x1 + x2)^2 is equivalent to x1 * x2, for example: coef(lm(kid_score ~ (mom_age + mom_iq)^2, kids)) ## (Intercept) mom_age mom_iq mom_age:mom_iq ## -32.69120826 2.55287614 1.09946961 -0.02130665 and (x1 + x2 + x2)^3 is equivalent to x1 * x2 * x3 coef(lm(kid_score ~ (mom_hs + mom_iq + mom_age)^3, kids)) ## (Intercept) mom_hs mom_iq ## -123.19702677 128.77032321 2.29825977 ## mom_age mom_hs:mom_iq mom_hs:mom_age ## 5.23979255 -1.62184546 -3.72860045 ## mom_iq:mom_age mom_hs:mom_iq:mom_age ## -0.06249009 0.05390860 One place where ^ can come in handy is if you want to include levels and all two way interactions of more than two variables. For example, coef(lm(kid_score ~ (mom_hs + mom_iq + mom_age)^2, kids)) ## (Intercept) mom_hs mom_iq mom_age mom_hs:mom_iq ## -29.68240313 15.83347387 1.30109421 0.94781914 -0.43693774 ## mom_hs:mom_age mom_iq:mom_age ## 1.39044445 -0.01656256 is a more compact way of specifying coef(lm(kid_score ~ mom_hs * mom_iq * mom_age - mom_hs:mom_iq:mom_age, kids)) ## (Intercept) mom_hs mom_iq mom_age mom_hs:mom_iq ## -29.68240313 15.83347387 1.30109421 0.94781914 -0.43693774 ## mom_hs:mom_age mom_iq:mom_age ## 1.39044445 -0.01656256 4.9.7 Exercise Run a regression of kid_score on mom_iq and mom_iq^2 and display a tidy summary of the regression output, including standard errors. Show Solution tidy(lm(kid_score ~ mom_iq + I(mom_iq^2), kids)) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -99.0 37.3 -2.65 0.00823 ## 2 mom_iq 3.08 0.730 4.21 0.0000307 ## 3 I(mom_iq^2) -0.0119 0.00352 -3.39 0.000767 Use ggplot2 to make a scatter-plot with mom_iq on the horizontal axis and kid_score on the vertical axis. Add the quadratic regression function from the first part. Show Hint Add the argument formula = y ~ x + I(x^2) to geom_smooth(). Show Solution ggplot(kids, aes(x = mom_iq, y = kid_score)) + geom_point() + geom_smooth(method = &#39;lm&#39;, formula = y ~ x + I(x^2)) Based on the results of the preceding two parts, is there any evidence that the slope of the predictive relationship between kid_score and mom_iq varies with mom_iq? Show Solution Yes: the estimated coefficient on mom_iq^2 is highly statistically significant, and from the plot we see that there is a \"practically significant\" amount of curvature in the predictive relationship. Suppose we wanted to run a regression of kid_score on mom_iq, mom_hs, and their interaction. Write down three different ways of specifying this regression using R's formula syntax. Show Solution kid_score ~ mom_iq + mom_hs + mom_iq:mom_hs kid_score ~ mom_iq * mom_hs kid_score ~ (mom_iq + mom_hs)^2 Run a regression of kid_score on mom_hs, mom_iq and their interaction. Display a tidy summary of the estimates, standard errors, etc. Is there any evidence that the predictive relationship between kid_score and mom_iq varies depending on whether a given child's mother graduated from high school? Explain. Show Solution tidy(lm(kid_score ~ mom_iq * mom_hs, kids)) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -11.5 13.8 -0.835 4.04e- 1 ## 2 mom_iq 0.969 0.148 6.53 1.84e-10 ## 3 mom_hs 51.3 15.3 3.34 9.02e- 4 ## 4 mom_iq:mom_hs -0.484 0.162 -2.99 2.99e- 3 Let's introduce a bit of notation to make things clearer. The regression specification is as follows: \\[ \\texttt{kid_score} = \\widehat{\\alpha} + \\widehat{\\beta} \\times \\texttt{mom_hs} + \\widehat{\\gamma} \\times \\texttt{mom_iq} + \\widehat{\\delta} \\times (\\texttt{mom_hs} \\times \\texttt{mom_iq}) \\] Now, mom_hs is a dummy variable that equals 1 if a child's mother graduated from high school For a child whose mother did not graduate from high school, our prediction becomes \\[ \\texttt{kid_score} = \\widehat{\\alpha} + \\widehat{\\gamma} \\times \\texttt{mom_iq} \\] compared to the following for a child whose mother did graduate from high school: \\[ \\texttt{kid_score} = (\\widehat{\\alpha} + \\widehat{\\beta}) + (\\widehat{\\gamma} + \\widehat{\\delta})\\times \\texttt{mom_iq} \\] Thus, the coefficient \\(\\widehat{\\delta}\\) on the interaction mom_iq:mom_hs is the difference of slopes: high-school minus no high school. The point estimate is negative, large, and highly statistically significant. We have fairly strong evidence that the predictive relationship between kid_score and mom_iq is less steep for children whose mothers attended high school. For more discussion of this example, see this document. If you're rusty on the F-test, this may help.↩︎ "],["predictive-regression-part-ii.html", "Lesson 5 Predictive Regression Part II 5.1 Regressions Used Below 5.2 Predicting New Observations 5.3 Testing a Linear Restriction 5.4 Heteroskedasticity-Robust Standard Errors and Tests 5.5 Publication Quality Tables", " Lesson 5 Predictive Regression Part II In our last lesson we covered the basics of predictive linear regression using lm(). We learned how to use formulas to set up different regression models, how to use summary() and broom() to display and format the results, and how to make simple plots of the regression line using ggplot(). We applied these tools to a simple dataset of kids test scores and discussed the correct way to interpret the results. In this lesson we'll complete our discussion of predictive linear regression by answering some frequently asked questions: How can I extract predictions from a fitted regression model? I need to test a linear restriction. Is there a way to do this in R? What about heteroskedasticity? Can I use R to compute robust standard errors? Journal articles have beautifully-formatted tables of regression results. How can I make one? 5.1 Regressions Used Below At several points in this lesson we'll find it helpful to have a few different sets of regression results at our fingertips, so I'll start by fitting three regressions from our last lesson, using the child test score dataset: reg_pooled is a regression of kid_score on mom_iq that pools children whose mothers graduated high school with those whose mothers did not. reg_hs_dummy is a regression of kid_score on mom_iq and mom_hs. Recall that mom_hs is a dummy variable that equals one if a child's mother attended high school. reg_interact is a regression of kid_score on mom_iq, mom_hs, and their interaction. library(tidyverse) kids &lt;- read_csv(&quot;http://ditraglia.com/data/child_test_data.csv&quot;) kids &lt;- kids %&gt;% rename(kid_score = kid.score, mom_hs = mom.hs, mom_iq = mom.iq, mom_age = mom.age) reg_pooled &lt;- lm(kid_score ~ mom_iq, kids) reg_hs_dummy &lt;- lm(kid_score ~ mom_iq + mom_hs, kids) reg_interact &lt;- lm(kid_score ~ mom_iq * mom_hs, kids) 5.2 Predicting New Observations The predict() command allows us to predict new observations using a fitted statistical model. As we'll see in future lessons, predict() works with a variety of models besides linear regression. In this lesson we'll restrict our attention to making predictions based on a linear regression fitted with lm(). The simplest way to use predict() is by passing it a single argument: a regression fitted with lm(). When used in this way, it returns the vector of fitted values from the regression. As such predict([REGRESSION]) is synonymous with fitted.values([REGRESSION]): yhat1 &lt;- predict(reg_hs_dummy) yhat2 &lt;- fitted.values(reg_hs_dummy) head(cbind(yhat1, yhat2)) ## yhat1 yhat2 ## 1 99.98056 99.98056 ## 2 82.07336 82.07336 ## 3 96.78075 96.78075 ## 4 87.76191 87.76191 ## 5 83.98152 83.98152 ## 6 86.57804 86.57804 This make sense if you think of the fitted values as in-sample predictions, the predictions that we would make for the individuals we used to fit our regression. To make predictions for new individuals, we need to supply a second argument to predict(), a data frame called newdata. This data frame should have the same column names as the data we used to fit our regression, and contain the same kind of information. In short: newdata contains the predictors \\(X\\) for one or more individuals who are not in our dataset; given this information, predict() extracts the estimated regression coefficients \\(\\widehat{\\beta}\\) and computes \\(X\\widehat{\\beta}\\). Let's try a simple example. Suppose we wanted to use reg_pooled_hs to predict kid_score for a child whose mother has an IQ of 100 and did not graduate from high school. One way to do this is \"by hand\" using coef() b_hat &lt;- coef(reg_hs_dummy) b_hat ## (Intercept) mom_iq mom_hs ## 25.731538 0.563906 5.950117 b_hat[1] + b_hat[2] * 100 + 0 * b_hat[3] ## (Intercept) ## 82.12214 Another is by first constructing a data frame with information on this particular hypothetical child and passing it as the newdata argument of predict(): new_kid &lt;- data.frame(mom_iq = 100, mom_hs = 0) new_kid ## mom_iq mom_hs ## 1 100 0 predict(reg_hs_dummy, new_kid) ## 1 ## 82.12214 The real value of the second approach becomes apparent when we want to make multiple predictions at once. For example, new_kids &lt;- data.frame(mom_iq = c(100, 120, 80), mom_hs = c( 0, 1, 1)) predict(reg_hs_dummy, new_kids) ## 1 2 3 ## 82.12214 99.35038 76.79414 It's crucial that the names of the columns in newdata match those of the regression that you pass as the first argument to predict(). This, for example will throw an error: other_kids &lt;- data.frame(iq = c(100, 120, 80), hs = c( 0, 1, 1)) predict(reg_hs_dummy, other_kids) ## Error in eval(predvars, data, env): object &#39;mom_iq&#39; not found Including extraneous columns in newdata, on the other hand, is perfectly fine. This can come in handy when you want to predict from multiple different fitted regressions. For example, it's perfectly fine to predict using reg_pooled and new_kids even though reg_pooled does not include mom_hs as a predictor: predict(reg_pooled, new_kids) ## 1 2 3 ## 86.79724 98.99673 74.59774 Before I turn you loose on the exercise, here's one more useful trick that doesn't technically count as a use of predict(), but is closely related. In our last lesson we learned how to use augment() from the broom package to add a column with residuals and fitted values to a data frame that we used to fit a regression. It turns out that we can use the same idea to add predicted values to a data frame of \\(X\\) values at which we'd like to predict. For example, to add the result of running predict(reg_pooled, new_kids) as an additional column of new_kids, we can run the following line of code: library(broom) augment(reg_pooled, newdata = new_kids) ## # A tibble: 3 × 3 ## mom_iq mom_hs .fitted ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 0 86.8 ## 2 120 1 99.0 ## 3 80 1 74.6 The third column is still called .fitted but in this case it actually contains predicted values. Note that these agree perfectly with those that we computed above. To avoid confusion, it's worth renaming this column using the rename() command from dplyr augment(reg_pooled, newdata = new_kids) %&gt;% rename(kid_score_pred = .fitted) ## # A tibble: 3 × 3 ## mom_iq mom_hs kid_score_pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 0 86.8 ## 2 120 1 99.0 ## 3 80 1 74.6 5.2.1 Exercise What values of kid_score would we predict for the children in the data frame new_kids based on the results of the regression reg_interact? Show Solution predict(reg_interact, new_kids) ## 1 2 3 ## 85.40690 97.93995 78.55537 Approximately 80% of the children in our kids dataset have mothers who graduated high school. The average IQ of these mothers is around 102 with a standard deviation of 15. For the 20% of mothers who did not graduate, the average IQ is around 92 with a standard deviation of 13. Under the assumption that the IQ for each group of mothers follows a normal distribution in the population as a whole, use this information to generate 10,000 simulated kids. Store their values of mom_iq and mom_hs as the columns of a data frame called sim_kids. Plot a histogram of the simulated values of mom_iq. How do they compare to the true values of mom_iq from kids? Show Hint First construct a vector of simulated mom_hs values using rbinom(). Call it mom_hs_sim. Next use ifelse() to construct a vector of means, mu, and standard deviations, sigma, that correspond to mom_hs_sim. Pass these as arguments to rnorm() to construct the simulated values of mom_iq. Show Solution n_sim_kids &lt;- 1e4 set.seed(54321) mom_hs_sim &lt;- rbinom(n_sim_kids, 1, 0.8) mu &lt;- ifelse(mom_hs_sim == 0, 92, 102) sigma &lt;- ifelse(mom_hs_sim == 0, 13, 15) mom_iq_sim &lt;- rnorm(n_sim_kids, mean = mu, sd = sigma) sim_kids &lt;- data.frame(mom_iq = mom_iq_sim, mom_hs = mom_hs_sim) # Here I *store* the plot rather than displaying it, so I can # put it next to the plot of the true mom_iq values sim_mom_iq_plot &lt;- sim_kids %&gt;% ggplot() + geom_histogram(aes(x = mom_iq), bins = 30) + ggtitle(&#39;simulated&#39;) # Again, I *store* the plot rather than displaying it true_mom_iq_plot &lt;- kids %&gt;% ggplot() + geom_histogram(aes(x = mom_iq), bins = 15) + ggtitle(&#39;true&#39;) # grid.arrange() from the gridExtra package is a handy way of # arranging multiple ggplots in a grid library(gridExtra) grid.arrange(sim_mom_iq_plot, true_mom_iq_plot, ncol = 2) The distribution of mom_iq_sim is roughly symmetric and unimodel. The actual distribution of mom_iq clearly isn't. Use augment() from broom too add a column called kid_score_pred to sim_kids, containing the predicted test scores for these simulated children based on the fitted regression reg_interact. Plot a histogram of kid_score_pred and compare it to that of kid_score from kids. Show Solution # Store rather than display this plot kid_score_pred_plot &lt;- augment(reg_interact, newdata = sim_kids) %&gt;% rename(kid_score_pred = .fitted) %&gt;% ggplot() + geom_histogram(aes(x = kid_score_pred), bins = 30) + ggtitle(&#39;simulated&#39;) # Store rather than display this plot kid_score_plot &lt;- kids %&gt;% ggplot() + geom_histogram(aes(x = kid_score), bins = 20) + ggtitle(&#39;true&#39;) # As above: a handy way of arranging multiple ggplots # in a grid layout grid.arrange(kid_score_pred_plot, kid_score_plot, ncol = 2) The distribution of kid_score_pred is clearly left skewed, like that of kid_score. But there are some important differences: our simulation produces values of kid_score that are too tightly clustered around the mode. In the real data, scores below 50 and above 100 are common, but in the simulation they're extremely rare. 5.3 Testing a Linear Restriction Null hypothesis significance testing (NHST) is widely-used, and widely abused. In a future lesson I'll try to convince you that it's rarely of much practical use in applied work.19 That said, there are some special situations where a hypothesis test can come in handy. More importantly, if you want to replicate the work of researchers who haven't gotten the memo on statistical significance, you need to know something about NHST. In this section we'll learn how to carry out classical test of a linear restriction. For the moment I will assume that the regression errors are homoskedastic. Later in this lesson we'll learn how to adjust for heteroskedasticity. The regression output provided by summary(), or in a tidier form by tidy(), makes it easy to test a null hypothesis of the form \\(H_0\\colon \\beta_j = 0\\) against the two-sided alternative \\(H_1 \\colon \\beta_j \\neq 0\\). Not only do they provide the point estimates \\(\\widehat{\\beta}_j\\) and associated standard errors \\(\\text{SE}_j\\), they also provide the t-statistic \\(\\widehat{\\beta}_j/\\text{SE}_j\\) and p-value for this very null hypothesis! In this example, we would soundly reject the null hypothesis that the coefficient on mom_iq is zero: tidy(reg_pooled) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 25.8 5.92 4.36 1.63e- 5 ## 2 mom_iq 0.610 0.0585 10.4 7.66e-23 For any other test that we might want to carry out, however, this output is less helpful. Suppose that we wanted to test \\(H_0\\colon \\beta_j = b\\) against the two-sided alternative where \\(b\\) is some constant that does not equal zero. We could of course construct the t-statistic \"by hand\" using the regression output. For example, we construct the test statistic for a test of the null hypothesis that the coefficient on mom_iq equals 1 as follows: results_pooled &lt;- tidy(reg_pooled) b &lt;- 1 test_stat &lt;- with(results_pooled, abs(estimate[2] - b) / std.error[2]) test_stat ## [1] 6.664718 From there we could calculate a p-value if desired. This approach works, but it's tedious. It also leaves us with ample opportunity to make a careless error in the calculation. Fortunately there's a better way! Testing \\(H_0\\colon \\beta_j = b\\) is a special case of the more general problem of testing a linear restriction. In full generality, a test of a linear restriction takes the form \\[ H_0\\colon \\mathbf{R}\\beta = \\mathbf{q} \\quad \\text{versus} \\quad H_1\\colon \\mathbf{R}\\beta \\neq \\mathbf{q} \\] where \\(\\mathbf{R}\\) is a \\(J \\times K\\) matrix of constants, \\(\\mathbf{q}\\) is \\(J\\)-vector of constants, and \\(\\beta\\) is the regression parameter vector. If you haven't seen this before or have nightmares because you have seen it before, don't be put off: the only reason I'm showing you this notation is to clarify which kinds of hypotheses constitute a linear restriction. Here are a some examples. Each of them can be written in the form given above by choosing the appropriate \\(\\mathbf{R}\\) matrix and \\(\\mathbf{q}\\) vector: \\(H_0\\colon \\beta_j = 2\\) \\(H_0\\colon \\beta_1 = \\beta_2\\) \\(H_0\\colon \\beta_1 + \\beta_2 + \\beta_3 = 1\\) \\(H_0\\colon \\beta_1 = 0 \\text{ and } \\beta_2 = \\beta_3\\) The linearHypothesis() command from the car package makes it easy to conduct a Wald test of a linear restriction based on a regression model fitted with lm(). The first argument of linearHypothesis() is the unrestricted model, a regression that we've already fit using lm().20 The second argument is a description of the hypothesis we wish to test. For example to test that the coefficient on mom_iq equals 1, we can use the following command: library(car) linearHypothesis(reg_pooled, &#39;mom_iq = 1&#39;) ## Linear hypothesis test ## ## Hypothesis: ## mom_iq = 1 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 433 158958 ## 2 432 144137 1 14820 44.419 8.107e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To test that the intercept equals zero, a very silly hypothesis indeed, we simply need to replace 'mom_iq = 1' with '(Intercept) = 0' linearHypothesis(reg_pooled, &#39;(Intercept) = 1&#39;) ## Linear hypothesis test ## ## Hypothesis: ## (Intercept) = 1 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 433 149998 ## 2 432 144137 1 5860.4 17.564 3.37e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To test the joint null hypothesis that both of these restrictions hold simultaneously, we supply a vector as the second argument. Each element of the vector describes one of the restrictions: linearHypothesis(reg_pooled, c(&#39;mom_iq = 1&#39;, &#39;(Intercept) = 1&#39;)) ## Linear hypothesis test ## ## Hypothesis: ## mom_iq = 1 ## (Intercept) = 1 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 434 246503 ## 2 432 144137 2 102366 153.4 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 By default linearHypothesis() carries out inference based on the F statistic. This is the \"finite sample\" Wald test. If you prefer the asymptotic version, based on a Chi-squared statistic, set the option test = 'Chisq', for example linearHypothesis(reg_pooled, test = &#39;Chisq&#39;, c(&#39;mom_iq = 1&#39;, &#39;(Intercept) = 1&#39;)) ## Linear hypothesis test ## ## Hypothesis: ## mom_iq = 1 ## (Intercept) = 1 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq ## ## Res.Df RSS Df Sum of Sq Chisq Pr(&gt;Chisq) ## 1 434 246503 ## 2 432 144137 2 102366 306.81 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In this example it makes hardly any difference wich version of the test we use. 5.3.1 Exercise Test the joint null hypothesis that the slope and intercept of the predictive relationship between kid_score and mom_iq is the same for kids whose mothers graduated from high school and those whose mothers did not. Does the p-value change much if you use the asymptotic version of the test rather than the finite-sample F? Show Solution tidy(reg_interact) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -11.5 13.8 -0.835 4.04e- 1 ## 2 mom_iq 0.969 0.148 6.53 1.84e-10 ## 3 mom_hs 51.3 15.3 3.34 9.02e- 4 ## 4 mom_iq:mom_hs -0.484 0.162 -2.99 2.99e- 3 linearHypothesis(reg_interact, c(&#39;mom_hs = 0&#39;, &#39;mom_iq:mom_hs = 0&#39;)) ## Linear hypothesis test ## ## Hypothesis: ## mom_hs = 0 ## mom_iq:mom_hs = 0 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq * mom_hs ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 432 144137 ## 2 430 138879 2 5258.7 8.1411 0.0003386 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 linearHypothesis(reg_interact, test = &#39;Chisq&#39;, c(&#39;mom_hs = 0&#39;, &#39;mom_iq:mom_hs = 0&#39;)) ## Linear hypothesis test ## ## Hypothesis: ## mom_hs = 0 ## mom_iq:mom_hs = 0 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq * mom_hs ## ## Res.Df RSS Df Sum of Sq Chisq Pr(&gt;Chisq) ## 1 432 144137 ## 2 430 138879 2 5258.7 16.282 0.0002913 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let n be the number of rows in kids. Generate two random vectors as follows: x is a vector of n independent standard normal noise while z equals mom_iq plus a vector of independent standard normal noise. Carry out a new regression, reg_augmented, that augments reg_interact by adding the predictors x and z. Then carry out an F-test the null hypothesis that x and z are irrelevant for predicting mpg after taking into account mom_iq and mom_hs. Interpret your findings. Do the results of the test make sense? Show Solution set.seed(54321) n &lt;- nrow(kids) x &lt;- rnorm(n) z &lt;- kids$mom_iq + rnorm(n) reg_augmented &lt;- lm(kid_score ~ mom_iq * mom_hs + x + z, kids) tidy(reg_augmented) ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -11.3 13.8 -0.818 0.414 ## 2 mom_iq 1.32 0.903 1.46 0.146 ## 3 mom_hs 51.2 15.4 3.33 0.000937 ## 4 x 0.839 0.849 0.988 0.324 ## 5 z -0.349 0.881 -0.396 0.692 ## 6 mom_iq:mom_hs -0.483 0.163 -2.97 0.00311 linearHypothesis(reg_augmented, c(&#39;x = 0&#39;, &#39;z = 0&#39;)) ## Linear hypothesis test ## ## Hypothesis: ## x = 0 ## z = 0 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq * mom_hs + x + z ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 430 138879 ## 2 428 138497 2 381.94 0.5902 0.5547 These results make sense: x was randomly generated so it clearly has nothing to do with kid_score. In contrast, because it's related to mom_iq, z clearly is related to kid_score, but doesn't contain and additional predictive information beyond that already contained in mom_iq. 5.4 Heteroskedasticity-Robust Standard Errors and Tests Consider a linear regression model of the form \\(Y_i = X_i&#39;\\beta + epsilon_i\\). If the variance of \\(\\epsilon_i\\) is unrelated to the predictors \\(X_i\\), we say that the regression errors are homoskedastic. This is just a fancy Greek work for constant variance. If instead, the variance of \\(\\epsilon_i\\) depends on the value of \\(x_i\\), we say that the regression errors are heteroskedastic. This is just a fancy Greek word for non-constant variance. Heteroskedasticity does not invalidate our least squares estimates of \\(\\beta\\), but it does invalidate the formulas used by lm() to calculate standard errors and p-values. Fortunately there's a simple solution. In this lesson I'll show you how to implement the solution without explaining how it works. In a future lesson we'll take a closer look at robust standard errors The command lm_robust() from the estimatr package works exactly like lm() except that it uses heteroskedasticity-robust standard errors by default. If you're familiar with Stata, lm_robust() is the equivalent of reg robust. Let's test this out on our model kid_score ~ mom_iq * mom_hs from above. Notice that the coefficients in reg_interact_robust are identical to those from reg_interact library(estimatr) reg_interact_robust &lt;- lm_robust(kid_score ~ mom_iq * mom_hs, kids) cbind(lm = coef(reg_interact), lm_robust = coef(reg_interact_robust)) ## lm lm_robust ## (Intercept) -11.4820211 -11.4820211 ## mom_iq 0.9688892 0.9688892 ## mom_hs 51.2682234 51.2682234 ## mom_iq:mom_hs -0.4842747 -0.4842747 The standard errors, on the other hand, are different: data.frame(tidy(reg_interact)[c(&#39;term&#39;, &#39;std.error&#39;)], robust.std.error = tidy(reg_interact_robust)$std.error) ## term std.error robust.std.error ## 1 (Intercept) 13.7579737 13.4927605 ## 2 mom_iq 0.1483437 0.1443046 ## 3 mom_hs 15.3375805 15.1188068 ## 4 mom_iq:mom_hs 0.1622171 0.1581182 In this example the heteroskedasticity-robust standard errors are slightly smaller, although the difference is too small to be of practical relevance. As a general rule, correcting for possible heteroskedasticity tends to increase our standard errors, but this is not always the case, as we see from the preceding example. There are actually various different \"flavors\" of heteroskedasticity-robust standard errors: HC0, HC1, HC2, and HC3. We'll have more to say about these in a future lesson. For now, the important thing to know is that lm_robust() defaults to HC2 whereas Stata defaults to HC1. To obtain a different flavor of robust standard error, you can set the se_type argument. To obtain standard errors that match those computed by reg robust in Stata you can either set se_type = 'HC1' or se_type = 'stata'. They do the same thing, but the latter is easier to remember. Heteroskedasticity doesn't just invalidate inference based on the t-tests from the lm summary output; it also invalidates any that we carry out by passing these results to linearHypothesis. Fortunately, there's an easy fix: as long as we fit our regression using lm_robust in place of lm, linearHypothesis will automatically carry out a heteroskedasticity-robust test. For example: linearHypothesis(reg_interact_robust, &#39;mom_iq = 0.8&#39;) ## Linear hypothesis test ## ## Hypothesis: ## mom_iq = 0.8 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq * mom_hs ## ## Res.Df Df Chisq Pr(&gt;Chisq) ## 1 431 ## 2 430 1 1.3698 0.2419 Because the robust test relies on an asymptotic result, it uses a \\(\\chi^2\\) distribution. The non-robust test based on reg_interact is slightly different even if we set test = 'Chisq' linearHypothesis(reg_interact, &#39;mom_iq = 0.8&#39;, test = &#39;Chisq&#39;) ## Linear hypothesis test ## ## Hypothesis: ## mom_iq = 0.8 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq * mom_hs ## ## Res.Df RSS Df Sum of Sq Chisq Pr(&gt;Chisq) ## 1 431 139297 ## 2 430 138879 1 418.63 1.2962 0.2549 Although both of these tests use a \\(\\chi^2\\) distribution with one degree of freedom, the test statistic differs between them. This is because the robust test uses a different variance-covariance matrix estimate when forming the Wald statistic. 5.4.1 Exercise If we had run reg robust in Stata, what standard errors would we have obtained in the regression model kid_score ~ mom_iq * mom_hs from above? Show Solution lm_robust(kid_score ~ mom_iq * mom_hs, kids, se_type = &#39;stata&#39;) %&gt;% tidy() %&gt;% select(term, std.error) ## term std.error ## 1 (Intercept) 13.3292049 ## 2 mom_iq 0.1423952 ## 3 mom_hs 14.9719985 ## 4 mom_iq:mom_hs 0.1563629 Set x equal to mom_iq from kids and let n equal length(x). Generate a vector of independent mean zero errors epsilon with standard deviation equal to exp(1.3 + 0.8 * scale(x)). Read the help file for scale() to make sure you understand what this function does. Plot x against epsilon. Are the errors contained in epsilon homoskedastic? Explain briefly. Show Solution set.seed(4321) x &lt;- kids$mom_iq n &lt;- length(x) epsilon &lt;- rnorm(n, mean = 0, sd = exp(1.3 + 0.8 * scale(x))) qplot(x, epsilon) The errors epsilon are clearly heteroskedastic: their variance is an increasing function of x. We can see this clearly in the plot: the values of epsilon \"fan out\" as x increases. Generate a vector of simulated outcomes y according to y &lt;- a + b * x + epsilon where a and b are the intercept and slope from reg_pooled. Then regress y on x and calculate three kinds of standard errors: the usual ones from lm(), the HC2-flavored robust versions from lm_robust(), and the ones that would be produced by the Stata command reg robust. Comment on the differences between them. Show Solution y &lt;- fitted.values(reg_pooled) + epsilon reg_sim &lt;- lm(y ~ x) reg_sim_robust &lt;- lm_robust(y ~ x) reg_sim_stata &lt;- lm_robust(y ~ x, se_type = &#39;stata&#39;) data.frame(tidy(reg_sim)[, c(&#39;term&#39;, &#39;std.error&#39;)], HC2.std.error = tidy(reg_sim_robust)$std.error, stata.std.error = tidy(reg_sim_stata)$std.error) ## term std.error HC2.std.error stata.std.error ## 1 (Intercept) 2.15350326 3.30916120 3.29797118 ## 2 x 0.02129732 0.03591114 0.03579134 Here the robust standard errors are considerably larger, but the HC2 version is nearly identical to the Stata version (HC1). In an earlier exercise from this lesson you tested the joint null hypothesis that the slope and intercept of the predictive relationship between kid_score and mom_iq was the same, regardless of whether a child's mother graduated from high school. Do the results change if you carry out a heteroskedasticity-robust version of the test? Show Solution linearHypothesis(reg_interact, c(&#39;mom_hs = 0&#39;, &#39;mom_iq:mom_hs = 0&#39;), test = &#39;Chisq&#39;) ## Linear hypothesis test ## ## Hypothesis: ## mom_hs = 0 ## mom_iq:mom_hs = 0 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq * mom_hs ## ## Res.Df RSS Df Sum of Sq Chisq Pr(&gt;Chisq) ## 1 432 144137 ## 2 430 138879 2 5258.7 16.282 0.0002913 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 linearHypothesis(reg_interact_robust, c(&#39;mom_hs = 0&#39;, &#39;mom_iq:mom_hs = 0&#39;)) ## Linear hypothesis test ## ## Hypothesis: ## mom_hs = 0 ## mom_iq:mom_hs = 0 ## ## Model 1: restricted model ## Model 2: kid_score ~ mom_iq * mom_hs ## ## Res.Df Df Chisq Pr(&gt;Chisq) ## 1 432 ## 2 430 2 15.101 0.0005258 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To make this an apples-to-apples comparison, I use the \\(\\chi^2\\) version of the non-robust test. Although the test statistics vary slightly, there's no meaningful difference in results here. 5.5 Publication Quality Tables A crucial part of communicating our results in a statistical analysis creating tables that are clear, and easy to read. In this section we'll learn how to use the modelsummary package to generate reproducible, publication-quality tables like those that appear in academic journals. It would take multiple lessons to do full justice to modelsummary. Here I'll show you some common and fairly simple use cases. To learn more, see this tutorial on basic options and this one on more advanced customization. 5.5.1 datasummary_skim() The datasummary_skim() function from modelsummary makes it easy to construct simple tables of summary statistics. Simply pass it a data frame, and it will return a helpful table complete with cute little histograms! (If you don't want the histograms, you can set histogram = FALSE.) library(modelsummary) datasummary_skim(kids) Unique (#) Missing (%) Mean SD Min Median Max kid_score 85 0 86.8 20.4 20.0 90.0 144.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } mom_hs 2 0 0.8 0.4 0.0 1.0 1.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } mom_iq 332 0 100.0 15.0 71.0 97.9 138.9 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } mom_age 13 0 22.8 2.7 17.0 23.0 29.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } The summary statistics for mom_hs are a bit silly: there's no point in reporting the standard deviation of a binary variable, since this is a deterministic function of the mean. The problem here is that datasummary_skim has no way of knowing that we have encoded a categorical variable using the values 0 and 1. In our last lesson I mentioned that it's better to explicitly store categorical variables as factors in R, and suggested replacing mom_hs with mom_education as follows: kids_cat &lt;- kids %&gt;% mutate(mom_education = if_else(mom_hs == 1, &#39;High School&#39;, &#39;No High School&#39;)) %&gt;% mutate(mom_education = factor(mom_education, levels = unique(mom_education))) %&gt;% select(-mom_hs) If we make explicit which variables are categorical and which aren't, datasummary_skim() will drop any categorical variables by default: datasummary_skim(kids_cat) Unique (#) Missing (%) Mean SD Min Median Max kid_score 85 0 86.8 20.4 20.0 90.0 144.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } mom_iq 332 0 100.0 15.0 71.0 97.9 138.9 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } mom_age 13 0 22.8 2.7 17.0 23.0 29.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } To make a table of only the categorical variables, set type = 'categorical' datasummary_skim(kids_cat, type = &#39;categorical&#39;) mom_education N % High School 341 78.6 No High School 93 21.4 5.5.2 datasummary_balance() It can sometimes be helpful to compare summary statistics across categories defined by a \"grouping variable.\" For example, we may be interested to know how kid_score, mom_iq, and mom_age vary with mom_education. The datasummary_balance() function makes this easy: datasummary_balance(~ mom_education, data = kids_cat) High School (N=341) No High School (N=93) Mean Std. Dev. Mean Std. Dev. Diff. in Means Std. Error kid_score 89.3 19.0 77.5 22.6 -11.8 2.6 mom_iq 102.2 14.8 91.9 12.6 -10.3 1.5 mom_age 23.1 2.6 21.7 2.7 -1.4 0.3 The second argument of datasummary_balance() is the data frame we wish to summarize. The first is a one-sided formula that takes the form ~ [GROUPING VARIABLE]. The idea here is that [GROUPING VARIABLE] is analogous to a RHS variable in a regression formula. The syntax makes sense if you think about it: this table is equivalent to \"regressing\" all of the variables in kids_cat on mom_eduation. 5.5.3 modelsummary() The modelsummary() produces summary tables for statistical models, including linear regressions. The simplest way to use it is by passing a single argument: a fitted model such as reg_pooled modelsummary(reg_pooled) Model 1 (Intercept) 25.800 (5.917) mom_iq 0.610 (0.059) Num.Obs. 434 R2 0.201 R2 Adj. 0.199 AIC 3757.2 BIC 3769.4 Log.Lik. −1875.608 F 108.643 RMSE 18.27 But it is with multiple models that modelsummary() shows its true power. Recall that we fitted a number of different regressions based on the kids dataset. To display them all at once in a format that permits easy comparisons, we simply need to create a list of the fitted models: kids_regressions &lt;- list(&#39;OLS 1&#39; = reg_pooled, &#39;OLS 2&#39; = reg_hs_dummy, &#39;OLS 3&#39; = reg_interact) When we pass kids_regressions to modelsummary() it will magically line up the estimates from each model that correspond to the same regressor, and add the names that I provided when creating the list of results: modelsummary(kids_regressions) OLS 1 OLS 2 OLS 3 (Intercept) 25.800 25.732 −11.482 (5.917) (5.875) (13.758) mom_iq 0.610 0.564 0.969 (0.059) (0.061) (0.148) mom_hs 5.950 51.268 (2.212) (15.338) mom_iq × mom_hs −0.484 (0.162) Num.Obs. 434 434 434 R2 0.201 0.214 0.230 R2 Adj. 0.199 0.210 0.225 AIC 3757.2 3752.0 3745.1 BIC 3769.4 3768.3 3765.5 Log.Lik. −1875.608 −1871.995 −1867.543 F 108.643 58.724 42.839 RMSE 18.27 18.14 17.97 Pretty good for a first pass! But let's clean things up a bit. For my taste, this table has far too many goodness of fit statistics. We can remove the ones that we don't want using the gof_omit option. To omit everything, set gof_omit = '.*'. Otherwise you can specify which measures to omit as follows: modelsummary(kids_regressions, gof_omit = &#39;Log.Lik|R2 Adj.|AIC|BIC&#39;) OLS 1 OLS 2 OLS 3 (Intercept) 25.800 25.732 −11.482 (5.917) (5.875) (13.758) mom_iq 0.610 0.564 0.969 (0.059) (0.061) (0.148) mom_hs 5.950 51.268 (2.212) (15.338) mom_iq × mom_hs −0.484 (0.162) Num.Obs. 434 434 434 R2 0.201 0.214 0.230 F 108.643 58.724 42.839 RMSE 18.27 18.14 17.97 That's much cleaner, but there's still room for improvement. With standard errors as large as 13 for some of the coefficient estimates in this table, it's silly if not downright innumerate to report three decimal places of precision in our estimates: differences on this scale are merely noise. The fmt option allows you to specify in excruciating detail how you would like the numbers in your table to be formatted. For full details, see this link. The simplest use of this option is to set a desired number of decimal places, for example: modelsummary(kids_regressions, gof_omit = &#39;Log.Lik|R2 Adj.|AIC|BIC&#39;, fmt = 2, title = &#39;Regression results for kids dataset&#39;, notes = &#39;Source: all data were fabricated by the authors.&#39;) Table 5.1: Regression results for kids dataset OLS 1 OLS 2 OLS 3 (Intercept) 25.80 25.73 −11.48 (5.92) (5.88) (13.76) mom_iq 0.61 0.56 0.97 (0.06) (0.06) (0.15) mom_hs 5.95 51.27 (2.21) (15.34) mom_iq × mom_hs −0.48 (0.16) Num.Obs. 434 434 434 R2 0.201 0.214 0.230 F 108.643 58.724 42.839 RMSE 18.27 18.14 17.97 Source: all data were fabricated by the authors. where I added in a title and footnote for good measure! 5.5.4 Robust Standard Errors If you want to display heteroskedasticity-robust standard errors in a table constructed with modelsummary() you have two options. The first, and simplest, is to pass it a model fitted with lm_robust. For example, using the simulated x and y data from above: ols &lt;- lm(y ~ x) robust &lt;- lm_robust(y ~ x) different_SEs &lt;- list(&#39;OLS&#39; = ols, &#39;Robust&#39; = robust) cbind(tidy(ols)[, c(&#39;term&#39;, &#39;std.error&#39;)], robust = tidy(robust)$std.error) ## term std.error robust ## 1 (Intercept) 2.15350326 3.30916120 ## 2 x 0.02129732 0.03591114 modelsummary(different_SEs, gof_omit = &#39;Log.Lik|R2 Adj.|R2|AIC|BIC|F|RMSE&#39;) OLS Robust (Intercept) 23.148 23.148 (2.154) (3.309) x 0.641 0.641 (0.021) (0.036) Num.Obs. 434 434 Std.Errors HC2 Notice that modelsummary() very helpfully indicates which flavor of robust standard errors appear in the Robust column. The second option is to fit your model using lm() and have modelsummary() do the appropriate standard error calculations for you using the vcov option: modelsummary(lm(y ~ x), vcov = &#39;HC2&#39;, gof_omit = &#39;Log.Lik|R2 Adj.|R2|AIC|BIC|F|RMSE&#39;) Model 1 (Intercept) 23.148 (3.309) x 0.641 (0.036) Num.Obs. 434 Std.Errors HC2 The heavily lifting in this case is done in the background by the sandwich package: see this link for more details. 5.5.5 Exercise Read the help file for the tips dataset, available in the reshape2 package. Then use datasummary_skim() to make two tables of summary statistics for tips: one for the categorical variables and one for the remaining variables. Show Solution library(reshape2) datasummary_skim(tips) Unique (#) Missing (%) Mean SD Min Median Max total_bill 229 0 19.8 8.9 3.1 17.8 50.8 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } tip 123 0 3.0 1.4 1.0 2.9 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } size 6 0 2.6 1.0 1.0 2.0 6.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } datasummary_skim(tips, type = &#39;categorical&#39;) N % sex Female 87 35.7 Male 157 64.3 smoker No 151 61.9 Yes 93 38.1 day Fri 19 7.8 Sat 87 35.7 Sun 76 31.1 Thur 62 25.4 time Dinner 176 72.1 Lunch 68 27.9 Suppose you were curious to know whether any of the columns of tips vary with sex. Use datasummary_balance() to construct a table of summary statistics that would allow you to explore this possibility. Comment on the results. Show Solution datasummary_balance(~ sex, tips) Female (N=87) Male (N=157) Mean Std. Dev. Mean Std. Dev. Diff. in Means Std. Error total_bill 18.1 8.0 20.7 9.2 2.7 1.1 tip 2.8 1.2 3.1 1.5 0.3 0.2 size 2.5 0.9 2.6 1.0 0.2 0.1 N Pct. N Pct. smoker No 54 62.1 97 61.8 Yes 33 37.9 60 38.2 day Fri 9 10.3 10 6.4 Sat 28 32.2 59 37.6 Sun 18 20.7 58 36.9 Thur 32 36.8 30 19.1 time Dinner 52 59.8 124 79.0 Lunch 35 40.2 33 21.0 Male bill payers appear to spend around $2.70 more overall, a difference that is fairly precisely estimated, but there's no clear difference in tip and only a small difference in party size. Male bill payers seem to be much more common at dinner compared to lunch. Choose three regression specifications to predict tip using other columns in the tips dataset. Any three specifications are fine: you're free to choose! Use modelsummary() to create a table of regression output for these models using the default lm() standard errors. Show Solution reg1 &lt;- lm(tip ~ total_bill, tips) reg2 &lt;- lm(tip ~ total_bill + size, tips) reg3 &lt;- lm(tip ~ total_bill + size + sex, tips) reg_results &lt;- list(&#39;OLS 1&#39; = reg1, &#39;OLS 2&#39; = reg2, &#39;OLS 3&#39; = reg3) modelsummary(reg_results, gof_omit = &#39;Log.Lik|R2 Adj.|AIC|BIC&#39;, fmt = 2) OLS 1 OLS 2 OLS 3 (Intercept) 0.92 0.67 0.68 (0.16) (0.19) (0.21) total_bill 0.11 0.09 0.09 (0.01) (0.01) (0.01) size 0.19 0.19 (0.09) (0.09) sexMale −0.03 (0.14) Num.Obs. 244 244 244 R2 0.457 0.468 0.468 F 203.358 105.948 70.362 RMSE 1.02 1.01 1.02 Repeat the preceding with HC2 standard errors. Show Solution modelsummary(reg_results, gof_omit = &#39;Log.Lik|R2 Adj.|AIC|BIC&#39;, fmt = 2, vcov = &#39;HC2&#39;) OLS 1 OLS 2 OLS 3 (Intercept) 0.92 0.67 0.68 (0.21) (0.21) (0.22) total_bill 0.11 0.09 0.09 (0.01) (0.02) (0.02) size 0.19 0.19 (0.11) (0.11) sexMale −0.03 (0.12) Num.Obs. 244 244 244 R2 0.457 0.468 0.468 F 77.546 51.918 35.076 Std.Errors HC2 HC2 HC2 To make a long story short: it's almost never plausible that the null hypothesis holds exactly. If the null is even slightly incorrect then we're certain to reject it if we gather enough data, but this doesn't tell us anything of scientific interest. What really matters is effect sizes and the precision with which we can estimate them; not p-values.↩︎ Recall that a Wald test compares the estimated coefficients from an unrestricted model to their hypothesized values under a specified restriction.↩︎ "],["the-normal-distribution.html", "Lesson 6 The Normal Distribution 6.1 Standard Normals as Building Blocks 6.2 The One-Dimensional Case 6.3 Generating Correlated Normal RVs 6.4 The Cholesky Decomposition 6.5 Epilogue", " Lesson 6 The Normal Distribution If you hear a \"prominent\" economist using the word \"equilibrium,\" or \"normal distribution,\" do not argue with him; just ignore him, or try to put a rat down his shirt. -- Nassim Nicholas Taleb The standard normal distribution. Taleb's exhortation notwithstanding, this lesson is all about the normal distribution! While its importance as a description of naturally-occurring data has probably been overstated, the normal distribution still plays a central role in econometrics and statistics. This isn't because data follow a normal distribution; it's because sampling distributions of estimators tend to, at least in large samples. The normal distribution is also important in model-building because we can use it as an ingredient to construct relatively simple models of complicated phenomena. From your introductory coursework in statistics and econometrics you're probably familiar with the one-dimensional normal distribution. This lesson will build on the intuition you already have in the one-dimensional case to help you gain a deeper understanding of the multivariate normal distribution. There's a good chance that you've seen some of this material before. But from past experience, many students are a bit fuzzy on the details. We'll start by reviewing the key commands for working with normal distributions in R: d/p/q/rnorm(). These will come up again and again in future lessons. Then we'll take up the challenge of simulating correlated normal draws. Along the way we'll learn how to calculate column means, variance-covariance matrices, and correlation matrices with the commands colMeans(), cov(), cor() and cov2cor(). We'll also learn to make kernel density plots using the ggplot commands geom_density() and geom_density2d_filled(). Our destination in this lecture is the Cholesky Decomposition, an important result from matrix algebra that's extremely helpful for understanding multivariate normal distributions. We'll work up to this result in easy steps, introducing some basic R commands for matrix algebra as we go, including %*%, t(), and chol(). At the very end, once you know how to generate correlated normals from scratch, I'll relent and allow you to use the extremely helpful command rmvnorm() from the mvtnorm package instead. An epilogue to this lesson makes the case for why doing things \"the hard way\" before you allow yourself to use a canned function like rmvnorm() is worth the trouble. 6.1 Standard Normals as Building Blocks Escher's \"Relativity\" in Lego by Andrew Lipson What's the definition of a \\(\\text{N}(\\mu, \\sigma^2)\\) random variable? You might be tempted to write down a probability density function, but I have a better idea: let's pass the buck! The \\(\\text{N}(\\mu, \\sigma^2)\\) distribution is what you end up with if you shift and scale a standard normal. In particular: we say that \\(X \\sim \\text{N}(\\mu, \\sigma^2)\\) if \\(X = \\mu + \\sigma Z\\) for some constants \\(\\mu\\) and \\(\\sigma &gt; 0\\), where \\(Z\\) is a standard normal random variable. From this perspective, any properties of normal distributions in general follow from properties of the standard normal distribution in particular. Because the standard normal distribution is fairly simple, this turns out to be a very convenient approach. One-dimensional normal distributions can only take us so far. In many important applications we need to work with two or more correlated normal random variables. For example, to run a simulation study exploring the properties of the instrumental variables estimator we need a way of creating correlated \"first stage\" and \"second stage\" errors. Fortunately we can use the same basic approach as above to build up a multivariate normal random vector \\(\\mathbf{X}\\). We start with a vector of independent standard normals, scale them with a matrix of constants and shift them with a vector of yet more constants. In other words: \\(\\mathbf{X} = \\boldsymbol{\\mu} + \\mathbf{A}\\mathbf{Z}\\). If this looks forbidding, don't worry; we'll build up the intuition step-by-step in this lesson. For the moment, simply take my word for it: the multivariate normal distribution becomes much less formidable once you break it down into its constituent independent standard normals. 6.2 The One-Dimensional Case We'll start a quick review of the one-dimensional case. After introducing the key R functions for normal distributions, I'll turn you loose on some simple exercises. 6.2.1 pnorm() The R function for the cumulative distribution function (CDF) of a standard normal is pnorm(). In other words, \\(\\mathbb{P}(Z\\leq z) = \\texttt{pnorm}(z)\\) for \\(Z\\sim \\text{N}(0,1)\\). So what about the CDF of \\(X \\sim \\text{N}(0, \\sigma^2)\\)? Since \\((X - \\mu) / \\sigma \\sim \\text{N}(0,1)\\), we have \\[ \\mathbb{P}(X\\leq x) = \\mathbb{P}\\left( \\frac{X - \\mu}{\\sigma} \\leq \\frac{x - \\mu}{\\sigma} \\right) = \\texttt{pnorm}\\left(\\frac{x - \\mu}{\\sigma}\\right). \\] This shows that we don't actually need a separate function for general normal distributions: we can do everything with pnorm() after subtracting the mean (centering) and dividing by the standard deviation (standardizing). For convenience, R allows us to supply a mean and standard deviation sd to pnorm() to obtain the CDF of an arbitrary normal random variable. This gives exactly the same result as centering and standardizing: pnorm(1, mean = 2, sd = 3) ## [1] 0.3694413 pnorm((1 - 2) / 3) ## [1] 0.3694413 6.2.2 dnorm() The R function for the probability density function of a standard normal is dnorm(). The function dnorm() is the derivative of the function pnorm(). As with pnorm(), R allows us to supply a mean and standard deviation sd to obtain the density of a \\(\\text{N}(\\mu, \\sigma^2)\\) random variable, but this is only for convenience since \\[ \\frac{d}{dx} \\mathbb{P}(X\\leq x) = \\frac{d}{dx} \\text{pnorm} \\left(\\frac{x - \\mu}{\\sigma}\\right) = \\frac{1}{\\sigma}\\texttt{dnorm}\\left(\\frac{x - \\mu}{\\sigma}\\right). \\] Accordingly, both of these give the same answer, although the first is easier to read: dnorm(1, mean = 2, sd = 3) ## [1] 0.1257944 dnorm((1 - 2) / 3) / 3 ## [1] 0.1257944 6.2.3 qnorm() The R function for the quantile function of a standard normal distribution is qnorm(). This is the inverse of the function pnorm(). jWhereas pnorm(z) returns the probability that \\(Z \\sim \\text{N}(0,1)\\) takes on a value no greater than z, pnorm(p) returns the value of z such that \\(\\mathbb{P}(Z \\leq z) = p\\). This means that qnorm(pnorm(z)) equals z and pnorm(qnorm(p)) equals p. For example: qnorm(pnorm(2)) ## [1] 2 pnorm(qnorm(0.3)) ## [1] 0.3 Like dnorm() and pnorm(), qnorm() allows us to specify a mean and standard deviation sd to obtain quantiles of an arbitrary normal random variable. But again, this is merely for convenience. Since \\(\\mathbb{P}(X \\leq x) = \\texttt{pnorm}\\big((x - \\mu)/\\sigma\\big)\\), \\[ \\begin{aligned} \\mathbb{P}(X \\leq x) &amp;= p\\\\ \\texttt{qnorm}\\left( \\mathbb{P}(X \\leq x) \\right) &amp;= \\texttt{qnorm}(p)\\\\ \\texttt{qnorm}\\left( \\texttt{pnorm}\\left(\\frac{x - \\mu}{\\sigma} \\right) \\right) &amp;= \\texttt{qnorm}(p)\\\\ \\frac{x - \\mu}{\\sigma} &amp;= \\texttt{qnorm}(p)\\\\ x &amp;= \\mu + \\sigma \\times \\texttt{qnorm}(p) \\end{aligned} \\] In words: the \\(p\\)th quantile of a \\(\\text{N}(\\mu, \\sigma^2)\\) RV equals \\(\\mu\\) plus \\(\\sigma\\) times the \\(p\\)th quantile of a standard normal. For example: qnorm(0.75, mean = 2, sd = 3) ## [1] 4.023469 2 + 3 * qnorm(0.75) ## [1] 4.023469 6.2.4 rnorm() The R function for making simulated standard normal draws is rnorm(). For example, we can make ten standard normal draws as follows set.seed(1066) rnorm(10) ## [1] -0.5917632 0.2926060 -0.9212630 0.2399853 0.9197199 -0.7864827 ## [7] 2.8104984 -0.9771701 1.7462778 0.1523001 To obtain draws from a general normal distribution we can set the optional arguments mean and sd as above. But as above, this is only for convenience. Remember that we defined a \\(\\text{N}(\\mu, \\sigma^2)\\) as what you get by shifting and scaling a standard normal. This means that shifting and scaling standard normal simulation draws is the same thing as simulating a \\(\\mathbf{N}(\\mu, \\sigma^2)\\) random variable, for example: set.seed(1066) x &lt;- rnorm(10, mean = 2, sd = 3) set.seed(1066) y &lt;- 2 + 3 * rnorm(10) all.equal(x, y) ## [1] TRUE 6.2.5 p/q/d/rnorm() are Vectorized R accepts a vector as first argument to pnorm(), dnorm(), and qnorm(). This is very handy in practice, since it allows us to calculate the normal CDF/density/quantile function at many points in a single command. For example, dnorm(c(-1.96, -1, 0, 1, 1.96)) ## [1] 0.05844094 0.24197072 0.39894228 0.24197072 0.05844094 pnorm(c(-1.96, -1, 0, 1, 1.96)) ## [1] 0.0249979 0.1586553 0.5000000 0.8413447 0.9750021 qnorm(c(0.025, 0.16, 0.5, 0.84, 0.975)) ## [1] -1.9599640 -0.9944579 0.0000000 0.9944579 1.9599640 The function rnorm() is a bit different. Its first argument is the number of normal simulation draws we wish to make. When we pass it a vector as its first argument, rnorm() calculates the length of the vector and returns an equivalent number of normal draws: rnorm(5) ## [1] -0.56094004 0.27176376 0.47237903 0.62703516 0.01277877 rnorm(c(-1, 0, 1)) ## [1] 0.01787882 0.31304030 -1.27600303 All four of the p/q/d/rnorm() functions accept a vector for their mean and sd arguments. This too can be handy in practice. We can use it, for example, to make normal draws with different means rnorm(2, mean = c(-100, 100), sd = 5) ## [1] -102.7192 104.0509 different variances rnorm(2, mean = 0, sd = c(1, 100)) ## [1] 0.6451931 -110.0389461 or both rnorm(2, mean = c(-10, 10), sd = c(0.01, 1)) ## [1] -9.995741 9.983758 6.2.6 Be Careful! A common R mistake is specifying the variance rather than standard deviation inside of dnorm(), pnorm(), qnorm(), or rnorm(). The best way to avoid making this mistake is by explicitly writing sd = [YOUR SD HERE]. For example, dnorm(2, mean = 1, sd = 3) is much less likely to lead to confusion than dnorm(2, 1, 3) although both produce the same result: identical(dnorm(2, 1, 3), dnorm(2, mean = 1, sd = 3)) ## [1] TRUE 6.2.7 Exercises Simulate 1000 normal draws with mean -3 and variance 4. Calculate the sample mean and variance of your simulation draws. Show Solution set.seed(1693) sims &lt;- rnorm(1000, mean = -3, sd = 2) c(sample_mean = mean(sims), sample_var = var(sims)) ## sample_mean sample_var ## -2.984270 3.890881 Transform your simulations from the preceding part so that they have a sample mean of zero and sample standard deviation of one. Check that your transformation worked as expected. Show Solution z &lt;- (sims - mean(sims)) / sd(sims) c(sample_mean = mean(z), sample_var = var(z)) ## sample_mean sample_var ## 6.704999e-17 1.000000e+00 Calculate the probability that a normal random variable with mean 2 and variance 9 takes on a value greater than 5. Show Solution 1 - pnorm(5, mean = 2, sd = 3) ## [1] 0.1586553 Calculate the probability that a standard normal random variable takes on a value between -1 and 2. Show Solution pnorm(2) - pnorm(-1) ## [1] 0.8185946 Find the value of \\(k\\) such that a standard normal random variable has a 50% probability of falling in the interval \\([-k, k]\\). Use pnorm() to check your work. Show Solution k &lt;- -1 * qnorm(0.25) k ## [1] 0.6744898 pnorm(k) - pnorm(-k) ## [1] 0.5 Use dnorm() to plot the density of a standard normal random variable with mean 1 and variance 1/9 without using the mean and sd arguments. Approximately what is the maximum value of the density function? Is this a mistake?! Show Solution library(ggplot2) x &lt;- seq(0, 2, len = 100) y &lt;- 3 * dnorm(3 * (x - 1)) ggplot(data.frame(x, y)) + geom_line(aes(x, y)) max(y) ## [1] 1.196277 6.3 Generating Correlated Normal RVs 6.3.1 Start with Uncorrelated Normal Draws We'll start by generating a large number of draws to represent two independent standard normals: \\(Z_1\\) and \\(Z_2\\) set.seed(99999) large_number &lt;- 1e5 z1 &lt;- rnorm(large_number) z2 &lt;- rnorm(large_number) z &lt;- cbind(z1, z2) rm(z1, z2) Now we have a matrix with two columns: the first is z1 and the second is z2 head(z) ## z1 z2 ## [1,] -0.4255127 -1.91351696 ## [2,] -0.2829203 0.05775194 ## [3,] -0.8986773 -0.48302150 ## [4,] 0.7065184 -2.37167063 ## [5,] 2.0916699 0.35882645 ## [6,] 1.6356643 -0.54297591 To calculate the sample mean of each column, we can use the function colMeans() colMeans(z) ## z1 z2 ## -0.001512035 0.002202837 To calculate the sample variance of each column of z along with the covariance between them in one fell swoop, we use the cov() function: cov_mat &lt;- cov(z) cov_mat ## z1 z2 ## z1 1.006378316 0.000473838 ## z2 0.000473838 1.003673852 This is the sample variance-covariance matrix of z. Its diagonal elements are the sample variances of z1 and z2, and its off-diagonal elements are the covariance between them. Because we simulated uncorrelated standard normal draws, the diagonal elements are approximately one and the off-diagonals are approximately zero. And since cov(z1, z2) equals cov(z2, z1), this matrix is symmetric; its [i,j] element equals its [j,i] element: c(cov_mat[1, 2], cov_mat[2, 1]) ## [1] 0.000473838 0.000473838 When you supply a matrix as your argument, var() and cov() give exactly the same result: identical(var(z), cov(z)) ## [1] TRUE That's because the variance-covariance matrix contains variances and covariances! To obtain the sample correlation matrix, use the function cor(): cor_mat &lt;- cor(z) cor(z) ## z1 z2 ## z1 1.0000000000 0.0004714688 ## z2 0.0004714688 1.0000000000 Now the diagonal elements are one: z1 is perfectly correlated with itself and so is z2. The off-diagonal elements are the correlation between z1 and z2. Again, this matrix is symmetric since cor(z1, z2) equals cor(z2, z1). Given a covariance matrix we can immediately compute the associated correlation matrix by dividing each covariance by the appropriate product of standard deviations: cov_mat[1, 2] / (sqrt(cov_mat[1, 1] * cov_mat[2, 2])) ## [1] 0.0004714688 cor_mat[1, 2] ## [1] 0.0004714688 Alternatively we can use the R function cov2cor() to do this for us: cov2cor(cor_mat) ## z1 z2 ## z1 1.0000000000 0.0004714688 ## z2 0.0004714688 1.0000000000 6.3.2 Exercise There's an R function called cov2cor() but there isn't one called cor2cov(). Why not? Show Solution A correlation matrix contains strictly less information than a covariance matrix. If I give you the correlation matrix of \\((X_1, X_2)\\), then I haven't told you the variances of either \\(X_1\\) or \\(X_2\\). This means you can't go from a correlation matrix to a covariance matrix, although you can go in the reverse direction. Read in the kids dataset from https://ditraglia.com/data/child_test_data.csv and calculate the sample means, covariance matrix, and correlation matrix of mom.iq and kid.score. Show Solution library(tidyverse) kids &lt;- read_csv(&#39;https://ditraglia.com/data/child_test_data.csv&#39;) dat &lt;- kids %&gt;% select(mom.iq, kid.score) colMeans(dat) ## mom.iq kid.score ## 100.00000 86.79724 cov(dat) ## mom.iq kid.score ## mom.iq 225.0000 137.2443 ## kid.score 137.2443 416.5962 cor(dat) ## mom.iq kid.score ## mom.iq 1.0000000 0.4482758 ## kid.score 0.4482758 1.0000000 6.3.3 Add Constants to Shift the Means Summary statistics are useful, but pictures are far more informative. A helpful way of visualizing the simulation draws in z is by using kernel density estimator. We'll discuss the details of kernel density estimation in a future lesson. For now, you can think of it as a \"smoothed histogram.\" For example, here's a pair of histograms for z1 and z2 library(gridExtra) z1_hist &lt;- ggplot(data.frame(z)) + geom_histogram(aes(x = z1), fill = &#39;black&#39;, alpha = 0.5) z2_hist &lt;- ggplot(data.frame(z)) + geom_histogram(aes(x = z2), fill = &#39;orange&#39;, alpha = 0.5) grid.arrange(z1_hist, z2_hist, ncol = 2) And here's the kernel density version of the same. Notice that the y-axis is no longer a count of observations that fall in a given bin, but a density; the total area under each curve equals one: z1_dens &lt;- ggplot(data.frame(z)) + geom_density(aes(x = z1), fill = &#39;black&#39;, alpha = 0.5) z2_dens &lt;- ggplot(data.frame(z)) + geom_density(aes(x = z2), fill = &#39;orange&#39;, alpha = 0.5) grid.arrange(z1_dens, z2_dens, ncol = 2) The function geom_density() works in effectively the same way as geom_histogram() but its output is different. The preceding two sets of plots show the marginal distributions of z1 and z2. This tells us how each random variable behaves taken by itself. To see how z1 and z2 co-vary, we need to examine the joint distribution. A two-dimensional kernel density plot is helpful way to do this. Rather than trying to plot a 3d surface, we'll take the \"view from above,\" draw in contour lines, and add a color scale using geom_density2d_filled(): ggplot(data.frame(z)) + geom_density2d_filled(aes(x = z1, y = z2)) + coord_fixed() # forces the aspect ratio to be 1:1 so you see the circles! The colors are darker in regions of low density and brighter in regions of high density. Because this plot is constructed from simulated data, it's a bit irregular but we can pick out some obvious features. First, the density is clearly highest around the point \\((0,0)\\), and falls as we move away from this point in any direction. This makes sense: the mean, median, and mode of both z1 and z2 are all zero. Second, the contours of equal density are circles. Notice that \\(z_1\\) and \\(z_2\\) only enter this function through the sum of squares \\((z_1^2 + z_2^2)\\). As long as this quantity remains fixed, the density stays the same. So what kind of shape does the expression \\((z_1^2 + z_2^2) = c\\) describe, for some fixed constant \\(c\\)? A circle centered at \\((0,0)\\)! It may not surprise you to notice that we can shift the centers of these circles, by adding constants to z1 and z2. You won't notice the difference in these plots unless you look carefully: only the axes have changed: # Shift the means from (0,0) to (1,-1) x &lt;- cbind(x1 = z[,1] + 1, x2 = z[,2] - 1) x_marginals &lt;- ggplot(data.frame(x)) + geom_density(aes(x = x1), fill = &#39;black&#39;, alpha = 0.5) + geom_density(aes(x = x2), fill = &#39;orange&#39;, alpha = 0.5) + xlab(&#39;&#39;) x_joint &lt;- ggplot(data.frame(x)) + geom_density2d_filled(aes(x = x1, y = x2)) + coord_fixed() # forces the aspect ratio to be 1:1 so you see the circles! grid.arrange(x_marginals, x_joint, ncol = 2) 6.3.4 Exercise Continuing from the exercise from above, create both marginal and joint kernel density plots of mom.iq and kid.score. Do these appear to be normally distributed? Show Solution ggplot(kids) + geom_density(aes(x = mom.iq), fill = &#39;black&#39;, alpha = 0.5) ggplot(kids) + geom_density(aes(x = kid.score), fill = &#39;orange&#39;, alpha = 0.5) ggplot(kids) + geom_density2d_filled(aes(x = mom.iq, y = kid.score)) Reading from the color scale, the height of the \"peak\" in each of the two-dimensional kernel density plots from above was around 0.16. Why is this? Show Solution If \\(Z_1\\) and \\(Z_2\\) are independent standard normal random variables, then their joint density equals the product of their marginal densities: \\[ \\begin{aligned} f(z_1, z_2) &amp;= f(z_1) f(z_2) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z_1^2}{2} \\right) \\times \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z_2^2}{2} \\right) \\\\ &amp;=\\frac{1}{2 \\pi} \\exp\\left\\{-\\frac{1}{2} (z_1^2 + z_2^2)\\right\\}. \\end{aligned} \\] Since \\(1/(2\\pi)\\) is positive and \\(-1/2\\) is negative, this function is maximized when \\(z_1 + z_2^2\\) is made as small as possible, i.e. at \\((0,0)\\). Substituting these values gives \\(f(0,0) = 1/(2\\pi) \\approx 0.159\\). The contours of equal density for a pair of uncorrelated standard normal variables are circles. Why? Show Solution From the expression in the previous solution, \\(f(z_1, z_2)\\) is constant whenever \\((z_1^2 + z_2^2)\\) is constant, and the expression \\((z_1^2 + z_2^2) = \\text{C}\\) describes a circle centered at \\((0,0)\\). 6.3.5 Multiply by Scalars to Change the Variance Shifting the means of two random variables by adding a constant to each has no effect on their respective variances or covariances. If \\(Z_1, Z_2 \\sim \\text{ iid N}(0,1)\\) and \\(c_1, c_2\\) are constants, then \\[ \\begin{aligned} \\text{Var}(Z_1 + c_1) &amp;= \\text{Var}(Z_2 + c_2) = \\text{Var}(Z_1) = 1\\\\ \\text{Cov}(Z_1 + c_1, Z_2 + c_2) &amp;= \\text{Cov}(Z_1, Z_2) = 0 \\end{aligned} \\] For this reason we'll work with zero mean normals throughout most of this lesson. After we've created normal random variables with the desired variances and covariances, we can always shift their means as needed. To change the variances of \\(Z_1\\) and \\(Z_2\\) without creating any covariance between them, we simply multiply each by a constant. While we won't delve into the details of the geometry, this turns circles of equal density into ellipses. For example: # Change the variances from (1, 1) to (4, 25) x &lt;- cbind(x1 = 2 * z[,1], x2 = 5 * z[,2]) cov(x) ## x1 x2 ## x1 4.02551326 0.00473838 ## x2 0.00473838 25.09184630 x_marginals &lt;- ggplot(data.frame(x)) + geom_density(aes(x = x1), fill = &#39;black&#39;, alpha = 0.5) + geom_density(aes(x = x2), fill = &#39;orange&#39;, alpha = 0.5) + xlab(&#39;&#39;) x_joint &lt;- ggplot(data.frame(x)) + geom_density2d_filled(aes(x = x1, y = x2)) + coord_fixed() # forces the aspect ratio to be 1:1 so you see the ellipses! grid.arrange(x_marginals, x_joint, ncol = 2) 6.3.6 Combine to Create Correlation There's one more fact about normal distributions that we'll need: a sum of independent normal random variables is itself a normal random variable. For our purposes, this means that \\(X \\equiv a Z_1 + b Z_2 + c\\) is a normal random variable for any constants \\(a, b\\), and \\(c\\).21 Its mean and variance are determined from the values of \\(a, b\\), and \\(c\\). Now we're ready to construct some correlated normals. Suppose I define \\(X_1 \\equiv 2 Z_1 + Z_2\\) and \\(X_2 \\equiv Z_1 + 4 Z_2\\). In other words, suppose that I combine \\(Z_1\\) and \\(Z_2\\) in two different ways by taking two distinct linear combinations. Because \\(X_1\\) and \\(X_2\\) depend on the same underlying standard normal random variables \\((Z_1, Z_2)\\) we would expect them to be correlated, and indeed they are: x &lt;- cbind(x1 = 2 * z[,1] + z[,2], x2 = z[,1] + 4 * z[,2]) cov(x) ## x1 x2 ## x1 5.031082 6.031717 ## x2 6.031717 17.068951 cor(x) ## x1 x2 ## x1 1.0000000 0.6508888 ## x2 0.6508888 1.0000000 An equivalent, but slightly nicer way of constructing x uses matrix multiplication. Written in matrix from, the two linear combinations from above can be expressed as \\(\\mathbf{X} = \\mathbf{A} \\mathbf{Z}\\) where \\[ \\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}, \\quad \\mathbf{Z} = \\begin{bmatrix} Z_1 \\\\ Z_2\\end{bmatrix}, \\quad \\mathbf{A} \\equiv \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 4\\end{bmatrix}. \\] To implement this in R, we need to introduce a few new commands. First, matrix() is used to convert a vector of values into a matrix: A &lt;- matrix(c(2, 1, 1, 4), byrow = TRUE, nrow = 2, ncol = 2) A ## [,1] [,2] ## [1,] 2 1 ## [2,] 1 4 The first argument of matrix() is the vector of values that we will use to fill our matrix; ncol and nrow are the desired numbers of columns and rows. By default, R fills matrices by column. To make things easier to read, I've added line breaks so you can \"see\" the matrix and set the argument byrow = TRUE so that R will fill A by row instead. Next we need to multiply \\((Z_1, Z_2)&#39;\\) by \\(\\mathbf{A}\\). Each row of the matrix z contains one simulated realization of \\(Z_1\\) and \\(Z_2\\), but our expression \\(\\mathbf{X} = \\mathbf{A}\\mathbf{Z}\\) from above represents \\(Z_1\\) and \\(Z_2\\) as a column rather than a row. This means we'll need to transpose them using the t() function. Here's an example of t() in action: A ## [,1] [,2] ## [1,] 2 1 ## [2,] 1 4 t(A) ## [,1] [,2] ## [1,] 2 1 ## [2,] 1 4 Finally, we need to know how to multiply matrices in R. There are two notions of a \"matrix product\" in R. The first is elementwise multiplication: A * B multiplies every element of A by the corresponding element of B for example A * A ## [,1] [,2] ## [1,] 4 1 ## [2,] 1 16 The second, and the one we need here, is matrix multiplication: A %*% B creates a matrix whose [i,j] element is the dot product of A[i,] with B[,j]. This is the \"usual\" matrix product that you know and love from linear algebra, for example: A %*% A ## [,1] [,2] ## [1,] 5 6 ## [2,] 6 17 Putting the pieces together gives a slicker way to construct the matrix x from above, namely t(A %*% t(z)): x_alt &lt;- t(A %*% t(z)) colnames(x_alt) &lt;- c(&#39;x1&#39;, &#39;x2&#39;) # give x_alt the same column names as x identical(x, x_alt) # identical() is picky: it checks names as well as values! ## [1] TRUE Because each of them is a sum of independent normals, both \\(X_1\\) and \\(X_2\\) are themselves normal random variables. They are both marginally normal and jointly normal: x_marginals &lt;- ggplot(data.frame(x)) + geom_density(aes(x1), fill = &#39;black&#39;, alpha = 0.5) + geom_density(aes(x2), fill = &#39;orange&#39;, alpha = 0.5) x_joint &lt;- ggplot(data.frame(x)) + geom_density2d_filled(aes(x1, x2)) + coord_fixed() # forces the aspect ratio to be 1:1 so you see the ellipses! grid.arrange(x_marginals, x_joint, ncol = 2) Notice that the correlation between \\(X_1\\) and \\(X_2\\) produces a tilted ellipse in the joint density plot. The ellipse slopes upward because \\(X_1\\) and \\(X_2\\) are positively correlated. So how can we tell what covariances we'll end up with for a particular choice of A? You'll work out the answer in the following exercise. 6.3.7 Exercise Suppose that \\(Z_1, Z_2 \\sim \\text{ iid N}(0,1)\\) and \\[ \\mathbf{X} = \\mathbf{A}\\mathbf{Z}, \\quad \\mathbf{X}= \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}, \\quad \\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d\\end{bmatrix}, \\quad \\begin{bmatrix} Z_1 \\\\ Z_2 \\end{bmatrix}. \\] Calculate the variance of \\(X_1\\), the variance of \\(X_2\\), and the covariance between \\(X_1\\) and \\(X_2\\) in terms of the constants \\(a, b, c\\), and \\(d\\). Using these calculations, show that the variance-covariance matrix of \\(\\mathbf{X}\\) equals \\(\\mathbf{A} \\mathbf{A}&#39;\\). Use this result to work out the variance-covariance matrix of my example from above with \\(a = 2, b = 1, c = 1, d = 4\\) and check that it agrees with the simulations. Show Hint Don't panic at the sight of the matrix multiplication! It's simply a more compact way of writing the following linear equations: \\[ \\begin{aligned} X_1 &amp;= a Z_1 + b Z_2 \\\\ X_2 &amp;= c Z_1 + d Z_2 \\end{aligned} \\] Now use what you know about the properties of variance and covariance, along with the fact that \\(Z_1\\) and \\(Z_2\\) are independent, to work out the required moments. Show Solution First we'll calculate the variances. Since \\(Z_1\\) and \\(Z_2\\) are independent and both have a variance of one, \\[ \\text{Var}(X_1) = \\text{Var}(a Z_1 + bZ_2) = a^2 \\text{Var}(Z_1) + b^2 \\text{Var}(Z_2) = a^2 + b^2 \\] Analogously, \\(\\text{Var}(X_2) = c^2 + d^2\\). Next we'll calculate the covariance. Again, since \\(Z_1\\) and \\(Z_2\\) are independent, \\[ \\begin{aligned} \\text{Cov}(X_1, X_2) &amp;= \\text{Cov}(a Z_1 + b Z_2, \\, c Z_1 + d Z_2) \\\\ &amp;= \\text{Cov}(aZ_1, cZ_1) + \\text{Cov}(bZ_2, dZ_2)\\\\ &amp;= ac \\,\\text{Var}(Z_1) + bd\\,\\text{Var}(Z_2) \\\\ &amp;= ac + bd \\end{aligned} \\] Now we collect these results into a matrix, the variance-covariance matrix of \\(\\mathbf{X}\\): \\[ \\text{Var}(\\mathbf{X}) \\equiv \\begin{bmatrix} \\text{Var}(X_1) &amp; \\text{Cov}(X_1, X_2) \\\\ \\text{Cov}(X_1, X_2) &amp; \\text{Var}(X_2) \\end{bmatrix} = \\begin{bmatrix} a^2 + b^2 &amp; ac + bd \\\\ ac + bd &amp; c^2 + d^2 \\end{bmatrix} \\] Multiplying through, this is precisely equal to \\(\\mathbf{A}\\mathbf{A}&#39;\\) \\[ \\mathbf{A}\\mathbf{A}&#39; = \\begin{bmatrix} a &amp; b \\\\ c &amp; d\\end{bmatrix} \\begin{bmatrix} a &amp; c \\\\ b &amp; d\\end{bmatrix} = \\begin{bmatrix} a^2 + b^2 &amp; ac + bd \\\\ ac + bd &amp; c^2 + d^2 \\end{bmatrix} \\] Finally, substituting the values from the example above A &lt;- matrix(c(2, 1, 1, 4), byrow = TRUE, ncol = 2, nrow = 2) A %*% t(A) ## [,1] [,2] ## [1,] 5 6 ## [2,] 6 17 The sample variance-covariance matrix from our simulations is quite close: var(x) ## x1 x2 ## x1 5.031082 6.031717 ## x2 6.031717 17.068951 6.4 The Cholesky Decomposition André-Louis Cholesky (1875-1918) 6.4.1 Going Backwards But we still don't know how to produce correlated normal draws! For example, suppose we need to generate two standard normals \\(X_1\\) and \\(X_2\\) with \\(\\text{Cor}(X_1, X_2) = 0.5\\). How can we achieve this? In effect, this requires us to work through the preceding exercise in reverse. That is, we need to find constants \\(a, b, c\\), and \\(d\\) such that \\[ \\begin{aligned} X_1 &amp;= a Z_1 + b Z_2 \\\\ X_2 &amp;= c Z_1 + d Z_2 \\end{aligned} \\] gives \\(\\text{Var}(X_1) = \\text{Var}(X_2) = 1\\) and \\(\\text{Cor}(X_1, X_2) = 0.5\\). There are in fact multiple choices of \\(a, b, c\\), and \\(d\\) that will do the trick. We'll choose the one that makes our lives the easiest. Suppose we set \\(a = 1\\) and \\(b = 0\\), so that \\(X_1 = Z_1\\). This ensures that \\(\\text{Var}(X_1) = 1\\). Now the question becomes: how can we choose \\(c\\) and \\(d\\) so that the variance of \\(X_2\\) is also one and the correlation between \\(X_1\\) and \\(X_2\\) equals 0.5? Using the calculations from the exercise above, \\(\\text{Var}(X_2) = c^2 + d^2\\) so we need to set \\(c^2 + d^2 = 1\\). Solving, this requires \\(d \\pm \\sqrt{1 - c^2}\\). Again let's keep things simple and choose the positive solution for \\(d\\). Using the calculations from the exercise above, \\(\\text{Cov}(X_1, X_2) = ac + bd\\) but since we've set \\(a = 1\\) and \\(b = 0\\) this simplifies to \\(\\text{Cov}(X_1, X_2) = c\\). Therefore \\(c = 0.5\\). Expressed in matrix form, we've shown that \\[ \\begin{bmatrix} 1 &amp; 0 \\\\ 0.5 &amp; \\sqrt{1 - 0.5^2}\\end{bmatrix} \\begin{bmatrix} Z_1 \\\\ Z_1 \\end{bmatrix} \\sim \\text{N}\\left(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 &amp; 0.5 \\\\ 0.5 &amp; 1\\end{bmatrix} \\right). \\] And this works exactly as expected: A &lt;- matrix(c(1, 0, 0.5, sqrt(1 - 0.5^2)), byrow = TRUE, nrow = 2, ncol = 2) x &lt;- t(A %*% t(z)) colnames(x) &lt;- c(&#39;x1&#39;, &#39;x2&#39;) cov(x) ## x1 x2 ## x1 1.0063783 0.5035995 ## x2 0.5035995 1.0047603 ggplot(data.frame(x)) + geom_density2d_filled(aes(x1, x2)) + coord_fixed() # forces the aspect ratio to be 1:1 so you see the ellipses! 6.4.2 Exercise Suppose we wanted the correlation between \\(X_1\\) and \\(X_2\\) to be \\(\\rho\\), a value that might not equal 0.5. Modify the argument from above accordingly. Show Solution All we have to do is replace \\(0.5\\) with \\(\\rho\\). Everything else goes through as before, so we obtain \\[ \\begin{bmatrix} 1 &amp; 0 \\\\ \\rho &amp; \\sqrt{1 - \\rho^2}\\end{bmatrix} \\begin{bmatrix} Z_1 \\\\ Z_1 \\end{bmatrix} \\sim \\text{N}\\left(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1\\end{bmatrix} \\right). \\] In our discussion above, \\(X_1\\) and \\(X_2\\) both had variance equal to one, so their correlation equaled their covariance. More generally, we may want to generate \\(X_1\\) with variance \\(\\sigma_1^2\\) and \\(X_2\\) with variance \\(\\sigma_2^2\\), where the covariance between them equals \\(\\sigma_{12}\\). Extend your reasoning from the preceding exercise to find an appropriate matrix \\(\\mathbf{A}\\) such that \\(\\mathbf{A}\\mathbf{Z}\\) has the desired variance-covariance matrix. Show Hint There are many matrices \\(\\mathbf{A}\\) that will do the trick. Following from our reasoning above, construct the one that obeys this convention: the diagonal elements of \\(\\mathbf{A}\\) should be positive, and the element above the diagonal should be zero. In other words: \\(b=0\\) and \\(a, d &gt; 0\\). Show Solution As above, we just have to work out the appropriate values of the constants \\(a, b, c\\), and \\(d\\) in the equations \\(X_1 = a Z_1 + b Z_2\\) and \\(X_2 = c Z_1 + d Z_2\\). Following our convention, \\(b=0\\) so \\(X_1 = a Z_1\\). Since \\(Z_1\\) is a standard normal, to give \\(X_1\\) a variance of \\(\\sigma_1^2\\) we need to set \\(a = \\sigma_1\\). By our convention, this is the positive square root of \\(\\sigma^2_1\\) so that \\(a &gt; 0\\). As we calculated in an earlier exercise,\\(\\text{Cov}(X_1, X_2) = ac + bd\\). Since \\(a = \\sigma_1\\) and \\(b = 0\\), this simplifies to \\(\\text{Cov}(X_1, X_2) = \\sigma_1 c\\). In order for this covariance to equal \\(\\sigma_{12}\\), we need to set \\(c = \\sigma_{12}/\\sigma_1\\). All that remains is to set the variance of \\(X_2\\) to \\(\\sigma_2^2\\). Again using a calculation from a previous exercise, \\(\\text{Var}(X_2) = c^2 + d^2\\). Substituting our solution for \\(c\\) and equating to \\(\\sigma_2^2\\) gives \\(d^2 = \\sigma_2^2 - \\sigma_{12}^2/\\sigma_1^2\\). To ensure that \\(d&gt;0\\) we set it equal to the positive square root: \\(d = \\sqrt{\\sigma_2^2 - \\sigma_{12}^2/\\sigma_1^2}\\). Collecting our results in matrix form: \\[ \\begin{bmatrix} \\sigma_1 &amp; 0 \\\\ \\sigma_{12}/\\sigma_1 &amp; \\sqrt{\\sigma_2^2 - \\sigma_{12}^2/\\sigma_1^2}\\end{bmatrix} \\begin{bmatrix} Z_1 \\\\ Z_1 \\end{bmatrix} \\sim \\text{N}\\left(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2\\end{bmatrix} \\right). \\] Check your calculations in the preceding part by transforming the simulations z from above into x such that x1 has variance one, x2 has variance four, and the correlation between them equals 0.4. Make a density plot of your result. Show Solution s1 &lt;- 1 s2 &lt;- 2 r &lt;- 0.4 s12 &lt;- r * (s1 * s2) A &lt;- matrix(c(s1, 0, s12 / s1, sqrt(s2^2 - s12^2 / s1^2)), byrow = TRUE, nrow = 2, ncol = 2) x &lt;- t(A %*% t(z)) colnames(x) &lt;- c(&#39;x1&#39;, &#39;x2&#39;) cov(x) ## x1 x2 ## x1 1.0063783 0.8059712 ## x2 0.8059712 4.0178160 cor(x) ## x1 x2 ## x1 1.0000000 0.4008149 ## x2 0.4008149 1.0000000 ggplot(data.frame(x)) + geom_density2d_filled(aes(x1, x2)) + coord_fixed() # forces the aspect ratio to be 1:1 so you see the ellipses! 6.4.3 What's the Square Root of a Matrix? In the preceding exercise, you found a way to take a \\((2\\times 2)\\) covariance matrix \\(\\boldsymbol{\\Sigma}\\) and decompose it according to \\(\\boldsymbol{\\Sigma} = \\mathbf{A} \\mathbf{A}&#39;\\) where \\(\\mathbf{A}\\) is a matrix of real numbers. Computing \\(\\mathbf{A}\\) is analogous to taking the positive square root of \\(\\sigma^2\\). There are two important facts about square roots. First, they don't always exist: there's no real number that equals \\(\\sqrt{-4}\\). Second, they're not necessarily unique: both \\(-2\\) and \\(2\\) equal four when squared. So is it always possible to write a variance-covariance matrix in the form \\(\\mathbf{A}\\mathbf{A}&#39;\\)? The answer turns out to be yes. Variance-covariance matrices are symmetric, but not every symmetric matrix is a variance-covariance matrix. Here's an example: \\[ \\begin{bmatrix} 4 &amp; 16 \\\\ 16 &amp; 9 \\end{bmatrix} \\] This can't be a variance-covariance matrix. Why not? Taking its correlation matrix gives M &lt;- matrix(c(4, 16, 16, 9), byrow = TRUE, nrow = 2, ncol = 2) cov2cor(M) ## [,1] [,2] ## [1,] 1.000000 2.666667 ## [2,] 2.666667 1.000000 and correlations definitely shouldn't be larger than one! In the same way that variances must be positive, variance-covariance matrices must be positive definite. A symmetric matrix \\(\\mathbf{M}\\) is called positive definite if \\(\\mathbf{v}&#39; \\mathbf{M} \\mathbf{v}&gt;0\\) for any vector \\(\\mathbf{v} \\neq \\mathbf{0}\\). This definition is effectively the matrix equivalent of a positive number. To see why the definition of positive definiteness makes sense, suppose that \\(\\mathbf{X} \\sim \\text{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})\\) where \\[ \\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}, \\quad \\boldsymbol{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2\\end{bmatrix} \\] as in the exercise above. Now define \\(Y = \\alpha X_1 + \\beta X_2\\). Unless \\(\\alpha\\) and \\(\\beta\\) are both zero, \\(Y\\) is a normal random variable so it certainly must have a positive variance. If we define \\(\\mathbf{v}&#39; = \\begin{bmatrix} \\alpha &amp; \\beta \\end{bmatrix}\\) then, a bit of algebra shows that \\(\\text{Var}(Y) = \\mathbf{v}&#39; \\boldsymbol{\\Sigma} \\mathbf{v}\\). (See the exercise below.) So requiring that \\(\\mathbf{v} \\boldsymbol{\\Sigma} \\mathbf{v}&#39; &gt; 0\\) is equivalent to requiring that any random variable \\(Y\\) that we form by taking a linear combination of \\(\\mathbf{X}\\) has a positive variance. There's nothing special about the two-dimensional case: this all applies in general. This is why a matrix that isn't positive definite can't be a variance-covariance matrix.22 Fortunately for anyone who wants to generate correlated normal draws, any positive definite matrix \\(\\boldsymbol{\\Sigma}\\), has a \"square root\" in the sense that it can be written as \\(\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}&#39;\\). But just as the square root of a real number isn't unique, \\(\\mathbf{A}\\) likewise isn't unique. For example, \\[ \\begin{bmatrix} 1 &amp; 0.5 \\\\ 0.5 &amp; 1 \\end{bmatrix} = \\mathbf{A}\\mathbf{A}&#39;= \\mathbf{B}\\mathbf{B}&#39;, \\quad \\mathbf{A} = \\begin{bmatrix} 1 &amp; 0 \\\\ 1/2 &amp; \\sqrt{3}/2 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\\\ \\frac{\\sqrt{2} - \\sqrt{6}}{4} &amp; \\frac{\\sqrt{2} + \\sqrt{6}}{4} \\end{bmatrix} \\] Adopting the convention that \\(\\mathbf{A}\\) should be lower triangular and that its diagonal elements should all be positive, however, does uniquely pin down \\(\\mathbf{A}\\): it picks out the first of these alternatives. This \\((2\\times 2)\\) example is a special case of the so-called Cholesky Decomposition: any positive definite matrix \\(\\Sigma\\) can be expressed as \\(\\Sigma = \\mathbf{L}\\mathbf{L}&#39;\\) where \\(\\mathbf{L}\\) is a lower triangular matrix with positive diagonal elements. The R command for the Cholesky decomposition is chol(). Just to make life confusing, chol() returns \\(\\mathbf{L}&#39;\\) rather than \\(\\mathbf{L}\\), Sigma &lt;- matrix(c(1, 0.5, 0.5, 1), byrow = TRUE, nrow = 2, ncol = 2) A &lt;- matrix(c(1, 0, 0.5, sqrt(3)/2), byrow = TRUE, nrow = 2, ncol = 2) identical(t(A), chol(Sigma)) ## [1] TRUE Here's how to remember this quirk: true to its name R returns an upper triangular matrix \\(\\mathbf{R}\\), so its version of Cholesky is \\(\\boldsymbol{\\Sigma} = \\mathbf{R} \\mathbf{R}&#39;\\). Armed with this knowledge, we can now generate more than two correlated normal random variables, as you'll discover in the following exercise. 6.4.4 Exercise Suppose that M is a \\((3 \\times 3)\\) matrix. Here's how to check if it's positive definite. First check M[1,1]; this element needs to be positive. Next check the \\((2\\times 2)\\) sub-matrix M[1:2,1:2]; its determinant, which you can calculate using det() needs to be positive. Finally check the determinant of M itself; this too needs to be positive. If M passes all the tests then it's positive definite! The same procedure works for smaller matrices and larger ones: we check that the determinant of each leading principal minor is positive. Here are two matrices, one of which is positive definite and one of which is not. Determine which is which. \\[ \\mathbf{A} =\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\ 2 &amp; 2 &amp; 1\\\\ 3 &amp; 1 &amp; 3 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 3 &amp; 2 &amp; 1 \\\\ 2 &amp; 3 &amp; 1 \\\\ 1 &amp; 1 &amp; 3 \\end{bmatrix} \\] Show Solution Both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are symmetric. The matrix \\(\\mathbf{A}\\) is not positive definite because its determinant is negative: A &lt;- matrix(c(1, 2, 3, 2, 2, 1, 3, 1, 3), byrow = TRUE, nrow = 3, ncol = 3) det(A) ## [1] -13 The matrix \\(\\mathbf{B}\\) is positive definite since B[1, 1] is positive, the determinant of B[1:2, 1:2] is positive, and the determinant of B itself is positive: B &lt;- matrix(c(3, 2, 1, 2, 3, 1, 1, 1, 3), byrow = TRUE, nrow = 3, ncol = 3) det(B[1:2, 1:2]) ## [1] 5 det(B) ## [1] 13 Use the positive definite matrix from the preceding exercise, along with the R function chol(), to make 100,000 draws from the corresponding 3-dimensional multivariate normal distribution. Use cov() to check your work. Show Solution R &lt;- chol(B) L &lt;- t(R) n_sims &lt;- 1e5 set.seed(29837) z &lt;- matrix(rnorm(3 * n_sims), nrow = n_sims, ncol = 3) x &lt;- t(L %*% t(z)) cov(x) ## [,1] [,2] [,3] ## [1,] 2.986340 1.985103 1.006587 ## [2,] 1.985103 2.983812 1.001514 ## [3,] 1.006587 1.001514 2.997953 Install the package mvtnorm() and read the help file for rmvnorm(). This this function to repeat the preceding exercise \"the easy way.\" Again, check your work using cov(). Show Solution #install.package(&#39;mvtnorm&#39;) library(mvtnorm) set.seed(29837) x_alt &lt;- rmvnorm(n_sims, sigma = B) cov(x_alt) ## [,1] [,2] [,3] ## [1,] 2.984944 1.997268 1.003196 ## [2,] 1.997268 2.992995 1.006207 ## [3,] 1.003196 1.006207 3.010338 In the discussion above I claimed that \\(\\text{Var}(Y) = \\mathbf{v}&#39;\\boldsymbol{\\Sigma}\\text{v}\\). Work out the algebra to establish this result. Show Solution \\[ \\begin{aligned} \\mathbf{v}&#39; \\boldsymbol{\\Sigma} \\mathbf{v} &amp;= \\begin{bmatrix} \\alpha &amp; \\beta \\end{bmatrix} \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2\\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}\\\\ &amp;= \\alpha^2 \\text{Var}(X_1) + 2\\alpha \\beta\\, \\text{Cov}(X_1, X_2) + \\beta^2 \\text{Var}(X_2) \\\\ &amp;= \\text{Var}(\\alpha X_1 + \\beta X_2) = \\text{Var}(Y). \\end{aligned} \\] 6.5 Epilogue So why did I pack all of this matrix algebra into a book that is ostensibly about empirical research methods? There are several reasons. First, you will one day encounter an error in your code telling you that a certain matrix is not positive definite! You need to understand what this means and why it's important. Second, the Cholesky decomposition comes up all over the place in applied econometrics and statistics. Now that you understand how to build it up from scratch in the \\((2\\times 2)\\) case, I hope you will find the general idea intuitive rather than mystifying when you next encounter it. Third, and most importantly, the approach of building up everything from scratch by combining Cholesky with independent standard normals pays big dividends in helping you understand the multivariate normal distribution. To show you what I mean, here are two important facts that you can now deduce effectively for free from what you've worked out above. 6.5.1 Affine Transformations of a Multivariate Normal You worked out the two-dimensional case above, but the same holds in \\(p\\) dimensions: if \\(\\mathbf{Z}\\) is a \\(p\\)-dimensional vector of standard normal random variables, \\(\\mathbf{A}\\) is a \\((p\\times p)\\) matrix of constants and \\(\\boldsymbol{\\mu}\\) is a \\(p\\)-dimensional vector of constants, then \\(\\mathbf{X} \\equiv \\boldsymbol{\\mu} + \\mathbf{A} \\mathbf{Z}\\) is a multivariate normal random vector with mean vector \\(\\boldsymbol{\\mu}\\) and variance-covariance matrix \\(\\boldsymbol{\\Sigma} = \\mathbf{A}\\mathbf{A}&#39;\\). The shifting and scaling operation that we use to go from \\(\\mathbf{Z}\\) to \\(\\mathbf{X}\\) has a name: it's called an affine transformation. So what happens if we apply an affine transformation to \\(\\mathbf{X}\\)? Suppose that \\(\\mathbf{Y} = \\mathbf{c} + \\mathbf{B} \\mathbf{X}\\) where \\(\\mathbf{c}\\) and \\(\\mathbf{B}\\) are constant. What is the distribution of \\(\\mathbf{Y}\\)? This turns out to be a free extra. Writing \\(\\mathbf{X}\\) in terms of \\(\\mathbf{Z}\\) reveals that \\(\\mathbf{Y}\\) is itself an affine transformation of \\(\\mathbf{Y}\\): \\[ Y \\equiv \\mathbf{c} + \\mathbf{B} \\mathbf{X} = \\mathbf{c} + \\mathbf{B} (\\boldsymbol{\\mu} + \\mathbf{A} \\mathbf{Z}) = (\\mathbf{c} + \\mathbf{B} \\boldsymbol{\\mu}) + (\\mathbf{B}\\mathbf{A}) \\mathbf{Z} \\] Now we can simply apply our result from above with \\(\\mathbf{c} + \\mathbf{B} \\boldsymbol{\\mu}\\) as the \"shift vector\" and \\(\\mathbf{B}\\mathbf{A}\\) as the \"scaling matrix.\" It follows that \\(\\mathbf{Y}\\) is a multivariate normal with mean \\(\\mathbf{c} + \\mathbf{B} \\boldsymbol{\\mu}\\) and variance-covariance matrix equal to \\[ (\\mathbf{B} \\mathbf{A})(\\mathbf{B} \\mathbf{A})&#39; = \\mathbf{B}\\mathbf{A} \\mathbf{A}&#39; \\mathbf{B}&#39;. \\] Since the transpose of a product equals the product of the transposes in the reverse order. In other words, affine transformations of normals are always normal. 6.5.2 Conditional Distributions of Bivariate Normal Suppose that \\(X_1\\) and \\(X_2\\) are jointly normal with mean vector \\(\\boldsymbol{\\mu}\\) and variance-covariance matrix \\(\\boldsymbol{Sigma}\\). What is the conditional distribution of \\(X_2\\) given \\(X_1\\)? There's a formula you could memorize, but if you're anything like me you'll forget this when you need it the most. Instead, let's use Cholesky to help us. If \\(\\boldsymbol{\\Sigma}\\) is positive definite, then we can construct \\(\\mathbf{A}\\) such that \\[ \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix} + \\begin{bmatrix} a &amp; 0 \\\\ c &amp; d \\end{bmatrix} \\begin{bmatrix} Z_1 \\\\ Z_2 \\end{bmatrix} \\] where \\(Z_1, Z_2\\) are independent standard normals, \\(\\mu_1\\) and \\(\\mu_2\\) are the means of \\(X_1\\) and \\(X_2\\), and \\(a, c, d\\) are constants with such that \\(a, d &gt; 0\\). This implies that \\(X_1 = \\mu_1 + aZ_1\\) and \\(X_2 = \\mu_2 + c Z_1 + d Z_2\\). So what does it mean to condition on \\(X_1 = x\\)? Since \\(a\\) and \\(\\mu_1\\) are constants, the only randomness in \\(X_1\\) comes from \\(Z_1\\). Thus, rearranging the expression for \\(X_1\\), we see that conditioning on \\(X_1 = x\\) is the same thing as conditioning on \\(Z_1 = (x - \\mu_1)/a\\). Substituting this into the expression for \\(X_2\\) gives \\[ X_2|(X_1 = x) = \\mu_2 + \\frac{c}{a} (x - \\mu_1) + d Z_2. \\] The only random variable in this expression is \\(Z_2\\): everything else is a constant. Indeed, conditional on \\(X_1 = x\\) we see that \\(X_2\\) is merely an affine transformation of \\(Z_2\\), so \\[ X_2|(X_1 = x) \\sim \\text{N}\\left(\\mu_{2|1}, \\sigma^2_{2|1} \\right), \\quad \\mu_{2|1}\\equiv \\mu_2 + \\frac{c}{a}(x - \\mu_1), \\quad \\sigma^2_{2|1}\\equiv d^2 \\] Two things jump out immediately. First, the conditional mean of \\(X_1\\) depends linearly on \\(x\\), the value that \\(X_1\\) takes on. Second, the conditional variance of \\(X_2\\) does not depend on \\(x\\). Substituting the values for \\(a\\) and \\(c\\) that you worked out in the exercise above we see that \\(a/c = \\sigma_{12}/\\sigma_1^2\\). This is the slope of the regression line from a population linear regression of \\(X_2\\) on \\(X_1\\), call it \\(\\beta\\). Substituting into the expression for \\(\\mu_{1|2}\\) gives \\[ \\begin{aligned} \\mu_{2|1} &amp;= \\left[\\mu_2 - \\left(\\frac{\\sigma_{12}}{\\sigma_1^2}\\right)\\mu_1\\right] + \\left( \\frac{\\sigma_{12}}{\\sigma_1^2}\\right) x \\\\ &amp;= \\left[E(X_2) - \\beta E(X_1)\\right] + \\beta x\\\\ &amp;= \\alpha + \\beta x \\end{aligned} \\] where we recognize \\(\\alpha\\) as the intercept of the population regression line. Similarly, substituting the value for \\(d^2\\) that you worked out in the exercises above, we obtain \\[ \\begin{aligned} \\sigma^2_{2|1} &amp;= \\sigma_2^2 - \\frac{\\sigma_{12}^2}{\\sigma_1^2} = \\sigma_2^2\\left( 1 - \\frac{\\sigma_{12}^2}{\\sigma_1^2 \\sigma_2^2}\\right) = \\sigma_2^2 (1 - \\rho_{12}^2). \\end{aligned} \\] And what is \\(\\rho_{12}^2\\)? It's the R-squared for the population linear regression of \\(X_2\\) on \\(X_1\\)! Linear regression is intrinsically related to the properties of multivariate normal distributions, giving you yet another way of remembering this useful result! Okay, fine: if \\(a = b = 0\\) then \\(X\\) merely equals the constant \\(c\\). But in any other case, \\(X\\) is normally distributed.↩︎ Strictly speaking a variance-covariance matrix only needs to be positive semi-definite, i.e. \\(\\mathbf{v}&#39; \\boldsymbol{\\Sigma} \\mathbf{v}\\geq 0\\) for any \\(\\mathbf{v} \\neq \\mathbf{0}\\). But if there's a non-zero vector \\(\\mathbf{v}\\) such that \\(\\mathbf{v} \\boldsymbol{\\Sigma} \\mathbf{v} = \\mathbf{0}\\) this means that one of the random variables contained in \\(\\mathbf{X}\\) is redundant in that it is perfectly correlated with some linear combination of the others. Such a variance-covariance matrix gives rise to what is called a singular normal distribution. These do come up from time to time, but discussing them further here would take us too far afield. There's already more than enough matrix algebra in this lesson!↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
