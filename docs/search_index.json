[["index.html", "Empirical Research Methods Preface About This Book Pre-requisites Why not Stata? Why not Matlab, Julia, or Python?", " Empirical Research Methods Francis J. DiTraglia 2022-02-27 Preface About This Book Supervising undergraduate, masters-level, and doctoral research students has shown me just how many of the skills that I take for granted in my day-to-day work were never taught in a course, but acquired through years of painful trial-and-error. You've probably heard that \"the only way to learn how to do research is by doing research.\" Indeed: classroom exercises are always somewhat artificial, and there is no substitute for getting your hands dirty with a problem that really matters to you. But trial-and-error is a slow and clumsy way to gain proficiency, and throwing students in at the deep end is neither a recipe for academic success nor for mental well-being. The goal of this book is to put some structure around the process through which students learn to do empirical work in economics, building a strong foundation for later self-directed reading and research. The book is divided into twenty-odd short chapters called lessons, each designed to take between one and two hours to complete. Broadly speaking, the material is a mix of applied econometrics, data science, and research skills. In keeping with the Swiss Army Knife logo, the idea is to teach you lots of little things that will come in handy later. While the topics covered below are something of a miscellany, there are strong connections between the lessons. For best results, complete them in order. A key theme that runs throughout the lessons is the importance of reproducible research using open-source tools. Reproducible research is about creating a clean and fully-documented path from raw data to final results, making errors less likely to occur and easier to find when they do. It also allows other researchers, or our future selves, to build on past work, expanding the sum total of knowledge. Of course I can only replicate your research if I can run your code, and this is why open-source software is so important. Fortunately there are many fantastic open-source programming languages to choose from. This book uses R, the lingua franca of statistics and an increasingly popular choice among economists. Pre-requisites This book does not assume advanced knowledge of programming, mathematics, or econometrics, but it does have some pre-requisites. My target audience is first-year graduate students and final-year undergraduates in economics. At Oxford, I use this book to teach a first-year master's level course on Empirical Research Methods that comes after students have completed 16 weeks of basic statistics and econometrics. I assume that you've taken an econometrics course that uses matrix notation and that you have basic familiarity with R programming. If you need to brush up on econometrics, I recommend Marno Verbeek's Guide to Modern Econometrics. I've linked to the third edition because it is particularly inexpensive to buy a used copy, but any edition will do. At a more advanced level, Bruce Hansen's two volume series Econometrics is both excellent and free to download online. If you haven't used R before or feel the need for a bit of review, I suggest reading Hands-On Programming with R. It's free, short, and will get you up to speed quickly. Why not Stata? Given that much of the material discussed below falls under the broad category of \"applied microeconometrics\" you may wonder why I chose R rather than Stata. Indeed, Stata is easy-to-use, and makes it relatively painless to implement \"textbook\" microeconometric methods.1 So why don't I like Stata? Before beginning my polemic I should be absolutely clear that Stata users are not bad people: hate the sin, love the sinner. Here begins the sermon. First, Stata is expensive. The price for a Business single-user Stata license is $765 per year.2 If you want support for multicore computing, the price is even higher: an 8-core version of Stata costs $1,395 annually. There is no discount for Government or nonprofits, but as an Oxford faculty member, I can obtain an 8-core version of Stata for the low price of $595 per year, or around 9% of my annual research allowance. In contrast, the tools that we will learn in this book, mainly R and C++, are completely free. This is particularly important in the modern world of high-performance cluster computing. If you're considering running your code on a multicore machine on Amazon, Google, or Microsoft cloud servers, you don't want to pay a software license fee for every core that you use. Second, Stata is almost comically behind the times. Let's see what's new in Stata version 16, released in February 2020.3 At the top of the list is the LASSO, a wildly popular technique for high-dimensional regression. Rob Tibshirani developed this method in a seminal paper from 1996, so it only took 24 years for it to be incorporated into Stata.4 Fortunately, Tibshirani and his co-authors made it easy for Stata, by releasing open-source software to implement the LASSO and related methods in R over a decade ago.5 Next on the list of new Stata features is linear programming, a technique that came to prominence in the late 1940s.6 Stata 16 also has the ability to call \"any Python package\"--something you can do for free in R using reticulate or in Python itself for that matter--and \"truly reproducible reporting.\" Reproducible reporting is incredibly valuable, and it's something that we'll cover in detail below. It's also been available in R, completely free of charge, since at least 2002.7 I suppose we shouldn't expect too much of a statistical computing package that only added support for matrix programming in 2005, a full 20 years after Stata version 1.0.8 Third, Stata is a black box. Because the underlying source code is kept secret, there's no way for a Stata user to know for certain what's happening under the hood. A few years ago I tried to determine precisely what instrument set Stata was using in its implementation of a well-known dynamic panel estimator. The documentation was vague, so I resorted to reverse-engineering the Stata results by trial-and-error in R. I never did get the results to match perfectly. In contrast, if you're not sure what a particular R function or package is doing, you can simply read the source code and find out. Fourth, and most importantly, Stata makes it hard to share with others. If I don't own a copy of Stata, I can't replicate your work. Even if I do own a copy of Stata, I still may not be able to so do: Stata's proprietary binary data formats are updated fairly regularly and do not maintain backwards compatibility. Datafiles created in Stata version 16, for example, cannot be opened in Stata 13. Indeed, depending on the number of variables included in your dataset, Stata 16 files cannot necessarily be opened even in Stata 15. Fortunately, as we'll see below, intrepid open-source programmers have developed free software to unlock data from Stata's proprietary and ever-changing binary formats. Why not Matlab, Julia, or Python? Unlike Stata, Matlab is a bona fide programming language and a fairly capable one at that. Nevertheless, my other critiques of Stata from above still apply: Matlab is extremely expensive, and it's not open source. In contrast, I have nothing bad to say about Python and Julia: they're great languages and you should consider learning one or both of them!9 In the end I decided to choose one language and R struck me as the best choice for the moment. In five or ten years time, I could easily imagine re-writing this book in Julia, but as of this writing R has the advantage of maturity and a large, and extremely supportive user community. Even if you ultimately decide that R isn't for you, fear not! After learning the material in this book, you'll find it fairly easy to transition to Python or Julia, should you so choose. Now let's get started! Arguably, Stata is too easy to learn precisely because of the incentives faced by a software developer with monopoly power: see Hal Varian's paper: Economic Incentives in Software Design.↩︎ These figures were accurate as of March 2021. For the latest prices, see https://www.stata.com/order/dl/.↩︎ https://www.stata.com/new-in-stata/↩︎ Tibshirani (1996) - Regression Shrinkage and Selection via the Lasso↩︎ Friedman et al (2010) - Regularization Paths for Generalized Linear Models via Coordinate Descent↩︎ For a history of linear programming, see Dantzig (1983). To be completely fair, the linear programming algorithm implemented in Stata 16 was only developed in 1992, a lag of merely 28 years.↩︎ Reproducible reporting in R started with sweave. These days we have a fantastic successor package called knitr, which I cover below.↩︎ The \"Mata\" programming language was added in Stata 9: https://www.stata.com/stata9/. For a timeline of Stata versions, see https://www.stata.com/support/faqs/resources/history-of-stata/.↩︎ A good resources aimed at economists is https://quantecon.org, available in both Python and Julia versions.↩︎ "],["the-hot-hand.html", "Lesson 1 The Hot Hand 1.1 Drawing Random Data in R 1.2 The Skeleton of a Simulation Study 1.3 Your Turn: Simulating the Hot Hand 1.4 Further Reading 1.5 Mini Projects 1.6 DRAFT MATERIAL", " Lesson 1 The Hot Hand If you've read 2002 Nobel Laureate Daniel Kahneman's best-selling book Thinking Fast and Slow, you may remember this passage about the hot hand illusion, a supposed illustration of the human tendency to see patterns in random noise: Amos [Tversky] and his students Tom Gilovich and Robert Vallone caused a stir with their study of misperceptions of randomness in basketball. The \"fact\" that players occasionally acquire a hot hand is generally accepted by players, coaches, and fans. The inference is irresistible: a player sinks three or four baskets in a row and you cannot help forming the causal judgment that this player is now hot, with a temporarily increased propensity to score. Players on both teams adapt to this judgment—teammates are more likely to pass to the hot scorer and the defense is more likely to doubleteam. Analysis of thousands of sequences of shots led to a disappointing conclusion: there is no such thing as a hot hand in professional basketball, either in shooting from the field or scoring from the foul line. Of course, some players are more accurate than others, but the sequence of successes and missed shots satisfies all tests of randomness. The hot hand is entirely in the eye of the beholders, who are consistently too quick to perceive order and causality in randomness. The hot hand is a massive and widespread cognitive illusion. The research that Kahneman mentions was published in a famous paper by Gilovich, Vallone &amp; Tversky (1985), and later summarized for a general audience in Gilovich &amp; Tversky (1989). The abstract of the original paper says it all: Basketball players and fans alike tend to believe that a player's chance of hitting a shot are greater following a hit than following a miss on the previous shot. However, detailed analyses of the shooting records of the Philadelphia 76ers provided no evidence for a positive correlation between the outcomes of successive shots. The same conclusions emerged from free-throw records of the Boston Celtics, and from a controlled shooting experiment with the men and women of Cornell's varsity teams. The outcomes of previous shots influenced Cornell players' predictions but not their performance. The belief in the hot hand and the \"detection\" of streaks in random sequences is attributed to a general misconception of chance according to which even short random sequences are thought to be highly representative of their generating process. Between 1985 and 2011, when Kahneman's book was published, this result was replicated numerous times under a variety of different conditions, making it one of the better-documented biases in human decision-making. But it turns out that the hot-hand illusion is itself an illusion. In a recent issue of Econometrica, Miller &amp; Sanjurjo (2018) point out a subtle but consequential error in the statistical tests used in the hot hand literature. It turns out that these tests were biased against detecting evidence of a hot hand, even if it did in fact exist. Correcting this mistake and re-analyzing Gilovich, Vallone and Tversky's original dataset \"reveals significant evidence of streak shooting, with large effect sizes.\" The hot hand is real, and the literature has now shifted to trying to estimate the size of the effect.10 There are some helpful lessons that we can draw from this episode. First, we all make mistakes--even Nobel Laureates! Second, successful replications tell us less than we might hope: they could easily reproduce the bias of the original study. But in my view, the real lesson of the hot hand affair is much simpler: you should always run a simulation study. The probabilistic error that led so many researchers to draw the wrong conclusion about the hot hand is really quite subtle.11 But at the same time, anyone who knows basic programming could have detected the mistake in five minutes if only they had bothered to look. In economics and statistics, simulation is a superpower. It helps us to understand our models, check for mistakes, and make unexpected connections, some of which may even lead to new theoretical results. If you'll pardon my continuation of the Swiss Army Knife metaphor, simulation is the knife: arguably the most useful tool in your toolbox. In this lesson, we'll cover some basic tools for carrying out a simulation experiment in R and use them to shed some light on the illusion of the hot hand illusion. The key R functions introduced in this lesson are as follows: sample() rbinom() rnorm() set.seed() identical() replicate() expand.grid() Map() mcMap() from the parallel package 1.1 Drawing Random Data in R Before we can use simulations to study the illusion of the hot hand illusion, we need to review the basics of drawing random data in R. We'll examine the functions sample(), rbinom() and set.seed() in detail. I'll also point you to a large number functions for simulating from well-known probability distributions in R. Finally, you'll have a chance to practice what you've learned by solving a few short exercises. 1.1.1 sample() R has many helpful built-in functions for making simulated random draws. The simplest is sample(), which makes size random draws without replacement from a vector x. To test this out, I'll create a very simple vector my_vector &lt;- c(&#39;there&#39;, &#39;is&#39;, &#39;no&#39;, &#39;largest&#39;, &#39;integer&#39;) The following line of code makes two draws without replacement from my_vector sample(x = my_vector, size = 2) # without replacement ## [1] &quot;is&quot; &quot;integer&quot; If I run the same line of code again, I may not get the same result: it's random!12 sample(x = my_vector, size = 2) # without replacement ## [1] &quot;integer&quot; &quot;largest&quot; To draw with replacement, set replace = TRUE sample(x = my_vector, size = 7, replace = TRUE) # with replacement ## [1] &quot;there&quot; &quot;largest&quot; &quot;largest&quot; &quot;no&quot; &quot;integer&quot; &quot;integer&quot; &quot;largest&quot; As usual in R, the argument names x, size, and replace are optional. But it is considered good coding style to explicitly supply an argument name whenever we're overriding a function's default behavior. This makes it easier for anyone reading our code to understand what's happening. Since sample() defaults to making draws without replacement, it's a good idea to write replace = TRUE rather then simply TRUE. But even without writing replace =, the code will still work as long as we supply all of the arguments in the correct order: sample(my_vector, 7, TRUE) # bad style ## [1] &quot;largest&quot; &quot;no&quot; &quot;is&quot; &quot;is&quot; &quot;there&quot; &quot;no&quot; &quot;largest&quot; sample(my_vector, 7, replace = TRUE) # good style ## [1] &quot;largest&quot; &quot;integer&quot; &quot;there&quot; &quot;largest&quot; &quot;largest&quot; &quot;no&quot; &quot;largest&quot; 1.1.2 Probability Distributions in R As a programming language targeted at statistical applications, R supplies built-in functions for all of the most common probability distributions.13 These functions follow a consistent naming convention. They being with either d, p, q, or r and are followed by an abbreviated name for a particular probability distribution. The prefix d denotes a density function (or mass function for a discrete distribution); p denotes a cumulative distribution function (CDF), q denotes a quantile function, and r denotes a function for making random draws from a particular distribution. For example: dunif() gives the probability density function of a uniform random variable, pnorm() gives the CDF of a normal random variable, qchisq() gives the quantile function of a Chi-squared, and rbinom allows us to make random draws from a Binomial distribution. The following table gives a full list of the relevant commands. R commands Distribution d/p/q/rbeta Beta d/p/q/rbinom Binomial d/p/q/rcauchy Cauchy d/p/q/rchisq Chi-Squared d/p/q/rexp Exponential d/p/q/rf F d/p/q/rgamma Gamma d/p/q/rgeom Geometric d/q/p/rhyper Hypergeometric d/p/q/rlogis Logistic d/p/q/rlnorm Log Normal d/p/q/rnbinom Negative Binomial d/p/q/rnorm Normal d/p/q/rpois Poisson d/p/q/rt Student's t d/p/q/runif Uniform d/p/q/rweibull Weibull There's a single help file for all of the d/p/q/r functions for a particular distribution. For example, if you enter ?dbeta at the console you'll be shown the help files for dbeta(), pbeta(), qbeta(), and rbeta(). To get a feel for how these functions work, let's take a look at rbinom(), the function for drawing from a Binomial distribution. Recall that a Binomial\\((m,p)\\) random variable equals the number of heads in \\(m\\) independent tosses of a coin with \\(\\mathbb{P}(\\text{Heads})=p\\). Or to use a bit of probability jargon, it equals the number of successes in \\(m\\) independent Bernoulli trials, each with probability of success \\(p\\).14 If \\(X\\) is a Binomial random variable with parameters \\(m\\) and \\(p\\), traditionally written as \\(X \\sim \\text{Binomial}(m, p)\\) then \\(X\\) must take on a value in the set \\(\\{0, 1, 2, ..., m\\}\\) and the probability that it takes on a particular value \\(x\\) in this set is \\[ \\mathbb{P}(X = x) = \\binom{m}{x} p^x (1 - p)^x \\] The function rbinom() makes random draws with the probabilities given by this formula. Its takes three arguments: size is the number of trials, \\(m\\) in the formula, prob is the probability of success, \\(p\\) in the formula, and n is the desired number of Binomial draws. For example, we can make a single draw from a Binomial\\((m = 10, p =1 /2)\\) distribution as follows rbinom(n = 1, size = 10, prob = 0.5) ## [1] 5 and fifteen draws from the same distribution by changing n to 10 rbinom(n = 15, size = 10, prob = 0.5) ## [1] 7 5 4 5 4 6 7 7 4 3 6 5 5 2 3 It's important not to confuse n with size. The former tells R how many Binomial draws to make. The latter tells R the value of the parameter \\(m\\) of the Binomial\\((m, p)\\) distribution. Perhaps you remember that if \\(X \\sim \\text{Binomial}(m, p)\\) then \\(\\mathbb{E}[X] = mp\\) and \\(\\text{Var}= np(1-p)\\). We can approximate these results numerically by simulating a large number of draws, say 5000, from a Binomial distribution: m &lt;- 20 p &lt;- 0.25 n_sims &lt;- 5000 sim_draws &lt;- rbinom(n_sims, m, p) and then comparing the theoretical value for \\(\\mathbb{E}(X)\\) to a simulation-based approximation: c(EV_Theoretical = m * p, EV_Simulation = mean(sim_draws)) ## EV_Theoretical EV_Simulation ## 5.0000 4.9574 and similarly for \\(\\text{Var}(X)\\) c(Var_Theoretical = m * p * (1 - p), Var_Simulation = var(sim_draws)) ## Var_Theoretical Var_Simulation ## 3.750000 3.709927 Reassuringly, our simulation results are very close to the theoretical values. They would be even closer if we used a larger value for n_sims. 1.1.3 set.seed() A key theme of this book is the importance of reproducible research. Anyone else who wants to check your work should be able to obtain exactly the same results as you did by running your code. But this seems to be at odds with the idea of simulating random data. For example, if I run rbinom(10, 4, 0.6) repeatedly, I'll most likely get different results each time: rbinom(10, 4, 0.6) ## [1] 2 3 2 2 2 2 1 4 4 1 rbinom(10, 4, 0.6) ## [1] 3 2 3 1 4 4 1 2 3 2 rbinom(10, 4, 0.6) ## [1] 3 2 3 3 3 3 2 3 4 1 The function set.seed() allows us to ensure that we obtain the same simulation draws whenever we re-run the same simulation code. To use it, we simply choose a seed, any integer between negative and positive \\((2^{31} - 1)\\), and supply it as an argument to set.seed(). Simulation draws made on a computer aren't really random: they're only pseudo-random. This means that they \"look\" random and pass statistical tests for randomness but are in fact generated by a completely deterministic algorithm. Setting the seed sets the initial condition of the pseudorandom number generator. Because the algorithm is deterministic, the same initial condition always leads to the same results. This is what allows us to replicate our simulation draws. Each time we make another draw, the seed changes. But we can always return it to its previous state using set.seed(). For example, suppose I set the seed to 1. and re-run my code from above as follows set.seed(1) x1 &lt;- rbinom(10, 4, 0.6) x1 ## [1] 3 3 2 1 3 1 1 2 2 4 If I run rbinom(10, 4, 0.6) again, I will most likely not get the same result, because the state of the pseudorandom number generator has changed: x2 &lt;- rbinom(10, 4, 0.6) x2 ## [1] 3 3 2 3 2 2 2 0 3 2 identical(x1, x2) # safe/reliable way to test if two objects are exactly equal ## [1] FALSE but if I reset the seed to 1 I'll obtain exactly the same result as before: set.seed(1) x3 &lt;- rbinom(10, 4, 0.6) x3 ## [1] 3 3 2 1 3 1 1 2 2 4 identical(x3, x1) ## [1] TRUE Whenever you write simulation code, start by choosing a seed and adding the line set.seed(MY-SEED-GOES-HERE) to the top of your R script. You'll often see people use set.seed(12345) or set.seed(54321). When I'm not feeling lazy, I like to generate a truly random number to use as my seed. The website random.org provides free access to bona fide random numbers generated from atmospheric noise. The \"True Random Number Generator\" on the top right of their main page allows you to make uniform integer draws on a range from \"Min\" to \"Max.\" Using the widest possible range, \\(\\pm1\\times 10^9\\), I generated the seed 420508570 which I'll use in the following exercises. 1.1.4 Exercises Set your seed to 420508570 at the start of your solution code for each of these exercises to obtain results that match the solutions. Run sample(x = my_vector, size = 10). What happens and why? Show Solution R will throw an error. You can't make ten draws without replacement from a set of five objects: set.seed(420508570) sample(x = my_vector, size = 10) ## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39; Write a line of code that makes five draws without replacement from the set of numbers \\(\\{1, 2, 3, ..., 100\\}\\). Show Solution set.seed(420508570) sample(1:100, 5) ## [1] 100 67 51 44 12 Create a vector of thirty elements called urn that represents an urn containing ten blue balls and twenty red balls. Draw five balls with replacement from urn and store the draws in a vector called draws. Then write a line of code to count up the number of blue balls in draws. Show Hint Use rep() and c() to construct urn. Use == and sum() to count up the number of blue balls in draws. See the relevant help files for details, e.g. ?rep. Show Solution set.seed(420508570) urn &lt;- c(rep(&#39;blue&#39;, 10), rep(&#39;red&#39;, 20)) draws &lt;- sample(urn, 5, replace = TRUE) draws ## [1] &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; sum(draws == &#39;blue&#39;) ## [1] 2 Make 1000 draws from a normal distribution with mean 5 and variance 4 and store them in a vector called normal_sims. Calculate the mean and variance of your draws, and plot a histogram. Show Hint Consult the help file for rnorm() paying close attention to the fact that R specifies the normal distribution in terms of a mean and standard deviation rather than a mean and variance. You can plot a histogram with any number of functions: e.g. the base R function hist() or qplot() from the ggplot2 package. Show Solution set.seed(420508570) normal_sims &lt;- rnorm(1000, 5, 2) # Variance = 4; Standard Dev. = 2 mean(normal_sims) ## [1] 4.895722 var(normal_sims) ## [1] 3.912262 ggplot2::qplot(normal_sims, bins = 25) There is no built-in R function called rbern() for simulating draws from the Bernoulli Distribution with probability of success \\(p\\). Write one of your own and use it to make ten Bernoulli(0.8) draws. Your function rbern() should take two arguments: the number of draws n and the probability of success p. Show Hint There are various ways to do this. The simplest is by setting the arguments of rbinom() appropriately. Show Solution rbern &lt;- function(n, p) { # Make n random draws from a Bernoulli(p) distribution rbinom(n, size = 1, prob = p) } set.seed(420508570) rbern(10, 0.8) ## [1] 1 1 1 1 0 1 1 1 1 1 1.2 The Skeleton of a Simulation Study While the specific details will vary, nearly every simulation study has the same basic structure: Generate simulated data. Calculate an estimate from the simulated data. Repeat steps 1 and 2 many times, saving each of the estimates. Summarize the results. Thinking in terms of this structure helps us to write code that is easier to understand, easier to generalize, and faster to run. The key is to break these steps down into functions that carry out a single, well-defined task. Generally these will include: A function to generate simulated data. A function to calculate an estimate from the data. A function that repeatedly calls i. and ii. and summarizes the results. This may sound a bit abstract, so in the remainder of this section we'll walk through the details in a simple example: estimating the bias of the maximum likelihood estimator for the variance of a normal distribution. Along the way we'll explore three extremely helpful R functions for carrying simulation studies: replicate(), expand.grid(), and Map(). In the next section you'll apply what you've learned to the hot hand example. 1.2.1 A Biased Estimator of \\(\\sigma^2\\) My introductory statistics students often ask me why the sample variance, \\(S^2\\), divides by \\((n-1)\\) rather than the sample size \\(n\\): \\[ S^2 \\equiv \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 \\] The answer is that dividing by \\((n-1)\\) yields an unbiased estimator: if \\(X_1, ..., X_n\\) are a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then \\(\\mathbb{E}[S^2] = \\sigma^2\\). So what would happen if we divided by \\(n\\) instead? Consider the estimator \\(\\widehat{\\sigma}^2\\) defined by \\[ \\widehat{\\sigma}^2 \\equiv \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2. \\] If \\(X_i \\sim \\text{Normal}(\\mu, \\sigma^2)\\) then \\(\\widehat{\\sigma}^2\\) is in fact the maximum likelihood estimator for \\(\\sigma^2\\). With a bit of algebra, we can show that \\(\\mathbb{E}[\\widehat{\\sigma}^2] = (n-1)\\sigma^2/n\\) which clearly does not equal the population variance.15 It follows that \\[ \\text{Bias}(\\widehat{\\sigma}^2) \\equiv \\mathbb{E}[\\widehat{\\sigma}^2 - \\sigma^2] = -\\sigma^2/n \\] so \\(\\widehat{\\sigma}^2\\) is biased downwards. Because the bias goes to zero as the sample size grows, however, it is still a consistent estimator of \\(\\sigma^2\\). Another way to see that \\(\\widehat{\\sigma}^2\\) is biased is by carrying out a simulation study. To do this, we generate data from a distribution with a known variance and calculate \\(\\widehat{\\sigma}^2\\). Then we generate a new dataset from the same distribution and again calculate the corresponding value of \\(\\widehat{\\sigma}^2\\). Repeating this a large number of times, we end up with many estimates \\(\\widehat{\\sigma}^2\\), each based on a dataset of the same size drawn independently from the same population. This collection of estimates gives us an approximation to the sampling distribution of \\(\\widehat{\\sigma}^2\\). Using this approximation, we can get a good estimate of \\(\\text{Bias}(\\widehat{\\sigma}^2)\\) by comparing the sample mean of our simulated estimates \\(\\widehat{\\sigma}^2\\) to the true variance \\(\\sigma^2\\). 1.2.2 draw_sim_data() The first thing we need is a function to generate simulated data. Let's draw the \\(X_1, ..., X_n\\) from a normal distribution with mean zero and variance s_sq. To do this, we write a simple R function as follows: draw_sim_data &lt;- function(n, s_sq) { rnorm(n, sd = sqrt(s_sq)) } The nice thing about writing such a function is that we can test that it's working correctly. For example, suppose you were worried that draw_sim_data does not in fact generate n draws from a normal distribution with mean zero and variance s_sq. Then you could simply draw a large sample and check! Here I'll verify that draw_sim_data() returns a vector of the expected length, with the desired mean and variance, drawn from normal distribution.16 Everything works as expected: set.seed(420508570) test_sims &lt;- draw_sim_data(5000, 9) length(test_sims) ## [1] 5000 mean(test_sims) ## [1] -0.04620928 var(test_sims) ## [1] 8.832994 qqnorm(test_sims) qqline(test_sims) 1.2.3 get_estimate() The next step is to write a function that calculates \\(\\widehat{\\sigma}^2\\). We can do this as follows: get_estimate &lt;- function(x) { sum((x - mean(x))^2) / length(x) # divides by n not (n-1) } Again it's a good idea to test your code before proceeding. There are several tests we could consider running. First, if all the elements of x are the same then get_estimate() should return zero because (x - mean(x)) will simply be a vector of zeros. Everything looks good: get_estimate(rep(5, 25)) ## [1] 0 get_estimate(rep(0, 10)) ## [1] 0 Second, get_estimate() should not in general give the same result as var(), R's built-in function for the sample variance. This is because the latter divides by \\(n\\) rather than \\((n-1)\\). But if \\(n\\) is very large, this difference should become negligible. Again, everything works as expected: set.seed(420508570) sim_small &lt;- draw_sim_data(5, 1) c(sigma_hat_sq = get_estimate(sim_small), Sample_Var = var(sim_small)) ## sigma_hat_sq Sample_Var ## 0.1710749 0.2138436 sim_big &lt;- draw_sim_data(5000, 1) c(sigma_hat_sq = get_estimate(sim_big), Sample_Var = var(sim_big)) ## sigma_hat_sq Sample_Var ## 0.9814408 0.9816371 1.2.4 get_bias() Now we're ready to actually carry out our simulation study. The final step is to write a function called get_bias() that repeatedly calls draw_sim_data() and get_estimate(), stores the resulting estimates \\(\\widehat{\\sigma}^2\\) and calculates a simulation estimate of the bias. Compared to the functions from above, this one will be more complicated, so I'll explain it in steps. First the code: get_bias &lt;- function(n, s_sq, n_reps = 5000) { draw_sim_replication &lt;- function() { sim_data &lt;- draw_sim_data(n, s_sq) get_estimate(sim_data) } sim_replications &lt;- replicate(n_reps, draw_sim_replication()) mean(sim_replications - s_sq) } The function get_bias() takes three arguments: n is the sample size for each replication of the simulation experiment, s_sq is the true population variance from which we will simulate normal data, and n_reps is the number of simulation replications, i.e. the number of times that we want to repeat the simulation. The final argument, n_reps is optional: if you call get_bias() and supply only the first two arguments, R will set n_reps equal to the default value of 5000. The first step inside of get_bias() constructs a function called draw_sim_replication() that doesn't take any input arguments. This may seem strange: I'll explain it in a moment. For now, focus on the steps that draw_sim_replication() carries out. It first runs draw_sim_data(n, s_q) and stores the result in a vector called sim_data. Next it feeds sim_data as an input to get_estimate(). In other words, it carries out one replication of our simulation experiment. But how does the call to draw_sim_data() \"know\" which values to use for n and s_sq given that draw_sim_replication() doesn't take any input arguments? The key is that draw_sim_replication() is created inside of another function: get_bias(). When draw_sim_replication() encounters a reference to n and s_sq, it substitutes the values that were supplied as arguments to get_bias(). Here's another way of looking at draw_sim_replication(). We want to be able to run our simulation study for different values of n and s_sq. After we tell get_bias() our desired values of n and s_sq, it constructs a function for us called draw_sim_replication() that hard codes these particular parameter values. From this point on, calling draw_sim_replication() does \"the right thing\" without our having to explicitly specify n and s_sq. The next step of get_bias() uses the function replicate() to repeatedly call the function draw_sim_replication() a total of n_reps times. The results are stored in a vector called sim_replications. In essence, replicate() is shorthand for a common way of using a for loop. In the following example, x and y will be identical. But constructing x requires much more work: we first need to set up an empty vector, and then explicitly loop over it. In contrast, replicate() does all of this behind the scenes to construct y: do_something &lt;- function() { return(42) } x &lt;- rep(NA, 50) for(i in 1:50) { x[i] &lt;- do_something() } y &lt;- replicate(50, do_something()) identical(x, y) ## [1] TRUE Finally, get_bias() uses the simulation replications stored in the vector sim_replications to approximate the bias of \\(\\widehat{\\sigma}^2\\) by comparing them to the true value of \\(\\sigma^2\\), namely s_sq. It does this by computing the simulation analogue of \\(\\mathbb{E}[\\widehat{\\sigma}^2 - \\sigma^2]\\), which is simply mean(sim_replications - s_sq). 1.2.5 Running the Simulation Study Now we're ready to run our simulation study: we simply need to call get_bias() with our desired values of n and s_sq, for example: set.seed(420508570) get_bias(n = 5, s_sq = 1) ## [1] -0.2007312 It works! Up to simulation error, this result agrees with the theoretical bias of \\(-\\sigma^2/n = -1/5\\). To see that this isn't simply a fluke, we could try different values of n and s_sq. Again, the results agree with the theoretical values: set.seed(420508570) c(theoretical = -1/3, simulation = get_bias(3, 1)) ## theoretical simulation ## -0.3333333 -0.3412176 1.2.6 expand.grid() and Map() Now we have a function get_bias() that can approximate the bias of \\(\\widehat{\\sigma}^2\\) for any values of n and s_sq that we care to specify. But what if we want to carry out a simulation study over a range of values for n and s_sq? One way to do this is with a pair of nested for loops: one that iterates over different values of n and another that iterates over different values of s_sq. But this isn't a great strategy for two reasons. First, loops within loops tend to be slow in R. Second, the book-keeping required to implement this strategy is a bit involved. Fortunately there's a much better way: use expand.grid() and Map(). First we'll set up a grid of values for n and s_sq: n_grid &lt;- 3:5 n_grid ## [1] 3 4 5 s_sq_grid &lt;- seq(from = 1, to = 3, by = 0.5) s_sq_grid ## [1] 1.0 1.5 2.0 2.5 3.0 Now suppose that we want to run get_bias() for every combination of values in n_grid and s_sq_grid. Using the built-in R function expand.grid() we can easily construct a data frame whose rows contain all of these combinations: parameters &lt;- expand.grid(n = n_grid, s_sq = s_sq_grid) parameters ## n s_sq ## 1 3 1.0 ## 2 4 1.0 ## 3 5 1.0 ## 4 3 1.5 ## 5 4 1.5 ## 6 5 1.5 ## 7 3 2.0 ## 8 4 2.0 ## 9 5 2.0 ## 10 3 2.5 ## 11 4 2.5 ## 12 5 2.5 ## 13 3 3.0 ## 14 4 3.0 ## 15 5 3.0 The next step is to evaluated get_bias() repeatedly, once for every combination of parameter values stored in the rows of parameters. The Map() function makes this easy: set.seed(420508570) bias &lt;- Map(get_bias, n = parameters$n, s_sq = parameters$s_sq) Much like replicate(), Map() is shorthand for a common kind of for loop. In this case we loop over the rows of parameters. The first argument to Map() is the name of the function that we want to call repeatedly, in our case get_bias(). The remaining arguments are vectors of values. These are the arguments that Map() passes to get_bias(). The result of running the above code is a list of value, one for each row of parameters. head(bias) ## [[1]] ## [1] -0.3412176 ## ## [[2]] ## [1] -0.2546259 ## ## [[3]] ## [1] -0.2005197 ## ## [[4]] ## [1] -0.5007384 ## ## [[5]] ## [1] -0.3816447 ## ## [[6]] ## [1] -0.292554 length(bias) ## [1] 15 For example, the first element of bias corresponds to get_bias(3, 1). By setting the same seed and running this command \"manually\" we can verify that everything works as expected: set.seed(420508570) identical(get_bias(3, 1), bias[[1]]) ## [1] TRUE This pattern using expand.grid() and Map() is extremely flexible. In our example, get_bias() returns a scalar so bias is just a list of numbers. But more generally Map() can return a list that contains any kind of object at all. Here's a slightly more interesting example. The function get_bias_and_var() is a very slight modification of get_bias() from above that returns a list of two named elements: bias and variance get_bias_and_var &lt;- function(n, s_sq, n_reps = 5000) { draw_sim_replication &lt;- function() { sim_data &lt;- draw_sim_data(n, s_sq) get_estimate(sim_data) } sim_replications &lt;- replicate(n_reps, draw_sim_replication()) list(bias = mean(sim_replications - s_sq), variance = var(sim_replications)) } We can use this function to calculate both the bias and variance of the MLE \\(\\widehat{\\sigma}^2\\) as follows set.seed(420508570) bias_and_variance &lt;- Map(get_bias_and_var, n = parameters$n, s_sq = parameters$s_sq) bias_and_variance[1:3] ## [[1]] ## [[1]]$bias ## [1] -0.3412176 ## ## [[1]]$variance ## [1] 0.4554343 ## ## ## [[2]] ## [[2]]$bias ## [1] -0.2546259 ## ## [[2]]$variance ## [1] 0.3643993 ## ## ## [[3]] ## [[3]]$bias ## [1] -0.2005197 ## ## [[3]]$variance ## [1] 0.3243315 1.2.7 Formatting the Results We've carried out our simulation experiment, but the results are a bit messy. The parameter values are stored in a data frame called parameters and the biases and variances are stored in a list called bias_and_var. Let's format things a bit more nicely. If you have a list my_list whose elements are \"rows\" and you want to bind them together into a data frame, you can use the somewhat inscrutable command do.call(rbind, my_list). For example: do.call(rbind, bias_and_variance) ## bias variance ## [1,] -0.3412176 0.4554343 ## [2,] -0.2546259 0.3643993 ## [3,] -0.2005197 0.3243315 ## [4,] -0.5007384 0.9496033 ## [5,] -0.3816447 0.8616167 ## [6,] -0.292554 0.7173205 ## [7,] -0.6609821 1.781236 ## [8,] -0.5061243 1.538961 ## [9,] -0.3953514 1.326648 ## [10,] -0.8513285 2.705087 ## [11,] -0.6122964 2.467789 ## [12,] -0.4727481 2.014822 ## [13,] -0.9570253 4.124553 ## [14,] -0.7513177 3.201657 ## [15,] -0.567449 2.916767 Now we'll overwrite bias_and_var with the above and bind its columns, cbind(), with those of parameters bias_and_variance &lt;- do.call(rbind, bias_and_variance) sim_results &lt;- cbind(parameters, bias_and_variance) sim_results ## n s_sq bias variance ## 1 3 1.0 -0.3412176 0.4554343 ## 2 4 1.0 -0.2546259 0.3643993 ## 3 5 1.0 -0.2005197 0.3243315 ## 4 3 1.5 -0.5007384 0.9496033 ## 5 4 1.5 -0.3816447 0.8616167 ## 6 5 1.5 -0.292554 0.7173205 ## 7 3 2.0 -0.6609821 1.781236 ## 8 4 2.0 -0.5061243 1.538961 ## 9 5 2.0 -0.3953514 1.326648 ## 10 3 2.5 -0.8513285 2.705087 ## 11 4 2.5 -0.6122964 2.467789 ## 12 5 2.5 -0.4727481 2.014822 ## 13 3 3.0 -0.9570253 4.124553 ## 14 4 3.0 -0.7513177 3.201657 ## 15 5 3.0 -0.567449 2.916767 For extra credit, we can use the function kable() from the knitr package to make the results look even prettier knitr::kable(sim_results) n s_sq bias variance 3 1.0 -0.3412176 0.4554343 4 1.0 -0.2546259 0.3643993 5 1.0 -0.2005197 0.3243315 3 1.5 -0.5007384 0.9496033 4 1.5 -0.3816447 0.8616167 5 1.5 -0.292554 0.7173205 3 2.0 -0.6609821 1.781236 4 2.0 -0.5061243 1.538961 5 2.0 -0.3953514 1.326648 3 2.5 -0.8513285 2.705087 4 2.5 -0.6122964 2.467789 5 2.5 -0.4727481 2.014822 3 3.0 -0.9570253 4.124553 4 3.0 -0.7513177 3.201657 5 3.0 -0.567449 2.916767 1.2.8 Bonus: Parallel Computing with mcMap() There's a good chance that your computer has more than one processor core. Why not use them to speed up your simulation study! If you're on Mac or Linux (RStudio Cloud counts as Linux), it's trivial to parallelize our simulation code from above using mcMap() from the parallel package. To make things more interesting, let's set up a larger parameter grid parameters &lt;- expand.grid(n = 3:10, s_sq = 1:10) The function mcMap() works exactly like Map() except that we have the option of setting an additional parameter mc.cores. This tells R how many of our machine's processor cores to devote to the computation. If you set mc.cores = 1, mcMap() set.seed(420508570) system.time(foo &lt;- parallel::mcMap(get_bias_and_var, n = parameters$n, s_sq = parameters$s_sq, mc.cores = 2)) ## user system elapsed ## 0.013 0.004 2.882 set.seed(420508570) system.time(bar &lt;- Map(get_bias_and_var, n = parameters$n, s_sq = parameters$s_sq)) ## user system elapsed ## 4.656 0.032 4.726 all.equal(foo, bar) ## [1] &quot;Component 1: Component 1: Mean relative difference: 0.07566185&quot; ## [2] &quot;Component 1: Component 2: Mean relative difference: 0.06338969&quot; ## [3] &quot;Component 2: Component 1: Mean relative difference: 0.01797713&quot; ## [4] &quot;Component 2: Component 2: Mean relative difference: 0.0419811&quot; ## [5] &quot;Component 3: Component 1: Mean relative difference: 0.07132466&quot; ## [6] &quot;Component 3: Component 2: Mean relative difference: 0.003915566&quot; ## [7] &quot;Component 4: Component 1: Mean relative difference: 0.00144335&quot; ## [8] &quot;Component 4: Component 2: Mean relative difference: 0.008037541&quot; ## [9] &quot;Component 5: Component 1: Mean relative difference: 0.06769406&quot; ## [10] &quot;Component 5: Component 2: Mean relative difference: 0.0367311&quot; ## [11] &quot;Component 6: Component 1: Mean relative difference: 0.09653207&quot; ## [12] &quot;Component 6: Component 2: Mean relative difference: 0.08002303&quot; ## [13] &quot;Component 7: Component 1: Mean relative difference: 0.05950735&quot; ## [14] &quot;Component 7: Component 2: Mean relative difference: 0.003509842&quot; ## [15] &quot;Component 8: Component 1: Mean relative difference: 0.1580416&quot; ## [16] &quot;Component 8: Component 2: Mean relative difference: 0.02413899&quot; ## [17] &quot;Component 9: Component 1: Mean relative difference: 0.06641339&quot; ## [18] &quot;Component 9: Component 2: Mean relative difference: 0.1169544&quot; ## [19] &quot;Component 10: Component 1: Mean relative difference: 0.03917788&quot; ## [20] &quot;Component 10: Component 2: Mean relative difference: 0.01816194&quot; ## [21] &quot;Component 11: Component 1: Mean relative difference: 0.04206732&quot; ## [22] &quot;Component 11: Component 2: Mean relative difference: 0.04972012&quot; ## [23] &quot;Component 12: Component 1: Mean relative difference: 0.01727456&quot; ## [24] &quot;Component 12: Component 2: Mean relative difference: 0.02388723&quot; ## [25] &quot;Component 13: Component 1: Mean relative difference: 0.09376061&quot; ## [26] &quot;Component 13: Component 2: Mean relative difference: 0.02228442&quot; ## [27] &quot;Component 14: Component 1: Mean relative difference: 0.04126805&quot; ## [28] &quot;Component 14: Component 2: Mean relative difference: 0.01909615&quot; ## [29] &quot;Component 15: Component 1: Mean relative difference: 0.01166129&quot; ## [30] &quot;Component 15: Component 2: Mean relative difference: 0.03093983&quot; ## [31] &quot;Component 16: Component 1: Mean relative difference: 0.03928752&quot; ## [32] &quot;Component 16: Component 2: Mean relative difference: 0.03300219&quot; ## [33] &quot;Component 17: Component 1: Mean relative difference: 0.06212456&quot; ## [34] &quot;Component 17: Component 2: Mean relative difference: 0.04231614&quot; ## [35] &quot;Component 18: Component 1: Mean relative difference: 0.04646418&quot; ## [36] &quot;Component 18: Component 2: Mean relative difference: 0.02229984&quot; ## [37] &quot;Component 19: Component 1: Mean relative difference: 0.06991444&quot; ## [38] &quot;Component 19: Component 2: Mean relative difference: 0.04636792&quot; ## [39] &quot;Component 20: Component 1: Mean relative difference: 0.02524556&quot; ## [40] &quot;Component 20: Component 2: Mean relative difference: 0.02777548&quot; ## [41] &quot;Component 21: Component 1: Mean relative difference: 0.01941893&quot; ## [42] &quot;Component 21: Component 2: Mean relative difference: 0.002190786&quot; ## [43] &quot;Component 22: Component 1: Mean relative difference: 0.02097511&quot; ## [44] &quot;Component 22: Component 2: Mean relative difference: 0.01981487&quot; ## [45] &quot;Component 23: Component 1: Mean relative difference: 0.1618288&quot; ## [46] &quot;Component 23: Component 2: Mean relative difference: 0.06102808&quot; ## [47] &quot;Component 24: Component 1: Mean relative difference: 0.06959227&quot; ## [48] &quot;Component 24: Component 2: Mean relative difference: 0.02654214&quot; ## [49] &quot;Component 25: Component 1: Mean relative difference: 0.01850481&quot; ## [50] &quot;Component 25: Component 2: Mean relative difference: 0.05908572&quot; ## [51] &quot;Component 26: Component 1: Mean relative difference: 0.008482547&quot; ## [52] &quot;Component 26: Component 2: Mean relative difference: 0.02792746&quot; ## [53] &quot;Component 27: Component 1: Mean relative difference: 0.03200767&quot; ## [54] &quot;Component 27: Component 2: Mean relative difference: 0.02209047&quot; ## [55] &quot;Component 28: Component 1: Mean relative difference: 0.03317915&quot; ## [56] &quot;Component 28: Component 2: Mean relative difference: 0.04462615&quot; ## [57] &quot;Component 29: Component 1: Mean relative difference: 0.01378655&quot; ## [58] &quot;Component 29: Component 2: Mean relative difference: 0.008613555&quot; ## [59] &quot;Component 30: Component 1: Mean relative difference: 0.08796235&quot; ## [60] &quot;Component 30: Component 2: Mean relative difference: 0.0298427&quot; ## [61] &quot;Component 31: Component 1: Mean relative difference: 0.01348962&quot; ## [62] &quot;Component 31: Component 2: Mean relative difference: 0.003335422&quot; ## [63] &quot;Component 32: Component 1: Mean relative difference: 0.107385&quot; ## [64] &quot;Component 32: Component 2: Mean relative difference: 0.01042212&quot; ## [65] &quot;Component 33: Component 1: Mean relative difference: 0.007827397&quot; ## [66] &quot;Component 33: Component 2: Mean relative difference: 0.01986365&quot; ## [67] &quot;Component 34: Component 1: Mean relative difference: 0.04798114&quot; ## [68] &quot;Component 34: Component 2: Mean relative difference: 0.05915583&quot; ## [69] &quot;Component 35: Component 1: Mean relative difference: 0.02896033&quot; ## [70] &quot;Component 35: Component 2: Mean relative difference: 0.001778245&quot; ## [71] &quot;Component 36: Component 1: Mean relative difference: 0.03499143&quot; ## [72] &quot;Component 36: Component 2: Mean relative difference: 0.06158166&quot; ## [73] &quot;Component 37: Component 1: Mean relative difference: 0.03383005&quot; ## [74] &quot;Component 37: Component 2: Mean relative difference: 0.02322873&quot; ## [75] &quot;Component 38: Component 1: Mean relative difference: 0.05972238&quot; ## [76] &quot;Component 38: Component 2: Mean relative difference: 0.04999912&quot; ## [77] &quot;Component 39: Component 1: Mean relative difference: 0.007108329&quot; ## [78] &quot;Component 39: Component 2: Mean relative difference: 0.02391228&quot; ## [79] &quot;Component 40: Component 1: Mean relative difference: 0.1486787&quot; ## [80] &quot;Component 40: Component 2: Mean relative difference: 0.05599759&quot; ## [81] &quot;Component 41: Component 1: Mean relative difference: 0.002214479&quot; ## [82] &quot;Component 41: Component 2: Mean relative difference: 0.02958618&quot; ## [83] &quot;Component 42: Component 1: Mean relative difference: 0.02027159&quot; ## [84] &quot;Component 42: Component 2: Mean relative difference: 0.04685947&quot; ## [85] &quot;Component 43: Component 1: Mean relative difference: 0.04933171&quot; ## [86] &quot;Component 43: Component 2: Mean relative difference: 0.009416366&quot; ## [87] &quot;Component 44: Component 1: Mean relative difference: 0.0200179&quot; ## [88] &quot;Component 44: Component 2: Mean relative difference: 0.02952605&quot; ## [89] &quot;Component 45: Component 1: Mean relative difference: 0.02475393&quot; ## [90] &quot;Component 45: Component 2: Mean relative difference: 0.05125672&quot; ## [91] &quot;Component 46: Component 1: Mean relative difference: 0.004892078&quot; ## [92] &quot;Component 46: Component 2: Mean relative difference: 0.008533936&quot; ## [93] &quot;Component 47: Component 1: Mean relative difference: 0.08964024&quot; ## [94] &quot;Component 47: Component 2: Mean relative difference: 0.05139751&quot; ## [95] &quot;Component 48: Component 1: Mean relative difference: 0.002431341&quot; ## [96] &quot;Component 48: Component 2: Mean relative difference: 0.04248982&quot; ## [97] &quot;Component 49: Component 1: Mean relative difference: 0.02423048&quot; ## [98] &quot;Component 49: Component 2: Mean relative difference: 0.0591975&quot; ## [99] &quot;Component 50: Component 1: Mean relative difference: 0.04458665&quot; ## [100] &quot;Component 50: Component 2: Mean relative difference: 0.03100931&quot; ## [101] &quot;Component 51: Component 1: Mean relative difference: 0.02466228&quot; ## [102] &quot;Component 51: Component 2: Mean relative difference: 0.01983895&quot; ## [103] &quot;Component 52: Component 1: Mean relative difference: 0.07285492&quot; ## [104] &quot;Component 52: Component 2: Mean relative difference: 0.0575895&quot; ## [105] &quot;Component 53: Component 1: Mean relative difference: 0.05539298&quot; ## [106] &quot;Component 53: Component 2: Mean relative difference: 0.02773766&quot; ## [107] &quot;Component 54: Component 1: Mean relative difference: 0.01224895&quot; ## [108] &quot;Component 54: Component 2: Mean relative difference: 0.03624102&quot; ## [109] &quot;Component 55: Component 1: Mean relative difference: 0.0550619&quot; ## [110] &quot;Component 55: Component 2: Mean relative difference: 0.05803373&quot; ## [111] &quot;Component 56: Component 1: Mean relative difference: 0.04886914&quot; ## [112] &quot;Component 56: Component 2: Mean relative difference: 0.009701203&quot; ## [113] &quot;Component 57: Component 1: Mean relative difference: 0.02370602&quot; ## [114] &quot;Component 57: Component 2: Mean relative difference: 0.101665&quot; ## [115] &quot;Component 58: Component 1: Mean relative difference: 0.06948062&quot; ## [116] &quot;Component 58: Component 2: Mean relative difference: 0.06355725&quot; ## [117] &quot;Component 59: Component 1: Mean relative difference: 0.01800966&quot; ## [118] &quot;Component 59: Component 2: Mean relative difference: 0.02268095&quot; ## [119] &quot;Component 60: Component 1: Mean relative difference: 0.02031243&quot; ## [120] &quot;Component 60: Component 2: Mean relative difference: 0.005770252&quot; ## [121] &quot;Component 61: Component 1: Mean relative difference: 0.04198355&quot; ## [122] &quot;Component 61: Component 2: Mean relative difference: 0.04071398&quot; ## [123] &quot;Component 62: Component 1: Mean relative difference: 0.06126732&quot; ## [124] &quot;Component 62: Component 2: Mean relative difference: 0.04494239&quot; ## [125] &quot;Component 63: Component 1: Mean relative difference: 0.05188636&quot; ## [126] &quot;Component 63: Component 2: Mean relative difference: 0.01330249&quot; ## [127] &quot;Component 64: Component 1: Mean relative difference: 0.06294422&quot; ## [128] &quot;Component 64: Component 2: Mean relative difference: 0.01789742&quot; ## [129] &quot;Component 65: Component 1: Mean relative difference: 0.02377168&quot; ## [130] &quot;Component 65: Component 2: Mean relative difference: 0.02207551&quot; ## [131] &quot;Component 66: Component 1: Mean relative difference: 0.03403929&quot; ## [132] &quot;Component 66: Component 2: Mean relative difference: 0.02618867&quot; ## [133] &quot;Component 67: Component 1: Mean relative difference: 0.01967982&quot; ## [134] &quot;Component 67: Component 2: Mean relative difference: 0.05883405&quot; ## [135] &quot;Component 68: Component 1: Mean relative difference: 0.07892974&quot; ## [136] &quot;Component 68: Component 2: Mean relative difference: 0.07588599&quot; ## [137] &quot;Component 69: Component 1: Mean relative difference: 0.1642618&quot; ## [138] &quot;Component 69: Component 2: Mean relative difference: 0.01681416&quot; ## [139] &quot;Component 70: Component 1: Mean relative difference: 0.003326852&quot; ## [140] &quot;Component 70: Component 2: Mean relative difference: 0.006770292&quot; ## [141] &quot;Component 71: Component 1: Mean relative difference: 0.06696817&quot; ## [142] &quot;Component 71: Component 2: Mean relative difference: 0.01428356&quot; ## [143] &quot;Component 72: Component 1: Mean relative difference: 0.01970603&quot; ## [144] &quot;Component 72: Component 2: Mean relative difference: 0.01955519&quot; ## [145] &quot;Component 73: Component 1: Mean relative difference: 0.0156728&quot; ## [146] &quot;Component 73: Component 2: Mean relative difference: 0.04891366&quot; ## [147] &quot;Component 74: Component 1: Mean relative difference: 0.05412007&quot; ## [148] &quot;Component 74: Component 2: Mean relative difference: 0.07474288&quot; ## [149] &quot;Component 75: Component 1: Mean relative difference: 0.06870354&quot; ## [150] &quot;Component 75: Component 2: Mean relative difference: 0.01743084&quot; ## [151] &quot;Component 76: Component 1: Mean relative difference: 0.06462654&quot; ## [152] &quot;Component 76: Component 2: Mean relative difference: 0.03950513&quot; ## [153] &quot;Component 77: Component 1: Mean relative difference: 0.0005507413&quot; ## [154] &quot;Component 77: Component 2: Mean relative difference: 0.02108105&quot; ## [155] &quot;Component 78: Component 1: Mean relative difference: 0.04636333&quot; ## [156] &quot;Component 78: Component 2: Mean relative difference: 0.04878875&quot; ## [157] &quot;Component 79: Component 1: Mean relative difference: 0.02936333&quot; ## [158] &quot;Component 79: Component 2: Mean relative difference: 0.01862487&quot; ## [159] &quot;Component 80: Component 1: Mean relative difference: 0.005782879&quot; ## [160] &quot;Component 80: Component 2: Mean relative difference: 0.008412481&quot; 1.3 Your Turn: Simulating the Hot Hand 1.4 Further Reading Advanced R book for more on replicate() and lapply() as well as scoping rules for functions within functions etc. 1.5 Mini Projects 1.5.1 The Hot Hand Revisited In the lesson we generated iid data. Now try simulating from a first-order Markov chain for various different parameters. How do the results change? 1.5.2 Which estimator of the variance is really better? Compare the RMSE of two variance estimators: the unbiased estimator and the MLE. 1.5.3 Optimal Stopping Simulate the secretary problem; try to find the optimal stopping rule. 1.6 DRAFT MATERIAL Should I introduce knitr and Rmarkdown in this lesson or the next one? Some basic simulation commands in R: sample(), rbinom(), rnorm(), etc. Read the help files. Write a function draw_sim_data() that makes 100 Bernoulli(1/2) draws. Optional arguments p and n? This simulates data when there is no hot hand set.seed() what does it do? Think about how to calculate the estimator: fraction of times that three ones are followed by another one compared to another zero. Suppose you had a function is_after_3_ones() that took a vector of 0 and 1 and returned, for each element, whether it is after three ones. How would you use it? Write this function. Put everything together with replicate() to do a simple sim for \\(p=1/2\\) and \\(n = 100\\). How about trying different values of \\(n\\) and \\(p\\)? Need to keep results organized: apply() family of functions (or maybe the tidy equivalents?) Try doing it in parallel with mclapply(). First explain the basic idea of parallel and why this is \"embarrassingly parallel.\" Show them how to time the code, illustrate with sys.sleep(). For the students who finish very quickly, have some extensions: a markov chain DGP, and is_after_k_ones() dgp &lt;- function(n = 100, p = 0.5) { rbinom(n, 1, p) } # Maybe have a challenge to write the version for after k ones, but start by # asking them to do the one for after 3 ones is_after_k_ones &lt;- function(x, k) { out &lt;- rep(NA) for(i in (k+1):length(x)) { out[i] &lt;- sum(x[(i - k):(i - 1)]) == k } return(out) } get_est &lt;- function(x) { #ones &lt;- which(x == 1) #mean(x[ones + 1], na.rm = TRUE) mean(x[is_after_k_ones(x, 3)], na.rm = TRUE) } n_reps &lt;- 1000 set.seed(1234) sim_results &lt;- replicate(n_reps, get_est(dgp())) library(ggplot2) qplot(sim_results) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. mean(sim_results, na.rm = TRUE) ## [1] 0.4620068 #ones &lt;- which(x == 1) #x[which(x == 1) + 1] #runs &lt;- rle(x) #foo &lt;- rle(x) #str(foo) #x #foo$lengths #foo$values See for example this write-up of Lantis &amp; Nessen (2021) in the February 2022 NBER digest.↩︎ See Miller &amp; Sanjurjo (2019) for a more accessible explanation that connects to several related probability puzzles.↩︎ Technically, \"random\" draws made on a computer are only pseudorandom. We'll discuss this further below.↩︎ For less common distributions, see CRAN Task View: Probability Distributions↩︎ A Bernoulli trial is a model for a possibly biased coin flip: if \\(X \\sim \\text{Bernoulli}(p)\\) then \\(\\mathbb{P}(X=1) = p\\) and \\(\\mathbb{P}(X=0) = 1-p\\).↩︎ To see this, first rewrite \\(\\sum_{i=1}^n (X_i - \\bar{X}_n)^2\\) as \\(\\sum_{i=1}^n (X_i - \\mu)^2 - n(\\bar{X}_n - \\mu)^2\\). This step is just algebra. Then take expectations, using the fact that the \\(X_i\\) are independent and identically distributed.↩︎ If you're unfamiliar with the Normal Q-Q plot that I used to check the normality of test_sims, you can read about it this blog post.↩︎ "],["using-webexercises.html", "Lesson 2 Using webexercises 2.1 Tally the number of correct answers: total_correct() 2.2 Create a TRUE/FALSE question: torf() 2.3 Create fill-in-the-blank Questions: fitb() 2.4 Create a multiple choice question: mcq(), mcqlong() 2.5 Hide Solutions / Create Hints", " Lesson 2 Using webexercises Let's take a look at some of the tools that webexercises provides us. It can be used to tally up the number of correct answers: total_correct() create TRUE/FALSE questions: torf() create fill-in-the-blank questions: fitb() create multiple choice questions: mcq() or longmcq() hide solutions / create hints: hide(), unhide(), or webex.hide = TRUE We'll now examine each of these possibilities in detail. 2.1 Tally the number of correct answers: total_correct() Suppose that you want to keep track of how many questions a user has answered correctly so far. You can do this with the total_correct() function. The elem and args arguments control formatting: elem is used to set header styles, say you wanted h3 instead of h2, and args feeds in raw CSS if you want to make manual tweaks: It doesn't matter where you put total_correct() in your document: it will still work as expected. In this document, for example, it appears near the top. Nevertheless, as you work down to the bottom and answer more questions, the tally will update dynamically. 2.2 Create a TRUE/FALSE question: torf() Supply the correct answer as the first argument to torf(). This should either be TRUE or FALSE, e.g. True or False: \\(\\pi\\) is a rational number. TRUEFALSE 2.3 Create fill-in-the-blank Questions: fitb() As with torf(), supply the correct answer as the first argument of fitb(). The difference is that fitb() allows you to supply something other than TRUE or FALSE as the correct answer, and displays a text field rather than a drop-down menu: \\(8 \\times 9 =\\) This correct answer doesn't have to be hard-coded: you can do calculations in place or supply the name of an R object created elsewhere in your RMarkdown document. This allows you to create a dynamic question and answer. Here's an example in which the question is randomly generated when you knit this document: The square root of 16 is: If the correct answer is E, but a user enters e, this will be marked as incorrect: fitb() is case sensitive. To turn this off, set ignore_case = TRUE. What is the letter after D? By default fitb() ignores whitespace. For example, rnorm ( 3 ) is counted as a correct answer to the following: Write R code to make three independent standard normal draws. If you want fitb() to treat whitespace as meaningful, set ignore_ws = TRUE. Some fill-in-the-blank questions have more than one correct answer. To create such a question, simply supply a vector as the first argument to fitb(): Type a vowel: Want to do something more complicated? No problem! You can supply regular expressions as a solution to fitb() as follows: Type any 3 letters: Here's an example with multiple correct answers to a coding question: How do you load the tidyverse package? I'm not clear on what the width argument does. Look it up! 2.4 Create a multiple choice question: mcq(), mcqlong() Use mcq() to create a \"short\" multiple choice question. The first argument is a vector of answers in which the correct answer is given the name answer and all other elements are un-named, e.g. c('Harris', 'Trump', answer = 'Biden', 'Clinton') in the following: Who was elected president of the United States in November 2020? HarrisTrumpBidenClinton To create a multiple choice question with radio buttons rather than a dropdown menu, use mcqlong(). This tends to result in nicer formatting than a drop-down if the answers to your question are \"long,\" hence the name of the function. When your answers are long, inline R code can be hard to read. To avoid this problem, you can set up your multiple choice answers in a vector within an R chunk with echo=FALSE and then supply that vector as the first argument to longmcq() in an inline R chunk. For example: Which of these statements about p-values is correct? A p-value is the probability that the null hypothesis is true A p-value is the probability of observing a test statistic at least as extreme as the one we actually observed, assuming that the null hypothesis is true. The p-value is the probability of making a mistake when testing a hypothesis. What is true about a 95% confidence interval for the population mean \\(\\mu\\)? Approximately 95% of the values in our dataset will fall within the confidence interval. The probability that \\(\\mu\\) lies in our confidence interval equals 0.95. If we repeatedly draw a sample of the same size from the same population, and construct an interval in this way for each sample, then approximately 95% of the intervals will contain \\(\\mu\\). 2.5 Hide Solutions / Create Hints This is the main piece of functionality that I was hoping to use for my Core ERM course: hiding hints and solutions to exercises and providing buttons that students can click to reveal them. There are two ways to do this: inline, and using R chunks. For inline hints and solutions, put hide() before and unhide() after the hint or solution. Each of these should be inserted as inline R code. Provide a text string as an argument to hide() to control the text displayed on the button. Here's an example: 2.5.1 Example 1: Inline Hint and Solution Make a histogram of 100 independent standard normal draws. Show Hint Look at the R help functions for rnorm() and hist(). Show Solution hist(rnorm(100)) The hide/unhide patter is most useful for hints, where you don't actually want to run any code, and only want to display a sentence or two. For solutions, whether or not you want to run them, it's you'll probably want to use an R chunk. To do this, set the webex.hide option to TRUE. Alternatively you can set webex.hide = 'Your Text Here' to control the text that appears on the button. For a solution that doesn't evaluate, set eval = FALSE. 2.5.2 Example 2: Chunk solution that doesn't execute Show Solution x &lt;- rnorm(100) hist(x) 2.5.3 Example 3: Chunk solution that evaluates Show Solution x &lt;- rnorm(100) hist(x) Sometimes you may want to start by both explaining the exercise and showing the desired output, while hiding the code used to generate it. To avoid re-typing code, create a named chunk with the chunk option echo = FALSE. This chunk will display the output only, not the code used to generate it. Then create an empty chunk below it with the same name but set eval = FALSE and webex.hide=TRUE or webex.hide='Your Text Here'. For this second chunk don't set echo = FALSE. 2.5.4 Example 4: Show the output, hide the code The mtcars dataset contains the following variables: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Write code to generate the following plot using this dataset: Show Solution plot(mpg ~ disp, data = mtcars, main = &quot;Fuel Efficiency (mpg) versus Engine Displacement (cc)&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
