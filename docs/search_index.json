[["index.html", "Empirical Research Methods Preface About This Book Pre-requisites Why not Stata? Why not Matlab, Julia, or Python?", " Empirical Research Methods Francis J. DiTraglia 2022-04-03 Preface About This Book Supervising undergraduate, masters-level, and doctoral research students has shown me just how many of the skills that I take for granted in my day-to-day work were never taught in a course, but acquired through years of painful trial-and-error. You've probably heard that \"the only way to learn how to do research is by doing research.\" Indeed: classroom exercises are always somewhat artificial, and there is no substitute for getting your hands dirty with a problem that really matters to you. But trial-and-error is a slow and clumsy way to gain proficiency, and throwing students in at the deep end is neither a recipe for academic success nor for mental well-being. The goal of this book is to put some structure around the process through which students learn to do empirical work in economics, building a strong foundation for later self-directed reading and research. The book is divided into twenty-odd short chapters called lessons, each designed to take between one and two hours to complete. Broadly speaking, the material is a mix of applied econometrics, data science, and research skills. In keeping with the Swiss Army Knife logo, the idea is to teach you lots of little things that will come in handy later. While the topics covered below are something of a miscellany, there are strong connections between the lessons. For best results, complete them in order. A key theme that runs throughout the lessons is the importance of reproducible research using open-source tools. Reproducible research is about creating a clean and fully-documented path from raw data to final results, making errors less likely to occur and easier to find when they do. It also allows other researchers, or our future selves, to build on past work, expanding the sum total of knowledge. Of course I can only replicate your research if I can run your code, and this is why open-source software is so important. Fortunately there are many fantastic open-source programming languages to choose from. This book uses R, the lingua franca of statistics and an increasingly popular choice among economists. Pre-requisites This book does not assume advanced knowledge of programming, mathematics, or econometrics, but it does have some pre-requisites. My target audience is first-year graduate students and final-year undergraduates in economics. At Oxford, I use this book to teach a first-year master's level course on Empirical Research Methods that comes after students have completed 16 weeks of basic statistics and econometrics. I assume that you've taken an econometrics course that uses matrix notation and that you have basic familiarity with R programming. If you need to brush up on econometrics, I recommend Marno Verbeek's Guide to Modern Econometrics. I've linked to the third edition because it is particularly inexpensive to buy a used copy, but any edition will do. At a more advanced level, Bruce Hansen's two volume series Econometrics is both excellent and free to download online. If you haven't used R before or feel the need for a bit of review, I suggest reading Hands-On Programming with R. It's free, short, and will get you up to speed quickly. Why not Stata? Given that much of the material discussed below falls under the broad category of \"applied microeconometrics\" you may wonder why I chose R rather than Stata. Indeed, Stata is easy-to-use, and makes it relatively painless to implement \"textbook\" microeconometric methods.1 So why don't I like Stata? Before beginning my polemic I should be absolutely clear that Stata users are not bad people: hate the sin, love the sinner. Here begins the sermon. First, Stata is expensive. The price for a Business single-user Stata license is $765 per year.2 If you want support for multicore computing, the price is even higher: an 8-core version of Stata costs $1,395 annually. There is no discount for Government or nonprofits, but as an Oxford faculty member, I can obtain an 8-core version of Stata for the low price of $595 per year, or around 9% of my annual research allowance. In contrast, the tools that we will learn in this book, mainly R and C++, are completely free. This is particularly important in the modern world of high-performance cluster computing. If you're considering running your code on a multicore machine on Amazon, Google, or Microsoft cloud servers, you don't want to pay a software license fee for every core that you use. Second, Stata is almost comically behind the times. Let's see what's new in Stata version 16, released in February 2020.3 At the top of the list is the LASSO, a wildly popular technique for high-dimensional regression. Rob Tibshirani developed this method in a seminal paper from 1996, so it only took 24 years for it to be incorporated into Stata.4 Fortunately, Tibshirani and his co-authors made it easy for Stata, by releasing open-source software to implement the LASSO and related methods in R over a decade ago.5 Next on the list of new Stata features is linear programming, a technique that came to prominence in the late 1940s.6 Stata 16 also has the ability to call \"any Python package\"--something you can do for free in R using reticulate or in Python itself for that matter--and \"truly reproducible reporting.\" Reproducible reporting is incredibly valuable, and it's something that we'll cover in detail below. It's also been available in R, completely free of charge, since at least 2002.7 I suppose we shouldn't expect too much of a statistical computing package that only added support for matrix programming in 2005, a full 20 years after Stata version 1.0.8 Third, Stata is a black box. Because the underlying source code is kept secret, there's no way for a Stata user to know for certain what's happening under the hood. A few years ago I tried to determine precisely what instrument set Stata was using in its implementation of a well-known dynamic panel estimator. The documentation was vague, so I resorted to reverse-engineering the Stata results by trial-and-error in R. I never did get the results to match perfectly. In contrast, if you're not sure what a particular R function or package is doing, you can simply read the source code and find out. Fourth, and most importantly, Stata makes it hard to share with others. If I don't own a copy of Stata, I can't replicate your work. Even if I do own a copy of Stata, I still may not be able to so do: Stata's proprietary binary data formats are updated fairly regularly and do not maintain backwards compatibility. Datafiles created in Stata version 16, for example, cannot be opened in Stata 13. Indeed, depending on the number of variables included in your dataset, Stata 16 files cannot necessarily be opened even in Stata 15. Fortunately, as we'll see below, intrepid open-source programmers have developed free software to unlock data from Stata's proprietary and ever-changing binary formats. Why not Matlab, Julia, or Python? Unlike Stata, Matlab is a bona fide programming language and a fairly capable one at that. Nevertheless, my other critiques of Stata from above still apply: Matlab is extremely expensive, and it's not open source. In contrast, I have nothing bad to say about Python and Julia: they're great languages and you should consider learning one or both of them!9 In the end I decided to choose one language and R struck me as the best choice for the moment. In five or ten years time, I could easily imagine re-writing this book in Julia, but as of this writing R has the advantage of maturity and a large, and extremely supportive user community. Even if you ultimately decide that R isn't for you, fear not! After learning the material in this book, you'll find it fairly easy to transition to Python or Julia, should you so choose. Now let's get started! Arguably, Stata is too easy to learn precisely because of the incentives faced by a software developer with monopoly power: see Hal Varian's paper: Economic Incentives in Software Design.↩︎ These figures were accurate as of March 2021. For the latest prices, see https://www.stata.com/order/dl/.↩︎ https://www.stata.com/new-in-stata/↩︎ Tibshirani (1996) - Regression Shrinkage and Selection via the Lasso↩︎ Friedman et al (2010) - Regularization Paths for Generalized Linear Models via Coordinate Descent↩︎ For a history of linear programming, see Dantzig (1983). To be completely fair, the linear programming algorithm implemented in Stata 16 was only developed in 1992, a lag of merely 28 years.↩︎ Reproducible reporting in R started with sweave. These days we have a fantastic successor package called knitr, which I cover below.↩︎ The \"Mata\" programming language was added in Stata 9: https://www.stata.com/stata9/. For a timeline of Stata versions, see https://www.stata.com/support/faqs/resources/history-of-stata/.↩︎ A good resources aimed at economists is https://quantecon.org, available in both Python and Julia versions.↩︎ "],["the-hot-hand.html", "Lesson 1 The Hot Hand 1.1 Drawing Random Data in R 1.2 The Skeleton of a Simulation Study 1.3 Your Turn: Simulating the Hot Hand 1.4 Further Reading 1.5 Mini Projects 1.6 DRAFT MATERIAL", " Lesson 1 The Hot Hand If you've read 2002 Nobel Laureate Daniel Kahneman's best-selling book Thinking Fast and Slow, you may remember this passage about the hot hand illusion, a supposed illustration of the human tendency to see patterns in random noise: Amos [Tversky] and his students Tom Gilovich and Robert Vallone caused a stir with their study of misperceptions of randomness in basketball. The \"fact\" that players occasionally acquire a hot hand is generally accepted by players, coaches, and fans. The inference is irresistible: a player sinks three or four baskets in a row and you cannot help forming the causal judgment that this player is now hot, with a temporarily increased propensity to score. Players on both teams adapt to this judgment—teammates are more likely to pass to the hot scorer and the defense is more likely to doubleteam. Analysis of thousands of sequences of shots led to a disappointing conclusion: there is no such thing as a hot hand in professional basketball, either in shooting from the field or scoring from the foul line. Of course, some players are more accurate than others, but the sequence of successes and missed shots satisfies all tests of randomness. The hot hand is entirely in the eye of the beholders, who are consistently too quick to perceive order and causality in randomness. The hot hand is a massive and widespread cognitive illusion. The research that Kahneman mentions was published in a famous paper by Gilovich, Vallone &amp; Tversky (1985), and later summarized for a general audience in Gilovich &amp; Tversky (1989). The abstract of the original paper says it all: Basketball players and fans alike tend to believe that a player's chance of hitting a shot are greater following a hit than following a miss on the previous shot. However, detailed analyses of the shooting records of the Philadelphia 76ers provided no evidence for a positive correlation between the outcomes of successive shots. The same conclusions emerged from free-throw records of the Boston Celtics, and from a controlled shooting experiment with the men and women of Cornell's varsity teams. The outcomes of previous shots influenced Cornell players' predictions but not their performance. The belief in the hot hand and the \"detection\" of streaks in random sequences is attributed to a general misconception of chance according to which even short random sequences are thought to be highly representative of their generating process. Between 1985 and 2011, when Kahneman's book was published, this result was replicated numerous times under a variety of different conditions, making it one of the better-documented biases in human decision-making. But it turns out that the hot-hand illusion is itself an illusion. In a recent issue of Econometrica, Miller &amp; Sanjurjo (2018) point out a subtle but consequential error in the statistical tests used in the hot hand literature. It turns out that these tests were biased against detecting evidence of a hot hand, even if it did in fact exist. Correcting this mistake and re-analyzing Gilovich, Vallone and Tversky's original dataset \"reveals significant evidence of streak shooting, with large effect sizes.\" The hot hand is real, and the literature has now shifted to trying to estimate the size of the effect.10 There are some helpful lessons that we can draw from this episode. First, we all make mistakes--even Nobel Laureates! Second, successful replications tell us less than we might hope: they could easily reproduce the bias of the original study. But in my view, the real lesson of the hot hand affair is much simpler: you should always run a simulation study. The probabilistic error that led so many researchers to draw the wrong conclusion about the hot hand is really quite subtle.11 But at the same time, anyone who knows basic programming could have detected the mistake in five minutes if only they had bothered to look. In economics and statistics, simulation is a superpower. It helps us to understand our models, check for mistakes, and make unexpected connections, some of which may even lead to new theoretical results. If you'll pardon my continuation of the Swiss Army Knife metaphor, simulation is the knife: arguably the most useful tool in your toolbox. In this lesson, we'll cover some basic tools for carrying out a simulation experiment in R and use them to shed some light on the illusion of the hot hand illusion. The key R functions introduced in this lesson are as follows: sample() rbinom() rnorm() set.seed() identical() replicate() expand.grid() Map() mcMap() from the parallel package 1.1 Drawing Random Data in R Before we can use simulations to study the illusion of the hot hand illusion, we need to review the basics of drawing random data in R. We'll examine the functions sample(), rbinom() and set.seed() in detail. I'll also point you to a large number functions for simulating from well-known probability distributions in R. Finally, you'll have a chance to practice what you've learned by solving a few short exercises. 1.1.1 sample() R has many helpful built-in functions for making simulated random draws. The simplest is sample(), which makes size random draws without replacement from a vector x. To test this out, I'll create a very simple vector my_vector &lt;- c(&#39;there&#39;, &#39;is&#39;, &#39;no&#39;, &#39;largest&#39;, &#39;integer&#39;) The following line of code makes two draws without replacement from my_vector sample(x = my_vector, size = 2) # without replacement ## [1] &quot;integer&quot; &quot;there&quot; If I run the same line of code again, I may not get the same result: it's random!12 sample(x = my_vector, size = 2) # without replacement ## [1] &quot;there&quot; &quot;no&quot; To draw with replacement, set replace = TRUE sample(x = my_vector, size = 7, replace = TRUE) # with replacement ## [1] &quot;there&quot; &quot;there&quot; &quot;there&quot; &quot;there&quot; &quot;integer&quot; &quot;no&quot; &quot;there&quot; As usual in R, the argument names x, size, and replace are optional. But it is considered good coding style to explicitly supply an argument name whenever we're overriding a function's default behavior. This makes it easier for anyone reading our code to understand what's happening. Since sample() defaults to making draws without replacement, it's a good idea to write replace = TRUE rather then simply TRUE. But even without writing replace =, the code will still work as long as we supply all of the arguments in the correct order: sample(my_vector, 7, TRUE) # bad style ## [1] &quot;no&quot; &quot;no&quot; &quot;is&quot; &quot;integer&quot; &quot;there&quot; &quot;no&quot; &quot;largest&quot; sample(my_vector, 7, replace = TRUE) # good style ## [1] &quot;there&quot; &quot;largest&quot; &quot;integer&quot; &quot;integer&quot; &quot;is&quot; &quot;no&quot; &quot;integer&quot; 1.1.2 Probability Distributions in R As a programming language targeted at statistical applications, R supplies built-in functions for all of the most common probability distributions.13 These functions follow a consistent naming convention. They being with either d, p, q, or r and are followed by an abbreviated name for a particular probability distribution. The prefix d denotes a density function (or mass function for a discrete distribution); p denotes a cumulative distribution function (CDF), q denotes a quantile function, and r denotes a function for making random draws from a particular distribution. For example: dunif() gives the probability density function of a uniform random variable, pnorm() gives the CDF of a normal random variable, qchisq() gives the quantile function of a Chi-squared, and rbinom allows us to make random draws from a Binomial distribution. The following table gives a full list of the relevant commands. R commands Distribution d/p/q/rbeta Beta d/p/q/rbinom Binomial d/p/q/rcauchy Cauchy d/p/q/rchisq Chi-Squared d/p/q/rexp Exponential d/p/q/rf F d/p/q/rgamma Gamma d/p/q/rgeom Geometric d/q/p/rhyper Hypergeometric d/p/q/rlogis Logistic d/p/q/rlnorm Log Normal d/p/q/rnbinom Negative Binomial d/p/q/rnorm Normal d/p/q/rpois Poisson d/p/q/rt Student's t d/p/q/runif Uniform d/p/q/rweibull Weibull There's a single help file for all of the d/p/q/r functions for a particular distribution. For example, if you enter ?dbeta at the console you'll be shown the help files for dbeta(), pbeta(), qbeta(), and rbeta(). To get a feel for how these functions work, let's take a look at rbinom(), the function for drawing from a Binomial distribution. Recall that a Binomial\\((m,p)\\) random variable equals the number of heads in \\(m\\) independent tosses of a coin with \\(\\mathbb{P}(\\text{Heads})=p\\). Or to use a bit of probability jargon, it equals the number of successes in \\(m\\) independent Bernoulli trials, each with probability of success \\(p\\).14 If \\(X\\) is a Binomial random variable with parameters \\(m\\) and \\(p\\), traditionally written as \\(X \\sim \\text{Binomial}(m, p)\\) then \\(X\\) must take on a value in the set \\(\\{0, 1, 2, ..., m\\}\\) and the probability that it takes on a particular value \\(x\\) in this set is \\[ \\mathbb{P}(X = x) = \\binom{m}{x} p^x (1 - p)^x \\] The function rbinom() makes random draws with the probabilities given by this formula. Its takes three arguments: size is the number of trials, \\(m\\) in the formula, prob is the probability of success, \\(p\\) in the formula, and n is the desired number of Binomial draws. For example, we can make a single draw from a Binomial\\((m = 10, p =1 /2)\\) distribution as follows rbinom(n = 1, size = 10, prob = 0.5) ## [1] 6 and fifteen draws from the same distribution by changing n to 10 rbinom(n = 15, size = 10, prob = 0.5) ## [1] 4 6 3 5 4 6 6 4 4 7 6 4 3 6 6 It's important not to confuse n with size. The former tells R how many Binomial draws to make. The latter tells R the value of the parameter \\(m\\) of the Binomial\\((m, p)\\) distribution. Perhaps you remember that if \\(X \\sim \\text{Binomial}(m, p)\\) then \\(\\mathbb{E}[X] = mp\\) and \\(\\text{Var}= np(1-p)\\). We can approximate these results numerically by simulating a large number of draws, say 5000, from a Binomial distribution: m &lt;- 20 p &lt;- 0.25 n_sims &lt;- 5000 sim_draws &lt;- rbinom(n_sims, m, p) and then comparing the theoretical value for \\(\\mathbb{E}(X)\\) to a simulation-based approximation: c(EV_Theoretical = m * p, EV_Simulation = mean(sim_draws)) ## EV_Theoretical EV_Simulation ## 5.0000 5.0034 and similarly for \\(\\text{Var}(X)\\) c(Var_Theoretical = m * p * (1 - p), Var_Simulation = var(sim_draws)) ## Var_Theoretical Var_Simulation ## 3.750000 3.934975 Reassuringly, our simulation results are very close to the theoretical values. They would be even closer if we used a larger value for n_sims. 1.1.3 set.seed() A key theme of this book is the importance of reproducible research. Anyone else who wants to check your work should be able to obtain exactly the same results as you did by running your code. But this seems to be at odds with the idea of simulating random data. For example, if I run rbinom(10, 4, 0.6) repeatedly, I'll most likely get different results each time: rbinom(10, 4, 0.6) ## [1] 2 4 2 2 4 2 3 3 3 2 rbinom(10, 4, 0.6) ## [1] 1 2 4 3 2 2 0 1 2 4 rbinom(10, 4, 0.6) ## [1] 2 3 2 2 1 2 2 2 3 3 The function set.seed() allows us to ensure that we obtain the same simulation draws whenever we re-run the same simulation code. To use it, we simply choose a seed, any integer between negative and positive \\((2^{31} - 1)\\), and supply it as an argument to set.seed(). Simulation draws made on a computer aren't really random: they're only pseudo-random. This means that they \"look\" random and pass statistical tests for randomness but are in fact generated by a completely deterministic algorithm. Setting the seed sets the initial condition of the pseudorandom number generator. Because the algorithm is deterministic, the same initial condition always leads to the same results. This is what allows us to replicate our simulation draws. Each time we make another draw, the seed changes. But we can always return it to its previous state using set.seed(). For example, suppose I set the seed to 1. and re-run my code from above as follows set.seed(1) x1 &lt;- rbinom(10, 4, 0.6) x1 ## [1] 3 3 2 1 3 1 1 2 2 4 If I run rbinom(10, 4, 0.6) again, I will most likely not get the same result, because the state of the pseudorandom number generator has changed: x2 &lt;- rbinom(10, 4, 0.6) x2 ## [1] 3 3 2 3 2 2 2 0 3 2 identical(x1, x2) # safe/reliable way to test if two objects are exactly equal ## [1] FALSE but if I reset the seed to 1 I'll obtain exactly the same result as before: set.seed(1) x3 &lt;- rbinom(10, 4, 0.6) x3 ## [1] 3 3 2 1 3 1 1 2 2 4 identical(x3, x1) ## [1] TRUE Whenever you write simulation code, start by choosing a seed and adding the line set.seed(MY-SEED-GOES-HERE) to the top of your R script. You'll often see people use set.seed(12345) or set.seed(54321). When I'm not feeling lazy, I like to generate a truly random number to use as my seed. The website random.org provides free access to bona fide random numbers generated from atmospheric noise. The \"True Random Number Generator\" on the top right of their main page allows you to make uniform integer draws on a range from \"Min\" to \"Max.\" Using the widest possible range, \\(\\pm1\\times 10^9\\), I generated the seed 420508570 which I'll use in the following exercises. 1.1.4 Exercises Set your seed to 420508570 at the start of your solution code for each of these exercises to obtain results that match the solutions. Run sample(x = my_vector, size = 10). What happens and why? Show Solution R will throw an error. You can't make ten draws without replacement from a set of five objects: set.seed(420508570) sample(x = my_vector, size = 10) ## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39; Write a line of code that makes five draws without replacement from the set of numbers \\(\\{1, 2, 3, ..., 100\\}\\). Show Solution set.seed(420508570) sample(1:100, 5) ## [1] 100 67 51 44 12 Create a vector of thirty elements called urn that represents an urn containing ten blue balls and twenty red balls. Draw five balls with replacement from urn and store the draws in a vector called draws. Then write a line of code to count up the number of blue balls in draws. Show Hint Use rep() and c() to construct urn. Use == and sum() to count up the number of blue balls in draws. See the relevant help files for details, e.g. ?rep. Show Solution set.seed(420508570) urn &lt;- c(rep(&#39;blue&#39;, 10), rep(&#39;red&#39;, 20)) draws &lt;- sample(urn, 5, replace = TRUE) draws ## [1] &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; sum(draws == &#39;blue&#39;) ## [1] 2 Make 1000 draws from a normal distribution with mean 5 and variance 4 and store them in a vector called normal_sims. Calculate the mean and variance of your draws, and plot a histogram. Show Hint Consult the help file for rnorm() paying close attention to the fact that R specifies the normal distribution in terms of a mean and standard deviation rather than a mean and variance. You can plot a histogram with any number of functions: e.g. the base R function hist() or qplot() from the ggplot2 package. Show Solution set.seed(420508570) normal_sims &lt;- rnorm(1000, 5, 2) # Variance = 4; Standard Dev. = 2 mean(normal_sims) ## [1] 4.895722 var(normal_sims) ## [1] 3.912262 ggplot2::qplot(normal_sims, bins = 25) There is no built-in R function called rbern() for simulating draws from the Bernoulli Distribution with probability of success \\(p\\). Write one of your own and use it to make ten Bernoulli(0.8) draws. Your function rbern() should take two arguments: the number of draws n and the probability of success p. Show Hint There are various ways to do this. The simplest is by setting the arguments of rbinom() appropriately. Show Solution rbern &lt;- function(n, p) { # Make n random draws from a Bernoulli(p) distribution rbinom(n, size = 1, prob = p) } set.seed(420508570) rbern(10, 0.8) ## [1] 1 1 1 1 0 1 1 1 1 1 1.2 The Skeleton of a Simulation Study While the specific details will vary, nearly every simulation study has the same basic structure: Generate simulated data. Calculate an estimate from the simulated data. Repeat steps 1 and 2 many times, saving each of the estimates. Summarize the results. Thinking in terms of this structure helps us to write code that is easier to understand, easier to generalize, and faster to run. The key is to break these steps down into functions that carry out a single, well-defined task. Generally these will include: A function to generate simulated data. A function to calculate an estimate from the data. A function that repeatedly calls i. and ii. and summarizes the results. This may sound a bit abstract, so in the remainder of this section we'll walk through the details in a simple example: estimating the bias of the maximum likelihood estimator for the variance of a normal distribution. Along the way we'll explore three extremely helpful R functions for carrying simulation studies: replicate(), expand.grid(), and Map(). In the next section you'll apply what you've learned to the hot hand example. 1.2.1 A Biased Estimator of \\(\\sigma^2\\) My introductory statistics students often ask me why the sample variance, \\(S^2\\), divides by \\((n-1)\\) rather than the sample size \\(n\\): \\[ S^2 \\equiv \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2 \\] The answer is that dividing by \\((n-1)\\) yields an unbiased estimator: if \\(X_1, ..., X_n\\) are a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then \\(\\mathbb{E}[S^2] = \\sigma^2\\). So what would happen if we divided by \\(n\\) instead? Consider the estimator \\(\\widehat{\\sigma}^2\\) defined by \\[ \\widehat{\\sigma}^2 \\equiv \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X}_n)^2. \\] If \\(X_i \\sim \\text{Normal}(\\mu, \\sigma^2)\\) then \\(\\widehat{\\sigma}^2\\) is in fact the maximum likelihood estimator for \\(\\sigma^2\\). With a bit of algebra, we can show that \\(\\mathbb{E}[\\widehat{\\sigma}^2] = (n-1)\\sigma^2/n\\) which clearly does not equal the population variance.15 It follows that \\[ \\text{Bias}(\\widehat{\\sigma}^2) \\equiv \\mathbb{E}[\\widehat{\\sigma}^2 - \\sigma^2] = -\\sigma^2/n \\] so \\(\\widehat{\\sigma}^2\\) is biased downwards. Because the bias goes to zero as the sample size grows, however, it is still a consistent estimator of \\(\\sigma^2\\). Another way to see that \\(\\widehat{\\sigma}^2\\) is biased is by carrying out a simulation study. To do this, we generate data from a distribution with a known variance and calculate \\(\\widehat{\\sigma}^2\\). Then we generate a new dataset from the same distribution and again calculate the corresponding value of \\(\\widehat{\\sigma}^2\\). Repeating this a large number of times, we end up with many estimates \\(\\widehat{\\sigma}^2\\), each based on a dataset of the same size drawn independently from the same population. This collection of estimates gives us an approximation to the sampling distribution of \\(\\widehat{\\sigma}^2\\). Using this approximation, we can get a good estimate of \\(\\text{Bias}(\\widehat{\\sigma}^2)\\) by comparing the sample mean of our simulated estimates \\(\\widehat{\\sigma}^2\\) to the true variance \\(\\sigma^2\\). 1.2.2 draw_sim_data() The first thing we need is a function to generate simulated data. Let's draw the \\(X_1, ..., X_n\\) from a normal distribution with mean zero and variance s_sq. To do this, we write a simple R function as follows: draw_sim_data &lt;- function(n, s_sq) { rnorm(n, sd = sqrt(s_sq)) } The nice thing about writing such a function is that we can test that it's working correctly. For example, suppose you were worried that draw_sim_data does not in fact generate n draws from a normal distribution with mean zero and variance s_sq. Then you could simply draw a large sample and check! Here I'll verify that draw_sim_data() returns a vector of the expected length, with the desired mean and variance, drawn from normal distribution.16 Everything works as expected: set.seed(420508570) test_sims &lt;- draw_sim_data(5000, 9) length(test_sims) ## [1] 5000 mean(test_sims) ## [1] -0.04620928 var(test_sims) ## [1] 8.832994 qqnorm(test_sims) qqline(test_sims) 1.2.3 get_estimate() The next step is to write a function that calculates \\(\\widehat{\\sigma}^2\\). We can do this as follows: get_estimate &lt;- function(x) { sum((x - mean(x))^2) / length(x) # divides by n not (n-1) } Again it's a good idea to test your code before proceeding. There are several tests we could consider running. First, if all the elements of x are the same then get_estimate() should return zero because (x - mean(x)) will simply be a vector of zeros. Everything looks good: get_estimate(rep(5, 25)) ## [1] 0 get_estimate(rep(0, 10)) ## [1] 0 Second, get_estimate() should not in general give the same result as var(), R's built-in function for the sample variance. This is because the latter divides by \\(n\\) rather than \\((n-1)\\). But if \\(n\\) is very large, this difference should become negligible. Again, everything works as expected: set.seed(420508570) sim_small &lt;- draw_sim_data(5, 1) c(sigma_hat_sq = get_estimate(sim_small), Sample_Var = var(sim_small)) ## sigma_hat_sq Sample_Var ## 0.1710749 0.2138436 sim_big &lt;- draw_sim_data(5000, 1) c(sigma_hat_sq = get_estimate(sim_big), Sample_Var = var(sim_big)) ## sigma_hat_sq Sample_Var ## 0.9814408 0.9816371 1.2.4 get_bias() Now we're ready to actually carry out our simulation study. The final step is to write a function called get_bias() that repeatedly calls draw_sim_data() and get_estimate(), stores the resulting estimates \\(\\widehat{\\sigma}^2\\) and calculates a simulation estimate of the bias. Compared to the functions from above, this one will be more complicated, so I'll explain it in steps. First the code: get_bias &lt;- function(n, s_sq, n_reps = 5000) { draw_sim_replication &lt;- function() { sim_data &lt;- draw_sim_data(n, s_sq) get_estimate(sim_data) } sim_replications &lt;- replicate(n_reps, draw_sim_replication()) mean(sim_replications - s_sq) } The function get_bias() takes three arguments: n is the sample size for each replication of the simulation experiment, s_sq is the true population variance from which we will simulate normal data, and n_reps is the number of simulation replications, i.e. the number of times that we want to repeat the simulation. The final argument, n_reps is optional: if you call get_bias() and supply only the first two arguments, R will set n_reps equal to the default value of 5000. The first step inside of get_bias() constructs a function called draw_sim_replication() that doesn't take any input arguments. This may seem strange: I'll explain it in a moment. For now, focus on the steps that draw_sim_replication() carries out. It first runs draw_sim_data(n, s_q) and stores the result in a vector called sim_data. Next it feeds sim_data as an input to get_estimate(). In other words, it carries out one replication of our simulation experiment. But how does the call to draw_sim_data() \"know\" which values to use for n and s_sq given that draw_sim_replication() doesn't take any input arguments? The key is that draw_sim_replication() is created inside of another function: get_bias(). When draw_sim_replication() encounters a reference to n and s_sq, it substitutes the values that were supplied as arguments to get_bias(). Here's another way of looking at draw_sim_replication(). We want to be able to run our simulation study for different values of n and s_sq. After we tell get_bias() our desired values of n and s_sq, it constructs a function for us called draw_sim_replication() that hard codes these particular parameter values. From this point on, calling draw_sim_replication() does \"the right thing\" without our having to explicitly specify n and s_sq. The next step of get_bias() uses the function replicate() to repeatedly call the function draw_sim_replication() a total of n_reps times. The results are stored in a vector called sim_replications. In essence, replicate() is shorthand for a common way of using a for loop. In the following example, x and y will be identical. But constructing x requires much more work: we first need to set up an empty vector, and then explicitly loop over it. In contrast, replicate() does all of this behind the scenes to construct y: do_something &lt;- function() { return(42) } x &lt;- rep(NA, 50) for(i in 1:50) { x[i] &lt;- do_something() } y &lt;- replicate(50, do_something()) identical(x, y) ## [1] TRUE Finally, get_bias() uses the simulation replications stored in the vector sim_replications to approximate the bias of \\(\\widehat{\\sigma}^2\\) by comparing them to the true value of \\(\\sigma^2\\), namely s_sq. It does this by computing the simulation analogue of \\(\\mathbb{E}[\\widehat{\\sigma}^2 - \\sigma^2]\\), which is simply mean(sim_replications - s_sq). 1.2.5 Running the Simulation Study Now we're ready to run our simulation study: we simply need to call get_bias() with our desired values of n and s_sq, for example: set.seed(420508570) get_bias(n = 5, s_sq = 1) ## [1] -0.2007312 It works! Up to simulation error, this result agrees with the theoretical bias of \\(-\\sigma^2/n = -1/5\\). To see that this isn't simply a fluke, we could try different values of n and s_sq. Again, the results agree with the theoretical values: set.seed(420508570) c(theoretical = -1/3, simulation = get_bias(3, 1)) ## theoretical simulation ## -0.3333333 -0.3412176 1.2.6 expand.grid() and Map() Now we have a function get_bias() that can approximate the bias of \\(\\widehat{\\sigma}^2\\) for any values of n and s_sq that we care to specify. But what if we want to carry out a simulation study over a range of values for n and s_sq? One way to do this is with a pair of nested for loops: one that iterates over different values of n and another that iterates over different values of s_sq. But this isn't a great strategy for two reasons. First, loops within loops tend to be slow in R. Second, the book-keeping required to implement this strategy is a bit involved. Fortunately there's a much better way: use expand.grid() and Map(). First we'll set up a grid of values for n and s_sq: n_grid &lt;- 3:5 n_grid ## [1] 3 4 5 s_sq_grid &lt;- seq(from = 1, to = 3, by = 0.5) s_sq_grid ## [1] 1.0 1.5 2.0 2.5 3.0 Now suppose that we want to run get_bias() for every combination of values in n_grid and s_sq_grid. Using the built-in R function expand.grid() we can easily construct a data frame whose rows contain all of these combinations: parameters &lt;- expand.grid(n = n_grid, s_sq = s_sq_grid) parameters ## n s_sq ## 1 3 1.0 ## 2 4 1.0 ## 3 5 1.0 ## 4 3 1.5 ## 5 4 1.5 ## 6 5 1.5 ## 7 3 2.0 ## 8 4 2.0 ## 9 5 2.0 ## 10 3 2.5 ## 11 4 2.5 ## 12 5 2.5 ## 13 3 3.0 ## 14 4 3.0 ## 15 5 3.0 The next step is to evaluated get_bias() repeatedly, once for every combination of parameter values stored in the rows of parameters. The Map() function makes this easy: set.seed(420508570) bias &lt;- Map(get_bias, n = parameters$n, s_sq = parameters$s_sq) Much like replicate(), Map() is shorthand for a common kind of for loop. In this case we loop over the rows of parameters. The first argument to Map() is the name of the function that we want to call repeatedly, in our case get_bias(). The remaining arguments are vectors of values. These are the arguments that Map() passes to get_bias(). The result of running the above code is a list of value, one for each row of parameters. head(bias) ## [[1]] ## [1] -0.3412176 ## ## [[2]] ## [1] -0.2546259 ## ## [[3]] ## [1] -0.2005197 ## ## [[4]] ## [1] -0.5007384 ## ## [[5]] ## [1] -0.3816447 ## ## [[6]] ## [1] -0.292554 length(bias) ## [1] 15 For example, the first element of bias corresponds to get_bias(3, 1). By setting the same seed and running this command \"manually\" we can verify that everything works as expected: set.seed(420508570) identical(get_bias(3, 1), bias[[1]]) ## [1] TRUE This pattern using expand.grid() and Map() is extremely flexible. In our example, get_bias() returns a scalar so bias is just a list of numbers. But more generally Map() can return a list that contains any kind of object at all. Here's a slightly more interesting example. The function get_bias_and_var() is a very slight modification of get_bias() from above that returns a list of two named elements: bias and variance get_bias_and_var &lt;- function(n, s_sq, n_reps = 5000) { draw_sim_replication &lt;- function() { sim_data &lt;- draw_sim_data(n, s_sq) get_estimate(sim_data) } sim_replications &lt;- replicate(n_reps, draw_sim_replication()) list(bias = mean(sim_replications - s_sq), variance = var(sim_replications)) } We can use this function to calculate both the bias and variance of the MLE \\(\\widehat{\\sigma}^2\\) as follows set.seed(420508570) bias_and_variance &lt;- Map(get_bias_and_var, n = parameters$n, s_sq = parameters$s_sq) bias_and_variance[1:3] ## [[1]] ## [[1]]$bias ## [1] -0.3412176 ## ## [[1]]$variance ## [1] 0.4554343 ## ## ## [[2]] ## [[2]]$bias ## [1] -0.2546259 ## ## [[2]]$variance ## [1] 0.3643993 ## ## ## [[3]] ## [[3]]$bias ## [1] -0.2005197 ## ## [[3]]$variance ## [1] 0.3243315 1.2.7 Formatting the Results We've carried out our simulation experiment, but the results are a bit messy. The parameter values are stored in a data frame called parameters and the biases and variances are stored in a list called bias_and_var. Let's format things a bit more nicely. If you have a list my_list whose elements are \"rows\" and you want to bind them together into a data frame, you can use the somewhat inscrutable command do.call(rbind, my_list). For example: do.call(rbind, bias_and_variance) ## bias variance ## [1,] -0.3412176 0.4554343 ## [2,] -0.2546259 0.3643993 ## [3,] -0.2005197 0.3243315 ## [4,] -0.5007384 0.9496033 ## [5,] -0.3816447 0.8616167 ## [6,] -0.292554 0.7173205 ## [7,] -0.6609821 1.781236 ## [8,] -0.5061243 1.538961 ## [9,] -0.3953514 1.326648 ## [10,] -0.8513285 2.705087 ## [11,] -0.6122964 2.467789 ## [12,] -0.4727481 2.014822 ## [13,] -0.9570253 4.124553 ## [14,] -0.7513177 3.201657 ## [15,] -0.567449 2.916767 Now we'll overwrite bias_and_var with the above and bind its columns, cbind(), with those of parameters bias_and_variance &lt;- do.call(rbind, bias_and_variance) sim_results &lt;- cbind(parameters, bias_and_variance) sim_results ## n s_sq bias variance ## 1 3 1.0 -0.3412176 0.4554343 ## 2 4 1.0 -0.2546259 0.3643993 ## 3 5 1.0 -0.2005197 0.3243315 ## 4 3 1.5 -0.5007384 0.9496033 ## 5 4 1.5 -0.3816447 0.8616167 ## 6 5 1.5 -0.292554 0.7173205 ## 7 3 2.0 -0.6609821 1.781236 ## 8 4 2.0 -0.5061243 1.538961 ## 9 5 2.0 -0.3953514 1.326648 ## 10 3 2.5 -0.8513285 2.705087 ## 11 4 2.5 -0.6122964 2.467789 ## 12 5 2.5 -0.4727481 2.014822 ## 13 3 3.0 -0.9570253 4.124553 ## 14 4 3.0 -0.7513177 3.201657 ## 15 5 3.0 -0.567449 2.916767 For extra credit, we can use the function kable() from the knitr package to make the results look even prettier knitr::kable(sim_results) n s_sq bias variance 3 1.0 -0.3412176 0.4554343 4 1.0 -0.2546259 0.3643993 5 1.0 -0.2005197 0.3243315 3 1.5 -0.5007384 0.9496033 4 1.5 -0.3816447 0.8616167 5 1.5 -0.292554 0.7173205 3 2.0 -0.6609821 1.781236 4 2.0 -0.5061243 1.538961 5 2.0 -0.3953514 1.326648 3 2.5 -0.8513285 2.705087 4 2.5 -0.6122964 2.467789 5 2.5 -0.4727481 2.014822 3 3.0 -0.9570253 4.124553 4 3.0 -0.7513177 3.201657 5 3.0 -0.567449 2.916767 1.2.8 Bonus: Parallel Computing with mcMap() There's a good chance that your computer has more than one processor core. Why not use them to speed up your simulation study! If you're on Mac or Linux (RStudio Cloud counts as Linux), it's trivial to parallelize our simulation code from above using mcMap() from the parallel package. To make things more interesting, let's set up a larger parameter grid parameters &lt;- expand.grid(n = 3:10, s_sq = 1:10) The function mcMap() works exactly like Map() except that we have the option of setting an additional parameter mc.cores. This tells R how many of our machine's processor cores to devote to the computation. If you set mc.cores = 1, mcMap() set.seed(420508570) system.time(foo &lt;- parallel::mcMap(get_bias_and_var, n = parameters$n, s_sq = parameters$s_sq, mc.cores = 2)) ## user system elapsed ## 1.979 0.064 2.044 set.seed(420508570) system.time(bar &lt;- Map(get_bias_and_var, n = parameters$n, s_sq = parameters$s_sq)) ## user system elapsed ## 3.854 0.028 3.882 all.equal(foo, bar) ## [1] &quot;Component 1: Component 1: Mean relative difference: 0.06149509&quot; ## [2] &quot;Component 1: Component 2: Mean relative difference: 0.04710864&quot; ## [3] &quot;Component 2: Component 1: Mean relative difference: 0.03128287&quot; ## [4] &quot;Component 2: Component 2: Mean relative difference: 0.01859252&quot; ## [5] &quot;Component 3: Component 1: Mean relative difference: 0.02524505&quot; ## [6] &quot;Component 3: Component 2: Mean relative difference: 0.02680897&quot; ## [7] &quot;Component 4: Component 1: Mean relative difference: 0.04489461&quot; ## [8] &quot;Component 4: Component 2: Mean relative difference: 0.02247478&quot; ## [9] &quot;Component 5: Component 1: Mean relative difference: 0.003152408&quot; ## [10] &quot;Component 5: Component 2: Mean relative difference: 0.01709459&quot; ## [11] &quot;Component 6: Component 1: Mean relative difference: 0.01240296&quot; ## [12] &quot;Component 6: Component 2: Mean relative difference: 0.02663298&quot; ## [13] &quot;Component 7: Component 1: Mean relative difference: 0.1075489&quot; ## [14] &quot;Component 7: Component 2: Mean relative difference: 0.003944765&quot; ## [15] &quot;Component 8: Component 1: Mean relative difference: 0.05583175&quot; ## [16] &quot;Component 8: Component 2: Mean relative difference: 0.02507958&quot; ## [17] &quot;Component 9: Component 1: Mean relative difference: 0.01482714&quot; ## [18] &quot;Component 9: Component 2: Mean relative difference: 0.0330364&quot; ## [19] &quot;Component 10: Component 1: Mean relative difference: 0.008100632&quot; ## [20] &quot;Component 10: Component 2: Mean relative difference: 0.06709488&quot; ## [21] &quot;Component 11: Component 1: Mean relative difference: 0.04625344&quot; ## [22] &quot;Component 11: Component 2: Mean relative difference: 0.04040989&quot; ## [23] &quot;Component 12: Component 1: Mean relative difference: 0.08989413&quot; ## [24] &quot;Component 12: Component 2: Mean relative difference: 0.05590757&quot; ## [25] &quot;Component 13: Component 1: Mean relative difference: 0.09476891&quot; ## [26] &quot;Component 13: Component 2: Mean relative difference: 0.005544054&quot; ## [27] &quot;Component 14: Component 1: Mean relative difference: 0.02380539&quot; ## [28] &quot;Component 14: Component 2: Mean relative difference: 0.009128659&quot; ## [29] &quot;Component 15: Component 1: Mean relative difference: 0.1210738&quot; ## [30] &quot;Component 15: Component 2: Mean relative difference: 0.04037108&quot; ## [31] &quot;Component 16: Component 1: Mean relative difference: 0.037408&quot; ## [32] &quot;Component 16: Component 2: Mean relative difference: 0.008688766&quot; ## [33] &quot;Component 17: Component 1: Mean relative difference: 0.04984693&quot; ## [34] &quot;Component 17: Component 2: Mean relative difference: 0.06739675&quot; ## [35] &quot;Component 18: Component 1: Mean relative difference: 0.01011775&quot; ## [36] &quot;Component 18: Component 2: Mean relative difference: 0.009698451&quot; ## [37] &quot;Component 19: Component 1: Mean relative difference: 0.03377475&quot; ## [38] &quot;Component 19: Component 2: Mean relative difference: 0.06191194&quot; ## [39] &quot;Component 20: Component 1: Mean relative difference: 0.08139882&quot; ## [40] &quot;Component 20: Component 2: Mean relative difference: 0.08888602&quot; ## [41] &quot;Component 21: Component 1: Mean relative difference: 0.07059438&quot; ## [42] &quot;Component 21: Component 2: Mean relative difference: 0.03692975&quot; ## [43] &quot;Component 22: Component 1: Mean relative difference: 0.08980679&quot; ## [44] &quot;Component 22: Component 2: Mean relative difference: 0.06080832&quot; ## [45] &quot;Component 23: Component 1: Mean relative difference: 0.04786009&quot; ## [46] &quot;Component 23: Component 2: Mean relative difference: 0.0006149753&quot; ## [47] &quot;Component 24: Component 1: Mean relative difference: 0.03040031&quot; ## [48] &quot;Component 24: Component 2: Mean relative difference: 0.01116832&quot; ## [49] &quot;Component 25: Component 1: Mean relative difference: 0.02701506&quot; ## [50] &quot;Component 25: Component 2: Mean relative difference: 0.03319803&quot; ## [51] &quot;Component 26: Component 1: Mean relative difference: 0.03981558&quot; ## [52] &quot;Component 26: Component 2: Mean relative difference: 0.07286661&quot; ## [53] &quot;Component 27: Component 1: Mean relative difference: 0.06022105&quot; ## [54] &quot;Component 27: Component 2: Mean relative difference: 0.07607582&quot; ## [55] &quot;Component 28: Component 1: Mean relative difference: 0.002308189&quot; ## [56] &quot;Component 28: Component 2: Mean relative difference: 0.02073226&quot; ## [57] &quot;Component 29: Component 1: Mean relative difference: 0.04221814&quot; ## [58] &quot;Component 29: Component 2: Mean relative difference: 0.002609435&quot; ## [59] &quot;Component 30: Component 1: Mean relative difference: 0.09551194&quot; ## [60] &quot;Component 30: Component 2: Mean relative difference: 0.05106915&quot; ## [61] &quot;Component 31: Component 1: Mean relative difference: 0.03760636&quot; ## [62] &quot;Component 31: Component 2: Mean relative difference: 0.01298729&quot; ## [63] &quot;Component 32: Component 1: Mean relative difference: 0.04650797&quot; ## [64] &quot;Component 32: Component 2: Mean relative difference: 0.003727321&quot; ## [65] &quot;Component 33: Component 1: Mean relative difference: 0.08639797&quot; ## [66] &quot;Component 33: Component 2: Mean relative difference: 0.1000014&quot; ## [67] &quot;Component 34: Component 1: Mean relative difference: 0.00870074&quot; ## [68] &quot;Component 34: Component 2: Mean relative difference: 0.08779199&quot; ## [69] &quot;Component 35: Component 1: Mean relative difference: 0.1471947&quot; ## [70] &quot;Component 35: Component 2: Mean relative difference: 0.04769264&quot; ## [71] &quot;Component 36: Component 1: Mean relative difference: 0.1002942&quot; ## [72] &quot;Component 36: Component 2: Mean relative difference: 0.0742667&quot; ## [73] &quot;Component 37: Component 1: Mean relative difference: 0.0536998&quot; ## [74] &quot;Component 37: Component 2: Mean relative difference: 0.01792097&quot; ## [75] &quot;Component 38: Component 1: Mean relative difference: 0.05012375&quot; ## [76] &quot;Component 38: Component 2: Mean relative difference: 0.0326725&quot; ## [77] &quot;Component 39: Component 1: Mean relative difference: 0.001007286&quot; ## [78] &quot;Component 39: Component 2: Mean relative difference: 0.01544382&quot; ## [79] &quot;Component 40: Component 1: Mean relative difference: 0.0316639&quot; ## [80] &quot;Component 40: Component 2: Mean relative difference: 0.01686306&quot; ## [81] &quot;Component 41: Component 1: Mean relative difference: 0.007044499&quot; ## [82] &quot;Component 41: Component 2: Mean relative difference: 0.06212322&quot; ## [83] &quot;Component 42: Component 1: Mean relative difference: 0.008718099&quot; ## [84] &quot;Component 42: Component 2: Mean relative difference: 0.01712632&quot; ## [85] &quot;Component 43: Component 1: Mean relative difference: 0.08325414&quot; ## [86] &quot;Component 43: Component 2: Mean relative difference: 0.04423943&quot; ## [87] &quot;Component 44: Component 1: Mean relative difference: 0.004437248&quot; ## [88] &quot;Component 44: Component 2: Mean relative difference: 0.02570064&quot; ## [89] &quot;Component 45: Component 1: Mean relative difference: 0.01789466&quot; ## [90] &quot;Component 45: Component 2: Mean relative difference: 0.01422507&quot; ## [91] &quot;Component 46: Component 1: Mean relative difference: 0.009239117&quot; ## [92] &quot;Component 46: Component 2: Mean relative difference: 0.004125723&quot; ## [93] &quot;Component 47: Component 1: Mean relative difference: 0.004809207&quot; ## [94] &quot;Component 47: Component 2: Mean relative difference: 0.02484232&quot; ## [95] &quot;Component 48: Component 1: Mean relative difference: 0.09797341&quot; ## [96] &quot;Component 48: Component 2: Mean relative difference: 0.06000661&quot; ## [97] &quot;Component 49: Component 1: Mean relative difference: 0.00552035&quot; ## [98] &quot;Component 49: Component 2: Mean relative difference: 0.0136581&quot; ## [99] &quot;Component 50: Component 1: Mean relative difference: 0.07598621&quot; ## [100] &quot;Component 50: Component 2: Mean relative difference: 0.01745085&quot; ## [101] &quot;Component 51: Component 1: Mean relative difference: 0.01641752&quot; ## [102] &quot;Component 51: Component 2: Mean relative difference: 0.01995855&quot; ## [103] &quot;Component 52: Component 1: Mean relative difference: 0.09275417&quot; ## [104] &quot;Component 52: Component 2: Mean relative difference: 0.01460155&quot; ## [105] &quot;Component 53: Component 1: Mean relative difference: 0.005564893&quot; ## [106] &quot;Component 53: Component 2: Mean relative difference: 0.01348838&quot; ## [107] &quot;Component 54: Component 1: Mean relative difference: 0.1156366&quot; ## [108] &quot;Component 54: Component 2: Mean relative difference: 0.004612475&quot; ## [109] &quot;Component 55: Component 1: Mean relative difference: 0.01444924&quot; ## [110] &quot;Component 55: Component 2: Mean relative difference: 0.02837381&quot; ## [111] &quot;Component 56: Component 1: Mean relative difference: 0.03616001&quot; ## [112] &quot;Component 56: Component 2: Mean relative difference: 0.008763745&quot; ## [113] &quot;Component 57: Component 1: Mean relative difference: 0.06309874&quot; ## [114] &quot;Component 57: Component 2: Mean relative difference: 0.15002&quot; ## [115] &quot;Component 58: Component 1: Mean relative difference: 0.06560207&quot; ## [116] &quot;Component 58: Component 2: Mean relative difference: 0.1058275&quot; ## [117] &quot;Component 59: Component 1: Mean relative difference: 0.06993669&quot; ## [118] &quot;Component 59: Component 2: Mean relative difference: 0.07861996&quot; ## [119] &quot;Component 60: Component 1: Mean relative difference: 0.02784954&quot; ## [120] &quot;Component 60: Component 2: Mean relative difference: 0.01656501&quot; ## [121] &quot;Component 61: Component 1: Mean relative difference: 0.04417603&quot; ## [122] &quot;Component 61: Component 2: Mean relative difference: 0.005620457&quot; ## [123] &quot;Component 62: Component 1: Mean relative difference: 0.01336418&quot; ## [124] &quot;Component 62: Component 2: Mean relative difference: 0.006717068&quot; ## [125] &quot;Component 63: Component 1: Mean relative difference: 0.06448091&quot; ## [126] &quot;Component 63: Component 2: Mean relative difference: 0.007571636&quot; ## [127] &quot;Component 64: Component 1: Mean relative difference: 0.1948133&quot; ## [128] &quot;Component 64: Component 2: Mean relative difference: 0.03868885&quot; ## [129] &quot;Component 65: Component 1: Mean relative difference: 0.06324527&quot; ## [130] &quot;Component 65: Component 2: Mean relative difference: 0.07522468&quot; ## [131] &quot;Component 66: Component 1: Mean relative difference: 0.04488434&quot; ## [132] &quot;Component 66: Component 2: Mean relative difference: 0.03355504&quot; ## [133] &quot;Component 67: Component 1: Mean relative difference: 0.05911418&quot; ## [134] &quot;Component 67: Component 2: Mean relative difference: 0.06012642&quot; ## [135] &quot;Component 68: Component 1: Mean relative difference: 0.1356427&quot; ## [136] &quot;Component 68: Component 2: Mean relative difference: 0.05133508&quot; ## [137] &quot;Component 69: Component 1: Mean relative difference: 0.1460275&quot; ## [138] &quot;Component 69: Component 2: Mean relative difference: 0.0191871&quot; ## [139] &quot;Component 70: Component 1: Mean relative difference: 0.00380694&quot; ## [140] &quot;Component 70: Component 2: Mean relative difference: 0.008051321&quot; ## [141] &quot;Component 71: Component 1: Mean relative difference: 0.04087702&quot; ## [142] &quot;Component 71: Component 2: Mean relative difference: 0.009812294&quot; ## [143] &quot;Component 72: Component 1: Mean relative difference: 0.06803322&quot; ## [144] &quot;Component 72: Component 2: Mean relative difference: 0.03635741&quot; ## [145] &quot;Component 73: Component 1: Mean relative difference: 0.04557451&quot; ## [146] &quot;Component 73: Component 2: Mean relative difference: 0.05040945&quot; ## [147] &quot;Component 74: Component 1: Mean relative difference: 0.09474043&quot; ## [148] &quot;Component 74: Component 2: Mean relative difference: 0.006748576&quot; ## [149] &quot;Component 75: Component 1: Mean relative difference: 0.0444338&quot; ## [150] &quot;Component 75: Component 2: Mean relative difference: 0.05602434&quot; ## [151] &quot;Component 76: Component 1: Mean relative difference: 0.05063147&quot; ## [152] &quot;Component 76: Component 2: Mean relative difference: 0.025101&quot; ## [153] &quot;Component 77: Component 1: Mean relative difference: 0.0518059&quot; ## [154] &quot;Component 77: Component 2: Mean relative difference: 0.01004066&quot; ## [155] &quot;Component 78: Component 1: Mean relative difference: 0.09793807&quot; ## [156] &quot;Component 78: Component 2: Mean relative difference: 0.008405234&quot; ## [157] &quot;Component 79: Component 1: Mean relative difference: 0.01725115&quot; ## [158] &quot;Component 79: Component 2: Mean relative difference: 0.001187539&quot; ## [159] &quot;Component 80: Component 1: Mean relative difference: 0.02704408&quot; ## [160] &quot;Component 80: Component 2: Mean relative difference: 0.009683642&quot; 1.3 Your Turn: Simulating the Hot Hand 1.4 Further Reading Advanced R book for more on replicate() and lapply() as well as scoping rules for functions within functions etc. 1.5 Mini Projects 1.5.1 The Hot Hand Revisited In the lesson we generated iid data. Now try simulating from a first-order Markov chain for various different parameters. How do the results change? 1.5.2 Which estimator of the variance is really better? Compare the RMSE of two variance estimators: the unbiased estimator and the MLE. 1.5.3 Optimal Stopping Simulate the secretary problem; try to find the optimal stopping rule. 1.6 DRAFT MATERIAL Should I introduce knitr and Rmarkdown in this lesson or the next one? Some basic simulation commands in R: sample(), rbinom(), rnorm(), etc. Read the help files. Write a function draw_sim_data() that makes 100 Bernoulli(1/2) draws. Optional arguments p and n? This simulates data when there is no hot hand set.seed() what does it do? Think about how to calculate the estimator: fraction of times that three ones are followed by another one compared to another zero. Suppose you had a function is_after_3_ones() that took a vector of 0 and 1 and returned, for each element, whether it is after three ones. How would you use it? Write this function. Put everything together with replicate() to do a simple sim for \\(p=1/2\\) and \\(n = 100\\). How about trying different values of \\(n\\) and \\(p\\)? Need to keep results organized: apply() family of functions (or maybe the tidy equivalents?) Try doing it in parallel with mclapply(). First explain the basic idea of parallel and why this is \"embarrassingly parallel.\" Show them how to time the code, illustrate with sys.sleep(). For the students who finish very quickly, have some extensions: a markov chain DGP, and is_after_k_ones() dgp &lt;- function(n = 100, p = 0.5) { rbinom(n, 1, p) } # Maybe have a challenge to write the version for after k ones, but start by # asking them to do the one for after 3 ones is_after_k_ones &lt;- function(x, k) { out &lt;- rep(NA) for(i in (k+1):length(x)) { out[i] &lt;- sum(x[(i - k):(i - 1)]) == k } return(out) } get_est &lt;- function(x) { #ones &lt;- which(x == 1) #mean(x[ones + 1], na.rm = TRUE) mean(x[is_after_k_ones(x, 3)], na.rm = TRUE) } n_reps &lt;- 1000 set.seed(1234) sim_results &lt;- replicate(n_reps, get_est(dgp())) library(ggplot2) qplot(sim_results) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. mean(sim_results, na.rm = TRUE) ## [1] 0.4620068 #ones &lt;- which(x == 1) #x[which(x == 1) + 1] #runs &lt;- rle(x) #foo &lt;- rle(x) #str(foo) #x #foo$lengths #foo$values See for example this write-up of Lantis &amp; Nessen (2021) in the February 2022 NBER digest.↩︎ See Miller &amp; Sanjurjo (2019) for a more accessible explanation that connects to several related probability puzzles.↩︎ Technically, \"random\" draws made on a computer are only pseudorandom. We'll discuss this further below.↩︎ For less common distributions, see CRAN Task View: Probability Distributions↩︎ A Bernoulli trial is a model for a possibly biased coin flip: if \\(X \\sim \\text{Bernoulli}(p)\\) then \\(\\mathbb{P}(X=1) = p\\) and \\(\\mathbb{P}(X=0) = 1-p\\).↩︎ To see this, first rewrite \\(\\sum_{i=1}^n (X_i - \\bar{X}_n)^2\\) as \\(\\sum_{i=1}^n (X_i - \\mu)^2 - n(\\bar{X}_n - \\mu)^2\\). This step is just algebra. Then take expectations, using the fact that the \\(X_i\\) are independent and identically distributed.↩︎ If you're unfamiliar with the Normal Q-Q plot that I used to check the normality of test_sims, you can read about it this blog post.↩︎ "],["mind-the-gap-part-i.html", "Lesson 2 Mind the Gap Part I 2.1 Getting Started 2.2 What is a tibble? 2.3 Filter Rows with filter 2.4 Sort data with arrange 2.5 Choose columns with select 2.6 The summarize verb 2.7 The group_by verb 2.8 Understanding the pipe: %&gt;% 2.9 Chaining commands 2.10 Change an existing variable or create a new one with mutate", " Lesson 2 Mind the Gap Part I Write an introduction about Gapminder and Hans Rosling and \"Factfulness.\" Possibly include the famous quiz and point out that exploratory data analysis can be extremely valuable. This chapter teaches some basic tools. This lesson provides a crash course in two R packages that we'll used heavily in the material that follows: dplyr and ggplot2. The dplyr package provides a number of powerful but easy-to-use tools for data manipulation in R. A good reference is the chapter entitled \"Data Transformation\" in R for Data Science. We'll be making heavy use of dplyr throughout the semester. Rather than trying to explain everything in advance, let's just dive right in. So what are these packages? Base R plotting and data manipulation have their place: you need to know about them. But when you use them to do anything moderately complicated, the code gets quite verbose and hard to understand, particularly for someone who didn't write it. dplyr is a powerful package for data wrangling: cleaning, merging, summarizing, etc. It makes things that are tricky using base R extremely simple. Most importantly its syntax is extremely intuitive and easy to read. There are two alternatives to base R for data wrangling: dplyr and data.table. The latter is unambiguously faster than dplyr. If you want to work with datasets that contain a million rows or more, it's worth learning data.table. I've used and taught both and dplyr is much easier to learn in my opinion. It also has some nice features that data.table lacks. Link to the Stack Overflow question for more pros and cons. ggplot2 is an extremely powerful and widely-used package for data visualization. Implements \"grammar of graphics\" Rather than trying to give a comprehensive overview of these packages, dive in and play with them using an example. 2.1 Getting Started Before we can get started, you'll need to install three packages: dplyr, ggplot2 and gapminder. To do this, you can either click on the \"Packages\" tab in RStudio or use the command install.packages() at the R console, e.g. install.packages(&#39;dplyr&#39;) install.packages(&#39;ggplot2&#39;) install.packages(&#39;gapminder&#39;) You only need to install a package once, but you need to load it every time you want to use it. To load a package, use the library() command, e.g. library(dplyr) library(ggplot2) library(gapminder) Now we're ready to go! I could tell you all about the data contained in gapminder, but an important part of this book is helping you to become self-sufficient. So instead I will leave this as an exercise for the reader! 2.1.1 Exercise After loading gapminder enter the command ?gapminder in the R console to view the R help file for this dataset. Read the documentation you find there to answer the following questions: How many rows and columns does gapminder contain? What information is contained in each row and column? What is the source of the data? 2.2 What is a tibble? The dplyr package uses a special operator called the pipe, written %&gt;% to chain together commands called verbs that act on objects called tibbles. This probably sounds complicated, but it's easier that it looks. Over the next few sections we'll slowly unpack the preceding sentence by looking at a number of simple examples. But first things first: what is a tibble? Let's see what happens if we display the gapminder dataset: gapminder ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows If you're used to working with dataframes in R, this may surprise you. Rather than trying to print all nrow(gapminder) rows on the screen, R helpfully shows us a useful summary of the information contained in gapminder. This is because gapminder is not a dataframe; it's a tibble, often abbreviated tbl. For our purposes, all you really need to know about tibbles is that they are souped up versions of R dataframes that are designed to work seamlessly with dplyr. To learn more see the chapter \"Tibbles\" in R for Data Science. 2.3 Filter Rows with filter We're ready to learn our first dplyr verb: filter selects rows. Here's an example: gapminder %&gt;% filter(year == 2007) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 2007 43.8 31889923 975. ## 2 Albania Europe 2007 76.4 3600523 5937. ## 3 Algeria Africa 2007 72.3 33333216 6223. ## 4 Angola Africa 2007 42.7 12420476 4797. ## 5 Argentina Americas 2007 75.3 40301927 12779. ## 6 Australia Oceania 2007 81.2 20434176 34435. ## 7 Austria Europe 2007 79.8 8199783 36126. ## 8 Bahrain Asia 2007 75.6 708573 29796. ## 9 Bangladesh Asia 2007 64.1 150448339 1391. ## 10 Belgium Europe 2007 79.4 10392226 33693. ## # … with 132 more rows Compare the results of running this command to what we got when we typed gapminder into the console above. Rather than displaying the whole dataset, now R is only showing us the 142 rows for which the column year has a value of 2007. So how does this work? The pipe operator %&gt;% \"pipes\" the tibble gapminder into the function filter(). The argument year == 2007 tells filter() that it should find all the rows such that the logical condition year == 2007 is TRUE. Oh no! Have we accidentally deleted all of the other rows of gapminder? No: we haven't made any changes to gapminder at all. If you don't believe me try entering gapminder at the console. All that this command does is display a subset of gapminder. If we wanted to store the result of running this command, we'd need to assign it to a variable, for example gapminder2007 &lt;- gapminder %&gt;% filter(year == 2007) gapminder2007 ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 2007 43.8 31889923 975. ## 2 Albania Europe 2007 76.4 3600523 5937. ## 3 Algeria Africa 2007 72.3 33333216 6223. ## 4 Angola Africa 2007 42.7 12420476 4797. ## 5 Argentina Americas 2007 75.3 40301927 12779. ## 6 Australia Oceania 2007 81.2 20434176 34435. ## 7 Austria Europe 2007 79.8 8199783 36126. ## 8 Bahrain Asia 2007 75.6 708573 29796. ## 9 Bangladesh Asia 2007 64.1 150448339 1391. ## 10 Belgium Europe 2007 79.4 10392226 33693. ## # … with 132 more rows We can also use filter to subset on two or more variables. For example, here we display data for the US in 2007: gapminder %&gt;% filter(year == 2007, country == &#39;United States&#39;) ## # A tibble: 1 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 United States Americas 2007 78.2 301139947 42952. Notice that I always put a linebreak after the pipe operator %&gt;% in my code examples. This isn't required to make the code run, but it's a good habit. As we start to write longer and more complicated dplyr commands, linebreaks will make it much easier to understand how our code works. 2.3.1 Exercises What is the difference between x = 3 and x == 3 in R? Show Solution The first assigns the value 3 to the variable x; the second tests whether x is equal to 3 and returns either TRUE or FALSE. Write code that uses filter to choose the subset of gapminder for which year is 2002. Show Solution gapminder %&gt;% filter(year == 2002) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 2002 42.1 25268405 727. ## 2 Albania Europe 2002 75.7 3508512 4604. ## 3 Algeria Africa 2002 71.0 31287142 5288. ## 4 Angola Africa 2002 41.0 10866106 2773. ## 5 Argentina Americas 2002 74.3 38331121 8798. ## 6 Australia Oceania 2002 80.4 19546792 30688. ## 7 Austria Europe 2002 79.0 8148312 32418. ## 8 Bahrain Asia 2002 74.8 656397 23404. ## 9 Bangladesh Asia 2002 62.0 135656790 1136. ## 10 Belgium Europe 2002 78.3 10311970 30486. ## # … with 132 more rows When I displayed data for the US in 2007, I put quotes around United States but not around year. Why? Show Solution This is because year contains numeric data while country contains character data, aka string data. If you instead try to choose the subset with year equal to 2005, something will go wrong. Try it. What happens and why? Show Solution If you go back to the help file for gapminder you'll see that it only contains data for every fifth year. The year 2005 isn't in our dataset so dplyr will display an empty tibble: gapminder %&gt;% filter(year == 2005) ## # A tibble: 0 × 6 ## # … with 6 variables: country &lt;fct&gt;, continent &lt;fct&gt;, year &lt;int&gt;, ## # lifeExp &lt;dbl&gt;, pop &lt;int&gt;, gdpPercap &lt;dbl&gt; Write code that stores the data for Asian countries in a tibble called gapminder_asia. Then display this tibble. Show Solution gapminder_asia &lt;- gapminder %&gt;% filter(continent == &#39;Asia&#39;) gapminder_asia ## # A tibble: 396 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 386 more rows Which country had the higher life expectancy in 1977: Ireland or Brazil? Which had the higher GDP per capita? Show Solution gapminder %&gt;% filter(year == 1977, country %in% c(&#39;Ireland&#39;, &#39;Brazil&#39;)) ## # A tibble: 2 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Brazil Americas 1977 61.5 114313951 6660. ## 2 Ireland Europe 1977 72.0 3271900 11151. 2.4 Sort data with arrange Suppose we wanted to sort gapminder by gdpPercap. To do this we can use the arrange command along with the pipe %&gt;% as follows: gapminder %&gt;% arrange(gdpPercap) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Congo, Dem. Rep. Africa 2002 45.0 55379852 241. ## 2 Congo, Dem. Rep. Africa 2007 46.5 64606759 278. ## 3 Lesotho Africa 1952 42.1 748747 299. ## 4 Guinea-Bissau Africa 1952 32.5 580653 300. ## 5 Congo, Dem. Rep. Africa 1997 42.6 47798986 312. ## 6 Eritrea Africa 1952 35.9 1438760 329. ## 7 Myanmar Asia 1952 36.3 20092996 331 ## 8 Lesotho Africa 1957 45.0 813338 336. ## 9 Burundi Africa 1952 39.0 2445618 339. ## 10 Eritrea Africa 1957 38.0 1542611 344. ## # … with 1,694 more rows The logic is very similar to what we saw above for filter. We pipe the tibble gapminder into the function arrange(). The argument gdpPercap tells arrange() that we want to sort by GDP per capita. Note that by default arrange() sorts in ascending order. If we want to sort in descending order, we use the function desc() as follows: gapminder %&gt;% arrange(desc(gdpPercap)) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Kuwait Asia 1957 58.0 212846 113523. ## 2 Kuwait Asia 1972 67.7 841934 109348. ## 3 Kuwait Asia 1952 55.6 160000 108382. ## 4 Kuwait Asia 1962 60.5 358266 95458. ## 5 Kuwait Asia 1967 64.6 575003 80895. ## 6 Kuwait Asia 1977 69.3 1140357 59265. ## 7 Norway Europe 2007 80.2 4627926 49357. ## 8 Kuwait Asia 2007 77.6 2505559 47307. ## 9 Singapore Asia 2007 80.0 4553009 47143. ## 10 Norway Europe 2002 79.0 4535591 44684. ## # … with 1,694 more rows 2.4.1 Exercises What is the lowest life expectancy in the gapminder dataset? Which country and year does it correspond to? Show Solution gapminder %&gt;% arrange(lifeExp) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda Africa 1992 23.6 7290203 737. ## 2 Afghanistan Asia 1952 28.8 8425333 779. ## 3 Gambia Africa 1952 30 284320 485. ## 4 Angola Africa 1952 30.0 4232095 3521. ## 5 Sierra Leone Africa 1952 30.3 2143249 880. ## 6 Afghanistan Asia 1957 30.3 9240934 821. ## 7 Cambodia Asia 1977 31.2 6978607 525. ## 8 Mozambique Africa 1952 31.3 6446316 469. ## 9 Sierra Leone Africa 1957 31.6 2295678 1004. ## 10 Burkina Faso Africa 1952 32.0 4469979 543. ## # … with 1,694 more rows What is the highest life expectancy in the gapminder dataset? Which country and year does it correspond to? Show Solution gapminder %&gt;% arrange(desc(lifeExp)) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. ## 3 Japan Asia 2002 82 127065841 28605. ## 4 Iceland Europe 2007 81.8 301931 36181. ## 5 Switzerland Europe 2007 81.7 7554661 37506. ## 6 Hong Kong, China Asia 2002 81.5 6762476 30209. ## 7 Australia Oceania 2007 81.2 20434176 34435. ## 8 Spain Europe 2007 80.9 40448191 28821. ## 9 Sweden Europe 2007 80.9 9031088 33860. ## 10 Israel Asia 2007 80.7 6426679 25523. ## # … with 1,694 more rows 2.5 Choose columns with select We use the select verb to choose a subset of columns. For example, to display only pop, country, and year, we would write gapminder %&gt;% select(pop, country, year) ## # A tibble: 1,704 × 3 ## pop country year ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 8425333 Afghanistan 1952 ## 2 9240934 Afghanistan 1957 ## 3 10267083 Afghanistan 1962 ## 4 11537966 Afghanistan 1967 ## 5 13079460 Afghanistan 1972 ## 6 14880372 Afghanistan 1977 ## 7 12881816 Afghanistan 1982 ## 8 13867957 Afghanistan 1987 ## 9 16317921 Afghanistan 1992 ## 10 22227415 Afghanistan 1997 ## # … with 1,694 more rows Now suppose that we wanted to select every column except pop. Here's one way to do it: gapminder %&gt;% select(country, continent, year, lifeExp, gdpPercap) ## # A tibble: 1,704 × 5 ## country continent year lifeExp gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 779. ## 2 Afghanistan Asia 1957 30.3 821. ## 3 Afghanistan Asia 1962 32.0 853. ## 4 Afghanistan Asia 1967 34.0 836. ## 5 Afghanistan Asia 1972 36.1 740. ## 6 Afghanistan Asia 1977 38.4 786. ## 7 Afghanistan Asia 1982 39.9 978. ## 8 Afghanistan Asia 1987 40.8 852. ## 9 Afghanistan Asia 1992 41.7 649. ## 10 Afghanistan Asia 1997 41.8 635. ## # … with 1,694 more rows but that takes a lot of typing! If there were more than a handful of columns in our tibble it would be very difficult to deselect a column in this way. Fortunately there's a shortcut: use the minus sign gapminder %&gt;% select(-pop) ## # A tibble: 1,704 × 5 ## country continent year lifeExp gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 779. ## 2 Afghanistan Asia 1957 30.3 821. ## 3 Afghanistan Asia 1962 32.0 853. ## 4 Afghanistan Asia 1967 34.0 836. ## 5 Afghanistan Asia 1972 36.1 740. ## 6 Afghanistan Asia 1977 38.4 786. ## 7 Afghanistan Asia 1982 39.9 978. ## 8 Afghanistan Asia 1987 40.8 852. ## 9 Afghanistan Asia 1992 41.7 649. ## 10 Afghanistan Asia 1997 41.8 635. ## # … with 1,694 more rows Just as we could when selecting, we can deselect multiple columns by separating their names with a comma: gapminder %&gt;% select(-pop, -year) ## # A tibble: 1,704 × 4 ## country continent lifeExp gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 28.8 779. ## 2 Afghanistan Asia 30.3 821. ## 3 Afghanistan Asia 32.0 853. ## 4 Afghanistan Asia 34.0 836. ## 5 Afghanistan Asia 36.1 740. ## 6 Afghanistan Asia 38.4 786. ## 7 Afghanistan Asia 39.9 978. ## 8 Afghanistan Asia 40.8 852. ## 9 Afghanistan Asia 41.7 649. ## 10 Afghanistan Asia 41.8 635. ## # … with 1,694 more rows It's easy to mix up the dplyr verbs select and filter. Here's a handy mnemonic: filteR filters Rows while seleCt selects Columns. Suppose we wanted to select only the column pop from gapminder. 2.5.1 Exercise Select only the columns year, lifeExp, and country in gapminder. Show Solution gapminder %&gt;% select(year, lifeExp, country) ## # A tibble: 1,704 × 3 ## year lifeExp country ## &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1952 28.8 Afghanistan ## 2 1957 30.3 Afghanistan ## 3 1962 32.0 Afghanistan ## 4 1967 34.0 Afghanistan ## 5 1972 36.1 Afghanistan ## 6 1977 38.4 Afghanistan ## 7 1982 39.9 Afghanistan ## 8 1987 40.8 Afghanistan ## 9 1992 41.7 Afghanistan ## 10 1997 41.8 Afghanistan ## # … with 1,694 more rows Select all the columns except year, lifeExp, and country in gapminder. Show Solution gapminder %&gt;% select(-year, -lifeExp, -country) ## # A tibble: 1,704 × 3 ## continent pop gdpPercap ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Asia 8425333 779. ## 2 Asia 9240934 821. ## 3 Asia 10267083 853. ## 4 Asia 11537966 836. ## 5 Asia 13079460 740. ## 6 Asia 14880372 786. ## 7 Asia 12881816 978. ## 8 Asia 13867957 852. ## 9 Asia 16317921 649. ## 10 Asia 22227415 635. ## # … with 1,694 more rows 2.6 The summarize verb Suppose we want to calculate the sample mean of the column lifeExp in gapminder. We can do this using the summarize verb as follows: gapminder %&gt;% summarize(mean_lifeExp = mean(lifeExp)) ## # A tibble: 1 × 1 ## mean_lifeExp ## &lt;dbl&gt; ## 1 59.5 Note the syntax: within summarize we have an assignment statement. In particular, we assign mean(lifeExp) to the variable mean_lifeExp. The key thing to know about summarize is that it always returns collapses a tibble with many rows into a single row. When we think about computing a sample mean, this makes sense: we want to summarize the column lifeExp as a single number. It doesn't actually make much sense to compute the mean of lifeExp because this involves averaging over different countries and different years. Instead let's compute the mean for a single year: 1952: gapminder %&gt;% filter(year == 1952) %&gt;% summarize(mean_lifeExp = mean(lifeExp)) ## # A tibble: 1 × 1 ## mean_lifeExp ## &lt;dbl&gt; ## 1 49.1 We can use summarize to compute multiple summary statistics for a single variable, the same summary statistic for multiple variables, or both: gapminder %&gt;% filter(year == 1952) %&gt;% summarize(mean_lifeExp = mean(lifeExp), sd_lifeExp = sd(lifeExp), mean_pop = mean(pop)) ## # A tibble: 1 × 3 ## mean_lifeExp sd_lifeExp mean_pop ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 49.1 12.2 16950402. Note that if we don't explicitly use an assignment statement, R will make up names for us based on the commands that we used: gapminder %&gt;% filter(year == 1952) %&gt;% summarize(mean(lifeExp), median(lifeExp), max(lifeExp)) ## # A tibble: 1 × 3 ## `mean(lifeExp)` `median(lifeExp)` `max(lifeExp)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 49.1 45.1 72.7 2.6.1 Exercise Use summarize to compute the 75th percentile of life expectancy in 1977. Show Solution gapminder %&gt;% filter(year == 1977) %&gt;% summarize(quantile(lifeExp, 0.75)) ## # A tibble: 1 × 1 ## `quantile(lifeExp, 0.75)` ## &lt;dbl&gt; ## 1 70.4 Use summarize to compute the 75th percentile of life expectancy among Asian countries in 1977. Show Solution gapminder %&gt;% filter(year == 1977, continent == &#39;Asia&#39;) %&gt;% summarize(quantile(lifeExp, 0.75)) ## # A tibble: 1 × 1 ## `quantile(lifeExp, 0.75)` ## &lt;dbl&gt; ## 1 65.9 2.7 The group_by verb The true power of summarize is its ability to compute grouped summary statistics in combination with another dplyr verb: group_by. In essence, group_by allows us to tell dplyr that we don't want to work with the whole dataset at once; rather we want to work with particular subsets or groups. The basic idea is similar to what we've done using filter in the past. For example, to calculate mean population (in millions) and mean life expectancy in the year 2007, we could use the following code: gapminder %&gt;% filter(year == 2007) %&gt;% summarize(meanPop = mean(pop) / 1000000, meanLifeExp = mean(lifeExp)) ## # A tibble: 1 × 2 ## meanPop meanLifeExp ## &lt;dbl&gt; &lt;dbl&gt; ## 1 44.0 67.0 Using group_by we could do the same thing for all years in the dataset at once: gapminder %&gt;% group_by(year) %&gt;% summarize(meanPop = mean(pop) / 1000000, meanLifeExp = mean(lifeExp)) ## # A tibble: 12 × 3 ## year meanPop meanLifeExp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 17.0 49.1 ## 2 1957 18.8 51.5 ## 3 1962 20.4 53.6 ## 4 1967 22.7 55.7 ## 5 1972 25.2 57.6 ## 6 1977 27.7 59.6 ## 7 1982 30.2 61.5 ## 8 1987 33.0 63.2 ## 9 1992 36.0 64.2 ## 10 1997 38.8 65.0 ## 11 2002 41.5 65.7 ## 12 2007 44.0 67.0 Notice what has changed in the second code block: we replaced filter(year == 2007) with group_by(year). This tells dplyr that, rather than simply restricting attention to data from 2007, we want to form subsets (groups) of the dataset that correspond to the values of the year variable. Whatever comes after group_by will then be calculated for these subsets. Here's another example. Suppose we wanted to calculate mean life expectancy and total population in each continent during the year 2007. To accomplish this, we can chain together the filter, group_by and summarize verbs as follows: gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarize(meanPop = mean(pop) / 1000000, meanLifeExp = mean(lifeExp)) ## # A tibble: 5 × 3 ## continent meanPop meanLifeExp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 17.9 54.8 ## 2 Americas 36.0 73.6 ## 3 Asia 116. 70.7 ## 4 Europe 19.5 77.6 ## 5 Oceania 12.3 80.7 We can also use group_by to subset over multiple variables at once. For example, to calculate mean life expectancy and total population in each continent separately for every year, we can use the following code: gapminder %&gt;% group_by(year, continent) %&gt;% summarize(meanPop = mean(pop) / 1000000, meanLifeExp = mean(lifeExp)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. ## # A tibble: 60 × 4 ## # Groups: year [12] ## year continent meanPop meanLifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 Africa 4.57 39.1 ## 2 1952 Americas 13.8 53.3 ## 3 1952 Asia 42.3 46.3 ## 4 1952 Europe 13.9 64.4 ## 5 1952 Oceania 5.34 69.3 ## 6 1957 Africa 5.09 41.3 ## 7 1957 Americas 15.5 56.0 ## 8 1957 Asia 47.4 49.3 ## 9 1957 Europe 14.6 66.7 ## 10 1957 Oceania 5.97 70.3 ## # … with 50 more rows 2.7.1 Exercise Why doesn't the following code work as expected? gapminder %&gt;% summarize(meanLifeExp = mean(lifeExp)) %&gt;% group_by(year) Show Solution The steps are carried out in the wrong order: we need to form groups first and then calculate our desired summaries. Calculate the median GDP per capita in each continent in 1977. Show Solution gapminder %&gt;% filter(year == 1977) %&gt;% group_by(continent) %&gt;% summarize(medGDPc = median(gdpPercap)) ## # A tibble: 5 × 2 ## continent medGDPc ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 1400. ## 2 Americas 6281. ## 3 Asia 3195. ## 4 Europe 14226. ## 5 Oceania 17284. Repeat 2. but sort your results in descending order. Show Solution gapminder %&gt;% filter(year == 1977) %&gt;% group_by(continent) %&gt;% summarize(medGDPc = median(gdpPercap)) %&gt;% arrange(desc(medGDPc)) ## # A tibble: 5 × 2 ## continent medGDPc ## &lt;fct&gt; &lt;dbl&gt; ## 1 Oceania 17284. ## 2 Europe 14226. ## 3 Americas 6281. ## 4 Asia 3195. ## 5 Africa 1400. Calculate the mean and standard deviation of life expectancy for separately for each continent in every year after 1977. Sort your results in ascending order by the standard deviation of life expectancy. Show Solution Show Solution gapminder %&gt;% filter(year &gt; 1977) %&gt;% group_by(continent, year) %&gt;% summarize(meanGDPc = mean(gdpPercap), sdGDPc = sd(gdpPercap)) %&gt;% arrange(sdGDPc) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the ## `.groups` argument. ## # A tibble: 30 × 4 ## # Groups: continent [5] ## continent year meanGDPc sdGDPc ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Oceania 1982 18555. 1304. ## 2 Oceania 1987 20448. 2038. ## 3 Africa 1987 2283. 2567. ## 4 Africa 1992 2282. 2644. ## 5 Africa 1997 2379. 2821. ## 6 Africa 2002 2599. 2973. ## 7 Africa 1982 2482. 3243. ## 8 Oceania 1992 20894. 3579. ## 9 Africa 2007 3089. 3618. ## 10 Oceania 1997 24024. 4206. ## # … with 20 more rows 2.8 Understanding the pipe: %&gt;% Let's revisit the pipe, %&gt;%, that we've used in the code examples above. I told you that the command gapminder %&gt;% filter(year == 2007) \"pipes\" the tibble gapminder into the function filter(). But what exactly does this mean? Take a look at the R help file for the dplyr function filter. We see that filter() takes something called .data as its first argument. Moving on to the \"Arguments\" section of the help file, we see that .data is \"A tbl\" i.e. a tibble. To better understand what this means, let's try using filter without the pipe: filter(gapminder, year == 2007, country == &#39;United States&#39;) ## # A tibble: 1 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 United States Americas 2007 78.2 301139947 42952. Notice that this gives us exactly the same result as gapminder %&gt;% filter(year == 2007, country == &#39;United States&#39;) ## # A tibble: 1 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 United States Americas 2007 78.2 301139947 42952. In other words The pipe is gives us an alternative way of supplying the first argument to a function. Let's try this with a more familiar R function: mean. The first argument of mean is a vector x. So let's try using the pipe to compute the mean of some data: x &lt;- c(1, 5, 2, 7, 2) x %&gt;% mean ## [1] 3.4 The pipe supplies a function with its first argument. If we want to specify additional arguments, we need to do so within the function call itself. For example, here's how we could use the pipe to compute the mean after dropping missing observations: y &lt;- c(1, 5, NA, 7, 2) y %&gt;% mean(na.rm = TRUE) ## [1] 3.75 One important note about the pipe: it's not a base R command. Instead it's a command provided by the package Magrittr. (If you're familiar with the Belgian painter Magritte, you may realize that the name of this package is quite witty!) This package is installed automatically along with dplyr. 2.8.1 Exercise Write R code that uses the pipe to calculate the sample variance of z &lt;- c(4, 1, 5, NA, 3) excluding the missing observation from the calculation. Show Solution z &lt;- c(4, 1, 5, NA, 3) z %&gt;% var(na.rm = TRUE) ## [1] 2.916667 Re-write the code from your solution to Exercise #4 without using the pipe. Show Solution arrange(gapminder,lifeExp) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda Africa 1992 23.6 7290203 737. ## 2 Afghanistan Asia 1952 28.8 8425333 779. ## 3 Gambia Africa 1952 30 284320 485. ## 4 Angola Africa 1952 30.0 4232095 3521. ## 5 Sierra Leone Africa 1952 30.3 2143249 880. ## 6 Afghanistan Asia 1957 30.3 9240934 821. ## 7 Cambodia Asia 1977 31.2 6978607 525. ## 8 Mozambique Africa 1952 31.3 6446316 469. ## 9 Sierra Leone Africa 1957 31.6 2295678 1004. ## 10 Burkina Faso Africa 1952 32.0 4469979 543. ## # … with 1,694 more rows arrange(gapminder, desc(lifeExp)) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. ## 3 Japan Asia 2002 82 127065841 28605. ## 4 Iceland Europe 2007 81.8 301931 36181. ## 5 Switzerland Europe 2007 81.7 7554661 37506. ## 6 Hong Kong, China Asia 2002 81.5 6762476 30209. ## 7 Australia Oceania 2007 81.2 20434176 34435. ## 8 Spain Europe 2007 80.9 40448191 28821. ## 9 Sweden Europe 2007 80.9 9031088 33860. ## 10 Israel Asia 2007 80.7 6426679 25523. ## # … with 1,694 more rows 2.9 Chaining commands In the examples we've looked at so far, the pipe doesn't seem all that useful: it's just an alternative way of specifying the first argument to a function. The true power and convenience of the pipe only becomes apparent we need to chain a series of commands together. For example, suppose we wanted to display the 1952 data from gapminder sorted by gdpPercap in descending order. Using the pipe, this is easy: gapminder %&gt;% filter(year == 1952) %&gt;% arrange(desc(gdpPercap)) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Kuwait Asia 1952 55.6 160000 108382. ## 2 Switzerland Europe 1952 69.6 4815000 14734. ## 3 United States Americas 1952 68.4 157553000 13990. ## 4 Canada Americas 1952 68.8 14785584 11367. ## 5 New Zealand Oceania 1952 69.4 1994794 10557. ## 6 Norway Europe 1952 72.7 3327728 10095. ## 7 Australia Oceania 1952 69.1 8691212 10040. ## 8 United Kingdom Europe 1952 69.2 50430000 9980. ## 9 Bahrain Asia 1952 50.9 120447 9867. ## 10 Denmark Europe 1952 70.8 4334000 9692. ## # … with 132 more rows Notice how I split the commands across multiple lines. This is good practice: it makes your code much easier to read. So what's happening when we chain commands in this way? The first step in the chain gapminder %&gt;% filter(year == 1952) returns a tibble: the subset of gapminder for which year is 1952. The next step %&gt;% arrange(gdpPercap) pipes this new tibble into the function arrange(), giving us the desired result. I hope you agree with me that this is pretty intuitive: even if we didn't know anything about dplyr we could almost figure out what this code is supposed to do. In stark contrast, let's look at the code we'd have to use if we wanted to accomplish the same task without using the pipe: arrange(filter(gapminder, year == 1952), desc(gdpPercap)) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Kuwait Asia 1952 55.6 160000 108382. ## 2 Switzerland Europe 1952 69.6 4815000 14734. ## 3 United States Americas 1952 68.4 157553000 13990. ## 4 Canada Americas 1952 68.8 14785584 11367. ## 5 New Zealand Oceania 1952 69.4 1994794 10557. ## 6 Norway Europe 1952 72.7 3327728 10095. ## 7 Australia Oceania 1952 69.1 8691212 10040. ## 8 United Kingdom Europe 1952 69.2 50430000 9980. ## 9 Bahrain Asia 1952 50.9 120447 9867. ## 10 Denmark Europe 1952 70.8 4334000 9692. ## # … with 132 more rows There are may reasons why this code is harder to read, but the most important one is that the commands arrange and filter have to appear in the code in the opposite of the order in which they are actually being carried out. This is because parentheses are evaluated from inside to outside. This is what's great about the pipe: it lets us write our code in a way that accords with the actual order of the steps we want to carry out. 2.9.1 Exercise What was the most populous European country in 1992? Write appropriate dplyr code using the pipe to display the information you need to answer this question. Show Solution gapminder %&gt;% filter(year == 1992, continent == &#39;Europe&#39;) %&gt;% arrange(desc(pop)) ## # A tibble: 30 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Germany Europe 1992 76.1 80597764 26505. ## 2 Turkey Europe 1992 66.1 58179144 5678. ## 3 United Kingdom Europe 1992 76.4 57866349 22705. ## 4 France Europe 1992 77.5 57374179 24704. ## 5 Italy Europe 1992 77.4 56840847 22014. ## 6 Spain Europe 1992 77.6 39549438 18603. ## 7 Poland Europe 1992 71.0 38370697 7739. ## 8 Romania Europe 1992 69.4 22797027 6598. ## 9 Netherlands Europe 1992 77.4 15174244 26791. ## 10 Hungary Europe 1992 69.2 10348684 10536. ## # … with 20 more rows Re-write your code from part 1. without using the pipe. Show Solution arrange(filter(gapminder, year == 1992, continent == &#39;Europe&#39;), desc(pop)) ## # A tibble: 30 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Germany Europe 1992 76.1 80597764 26505. ## 2 Turkey Europe 1992 66.1 58179144 5678. ## 3 United Kingdom Europe 1992 76.4 57866349 22705. ## 4 France Europe 1992 77.5 57374179 24704. ## 5 Italy Europe 1992 77.4 56840847 22014. ## 6 Spain Europe 1992 77.6 39549438 18603. ## 7 Poland Europe 1992 71.0 38370697 7739. ## 8 Romania Europe 1992 69.4 22797027 6598. ## 9 Netherlands Europe 1992 77.4 15174244 26791. ## 10 Hungary Europe 1992 69.2 10348684 10536. ## # … with 20 more rows 2.10 Change an existing variable or create a new one with mutate It's a little hard to read the column pop in gapminder since there are so many digits. Suppose that, instead of raw population, we wanted to display population in millions. This requires us to pop by 1000000, which we can do using the function mutate() from dplyr as follows: gapminder %&gt;% mutate(pop = pop / 1000000) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8.43 779. ## 2 Afghanistan Asia 1957 30.3 9.24 821. ## 3 Afghanistan Asia 1962 32.0 10.3 853. ## 4 Afghanistan Asia 1967 34.0 11.5 836. ## 5 Afghanistan Asia 1972 36.1 13.1 740. ## 6 Afghanistan Asia 1977 38.4 14.9 786. ## 7 Afghanistan Asia 1982 39.9 12.9 978. ## 8 Afghanistan Asia 1987 40.8 13.9 852. ## 9 Afghanistan Asia 1992 41.7 16.3 649. ## 10 Afghanistan Asia 1997 41.8 22.2 635. ## # … with 1,694 more rows Note the syntax here: within mutate() we have an assignment statement, namely pop = pop / 1000000. This tells R to calculate pop / 1000000 and assign the result to pop, in place of the original variable. We can also use mutate() to create a new variable. The gapminder dataset doesn't contain overall GDP, only GDP per capita. To calculate GDP, we need to multiply gdpPercap by pop. But wait! Didn't we just change pop so it's expressed in millions? No: we never stored the results of our previous command, we simply displayed them. Just as I discussed above, unless you overwrite it, the original gapminder dataset will be unchanged. With this in mind, we can create the gdp variable as follows: gapminder %&gt;% mutate(gdp = pop * gdpPercap) ## # A tibble: 1,704 × 7 ## country continent year lifeExp pop gdpPercap gdp ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. ## # … with 1,694 more rows 2.10.1 Exercise Explain why we used = rather than == in the mutate() examples above. Show Solution We used = because we're carrying out an assignment operation. In contrast == tests for equality, returning TRUE or FALSE. Which country in the Americas had the shortest life expectancy in months in the year 1962? Write appropriate dplyr code using the pipe to display the information you need to answer this question. Show Solution gapminder %&gt;% mutate(lifeExpMonths = 12 * lifeExp) %&gt;% filter(year == 1962, continent == &#39;Americas&#39;) %&gt;% arrange(lifeExpMonths) ## # A tibble: 25 × 7 ## country continent year lifeExp pop gdpPercap lifeExpMonths ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bolivia Americas 1962 43.4 3593918 2181. 521. ## 2 Haiti Americas 1962 43.6 3880130 1797. 523. ## 3 Guatemala Americas 1962 47.0 4208858 2750. 563. ## 4 Honduras Americas 1962 48.0 2090162 2291. 576. ## 5 Nicaragua Americas 1962 48.6 1590597 3634. 584. ## 6 Peru Americas 1962 49.1 10516500 4957. 589. ## 7 El Salvador Americas 1962 52.3 2747687 3777. 628. ## 8 Dominican Republic Americas 1962 53.5 3453434 1662. 642. ## 9 Ecuador Americas 1962 54.6 4681707 4086. 656. ## 10 Brazil Americas 1962 55.7 76039390 3337. 668. ## # … with 15 more rows "],["mind-the-gap-part-ii.html", "Lesson 3 Mind the Gap Part II 3.1 A simple scatterplot using ggplot2 3.2 Plotting on the log scale 3.3 The color and size aesthetics 3.4 Faceting - Plotting multiple subsets at once 3.5 Plotting summarized data 3.6 Line plots 3.7 Bar plots 3.8 Histograms 3.9 Boxplots", " Lesson 3 Mind the Gap Part II Some kind of intro? Or should this be a separate lesson? I think I should split this off into a separate lesson. Add some extra dplyr stuff above, taken from the Econ 224 problem sets. 3.1 A simple scatterplot using ggplot2 Now that we know the basics of dplyr, we'll turn our attention to graphics. R has many powerful build-in graphics functions that may be familiar to you from Econ 103. In this class, however, we'll use a very powerful package for statistical visualization called ggplot2. There's nothing more for you to instead or load, since ggplot2 is included in the tidyverse package, which you've already installed and loaded. For more details on ggplot2 see the chapter entitled \"Data Visualisation\" in R for Data Science. We'll start off by constructing a subset of the gapminder dataset that contains information from the year 2007 that we'll use for our plots below. gapminder_2007 &lt;- gapminder %&gt;% filter(year == 2007) It takes some time to grow accustomed to ggplot2 syntax, so rather than giving you a lot of detail, we're going to look at a series of increasingly more complicated examples. Our first example will be a simple scatterplot using gapminder_2007. Each point will correspond to a single country in 2007. Its x-coordinate will be GDP per capita and its y-coordinate will be life expectancy. Here's the code: ggplot(gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp)) We see that GDP per capita is a very strong predictor of life expectancy, although the relationship is non-linear. Notice how I add a linebreak after the +. This is analogous to how I always add a linebreak after the pipe %&gt;%. While it isn't necessary for the code to run correctly, it improves readability. 3.1.1 Exercise Using my code example as a template, make a scatterplot with pop on the x-axis and lifeExp on the y-axis using gapminder_2007. Does there appear to be a relationship between population and life expectancy? Show Solution There is no clear relationship between population and life expectancy based on the 2007 data: ggplot(gapminder_2007) + geom_point(mapping = aes(x = pop, y = lifeExp)) Repeat 1. with gdpPercap on the y-axis. Show Solution There is no clear relationship between population and GDP per capita based on the 2007 data: ggplot(gapminder_2007) + geom_point(mapping = aes(x = pop, y = gdpPercap)) 3.2 Plotting on the log scale It's fairly common to transform data onto a log scale before carrying out further analysis or plotting. If you've taken Econ 104, you may already be familiar with log transformations. If not, don't worry about it: we'll discuss them later in the course. For now, we'll content ourselves with learning how to transform the axes in a ggplot to the log base 10 scale. To transform the x-axis, it's as easy as adding a + scale_x_log10() to the end of our command from above: ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp)) + scale_x_log10() Notice how I split the code across multiple lines and ended each of the intermediate lines with the +. This makes things much easier to read. 3.2.1 Exercise Using my code example as a template, make a scatterplot with the log base 10 of pop on the x-axis and lifeExp on the y-axis using the gapminder_2007 dataset. Show Solution ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = pop, y = lifeExp)) + scale_x_log10() Suppose that rather than putting the x-axis on the log scale, we wanted to put the y-axis on the log scale. Figure out how to do this, either by clever guesswork or a google search, and then redo my example with gdpPercap and lifeExp with gdpPercap in levels and lifeExp in logs. Show Solution ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp)) + scale_y_log10() Repeat 2. but with both axes on the log scale. Show Solution ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp)) + scale_x_log10() + scale_y_log10() 3.3 The color and size aesthetics It's time to start unraveling the somewhat mysterious-looking syntax of ggplot. To make a graph using ggplot we use the following template: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) replacing &lt;DATA&gt;, &lt;GEOM_FUNCTION&gt;, and &lt;MAPPINGS&gt; to specify what we want to plot and how it should appear. The first part is easy: we replace &lt;DATA&gt; with the dataset we want to plot, for example gapminder_2007 in the example from above. The second part is also fairly straightforward: we replace &lt;GEOM_FUNCTION&gt; with the name of a function that specifies the kind of plot we want to make. So far we've only seen one example: geom_point() which tells ggplot that we want to make a scatterplot. We'll see more examples in a future lab. For now, I want to focus on the somewhat more complicated-looking mapping = aes(&lt;MAPPINGS&gt;). The abbreviation aes is short for aesthetic and the code mapping = aes(&lt;MAPPINGS&gt;) defines what is called an aesthetic mapping. This is just a fancy way of saying that it tells R how we want our plot to look. The information we need to put in place of &lt;MAPPINGS&gt; depends on what kind of plot we're making. Thus far we've only examined geom_point() which produces a scatterplot. For this kind of plot, the minimum information we need to provide is the location of each point. For example, in our example above we wrote aes(x = gdpPercap, y = lifeExp) to tell R that gdpPercap gives the x-axis location of each point, and lifeExp gives the y-axis location. When making a scatterplot with geom_point we are not limited to specifying the x and y coordinates of each point; we can also specify the size and color of each point. This gives us a useful way of displaying more than two variables in a two-dimensional plot. We do this using aes. For example, let's use the color of each point to indicate continent ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + scale_x_log10() Notice how ggplot automatically generates a helpful legend. This plot makes it easy to see at a glance that the European countries in 2007 ten to have high GDP per capita and high life expectancy, while the African countries have the opposite. We can also use the size of each point to encode information, e.g. population: ggplot(data = gapminder_2007) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() 3.3.1 Exercise Would it make sense to set size = continent? What about setting col = pop? Explain briefly. Show Solution Neither of these makes sense since continent is categorical and pop is continuous: color is useful for categorical variables and size for continuous ones. The following code is slightly different from what I've written above. What is different. Try running it. What happens? Explain briefly. ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp)) + scale_x_log10() Show Solution It still works! You don't have to explicitly write data or mapping when using ggplot. I only included these above for clarity. In the future I'll leave them out to make my code more succinct. ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp)) + scale_x_log10() Create a tibble called gapminder_1952 that contains data from gapminder from 1952. Show Solution gapminder_1952 &lt;- gapminder %&gt;% filter(year == 1952) Use gapminder_1952 from the previous part to create a scatter plot with population on the x-axis, life expectancy on the y-axis, and continent represented by the color of the points. Plot population on the log scale (base 10). Show Solution ggplot(gapminder_1952) + geom_point(aes(x = pop, y = lifeExp, color = continent)) + scale_x_log10() Suppose that instead of indicating continent using color, you wanted all the points in the plot from 3. to be blue. Consult the chapter \"Visualising Data\" from R for Data Science to find out how to do this. Show Solution When you want color to be a variable from your dataset, put color = &lt;VARIABLE&gt; inside of aes; when you simply want to set the colors of all the points, put color = '&lt;COLOR&gt;' outside of aes, for example ggplot(gapminder_1952) + geom_point(aes(x = pop, y = lifeExp), color = &#39;blue&#39;) + scale_x_log10() 3.4 Faceting - Plotting multiple subsets at once Let's pick up where we left off in lab #1, with a plot of GDP per capita and life expectancy in 2007: gapminder_2007 &lt;- gapminder %&gt;% filter(year == 2007) ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() This is an easy way to make a plot for a single year. But what if you wanted to make the same plot for every year in the gapminder dataset? It would take a lot of copying-and-pasting of the preceding code chunk to accomplish this. Fortunately there's a much easier way: faceting. In ggplot2 a facet is a subplot that corresponds to a subset of your dataset, for example the year 2007. We'll now use faceting to reproduce the plot from above for all the years in gapminder simultaneously: ggplot(gapminder) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() + facet_wrap(~ year) Note the syntax here: in a similar way to how we added scale_x_log10() to plot on the log scale, we add facet_wrap(~ year) to facet by year. The tilde ~ is important: this has to precede the variable by which you want to facet. Now that we understand how to produce it, let's take a closer look at this plot. Notice how this plot allows us to visualize five variables simultaneously. By looking at how the plots change over time, we see a pattern of increasing GDP per capita and life expectancy throughout the world between 1952 and 2007. Notice in particular the dramatic improvements in both variables in the Asian economies. 3.4.1 Exercise What would happen if I were to run the following code? Explain briefly. ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() + facet_wrap(~ year) Show Solution We'll only get one facet since the tibble gapminder_2007 only has data for 2007: ggplot(gapminder_2007) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + scale_x_log10() + facet_wrap(~ year) Make a scatterplot with data from gapminder for the year 1977. Your plot should be faceted by continent with GDP per capita on the log scale on the x-axis, life expectancy on the y-axis, and population indicated by the size of each point. Show Solution gapminder_1977 &lt;- gapminder %&gt;% filter(year == 1977) ggplot(gapminder_1977) + geom_point(aes(x = gdpPercap, y = lifeExp, size = pop)) + scale_x_log10() + facet_wrap(~ continent) What would happen if you tried to facet by pop? Explain briefly. Show Solution You'll get something crazy if you try this. Population is continuous rather than categorical so every country has a different value for this variable. You'll end up with one plot for every country, containing a single point: # Not run: it takes a long time and looks nasty! gapminder_1977 &lt;- gapminder %&gt;% filter(year == 1977) ggplot(gapminder_1977) + geom_point(aes(x = gdpPercap, y = lifeExp, color = continent)) + scale_x_log10() + facet_wrap(~ pop) 3.5 Plotting summarized data By combining summarize and group_by with ggplot, it's easy to make plots of grouped data. For example, here's how we could plot total world population in millions from 1952 to 2007. First we construct a tibble which I'll name by_year containing the desired summary statistic grouped by year and display it: by_year &lt;- gapminder %&gt;% mutate(popMil = pop / 1000000) %&gt;% group_by(year) %&gt;% summarize(totalpopMil = sum(popMil)) by_year ## # A tibble: 12 × 2 ## year totalpopMil ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 2407. ## 2 1957 2664. ## 3 1962 2900. ## 4 1967 3217. ## 5 1972 3577. ## 6 1977 3930. ## 7 1982 4289. ## 8 1987 4691. ## 9 1992 5111. ## 10 1997 5515. ## 11 2002 5887. ## 12 2007 6251. Then we make a scatterplot using ggplot: ggplot(by_year) + geom_point(aes(x = year, y = totalpopMil)) Here's a more complicated example where we additionally use color to plot each continent separately: by_year_continent &lt;- gapminder %&gt;% mutate(popMil = pop / 1000000) %&gt;% group_by(year, continent) %&gt;% summarize(totalpopMil = sum(popMil)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. by_year ## # A tibble: 12 × 2 ## year totalpopMil ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 2407. ## 2 1957 2664. ## 3 1962 2900. ## 4 1967 3217. ## 5 1972 3577. ## 6 1977 3930. ## 7 1982 4289. ## 8 1987 4691. ## 9 1992 5111. ## 10 1997 5515. ## 11 2002 5887. ## 12 2007 6251. ggplot(by_year_continent) + geom_point(aes(x = year, y = totalpopMil, color = continent)) Make sure you understand how the preceding example works before attempting the exercise. 3.5.1 Exercise What happens if you append + expand_limits(y = 0) to the preceding ggplot code? Why might this be helpful in some cases? Show Solution The function expand_limits() lets us tweak the limits of our x or y-axis in a ggplot. In this particular example expand_limits(y = 0) ensures that the y-axis begins at zero. Without using this command, ggplot will choose the y-axis on its own so that there is no \"empty space\" in the plot. Sometimes we may want to override this behavior. Make a scatter with average GDP per capita across all countries in gapminder in the y-axis and year on the x-axis. Show Solution by_year &lt;- gapminder %&gt;% group_by(year) %&gt;% summarize(meanGDPc = mean(gdpPercap)) ggplot(by_year) + geom_point(aes(x = year, y = meanGDPc)) Repeat 2. broken down by continent, using color to distinguish the points. Put mean GDP per capita on the log scale. Show Solution by_year &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarize(meanGDPc = mean(gdpPercap)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. ggplot(by_year) + geom_point(aes(x = year, y = meanGDPc, color = continent)) + scale_y_log10() 3.6 Line plots Thus far we've only learned how to make one kind of plot with ggplot: a scatterplot, which we constructed using geom_scatter(). Sometimes we want to connect the dots in a scatterplot, for example when we're interested in visualizing a trend over time. The resulting plot is called a line plot. To make one, simply replace geom_scatter() with geom_line(). For example: by_year_continent &lt;- gapminder %&gt;% mutate(popMil = pop / 1000000) %&gt;% group_by(year, continent) %&gt;% summarize(totalpopMil = sum(popMil)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. by_year ## # A tibble: 60 × 3 ## # Groups: year [12] ## year continent meanGDPc ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Africa 1253. ## 2 1952 Americas 4079. ## 3 1952 Asia 5195. ## 4 1952 Europe 5661. ## 5 1952 Oceania 10298. ## 6 1957 Africa 1385. ## 7 1957 Americas 4616. ## 8 1957 Asia 5788. ## 9 1957 Europe 6963. ## 10 1957 Oceania 11599. ## # … with 50 more rows ggplot(by_year_continent) + geom_line(aes(x = year, y = totalpopMil, color = continent)) 3.6.1 Exercise Repeat exercise 5-3 with a line plot rather than a scatterplot. Show Solution by_year &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarize(meanGDPc = mean(gdpPercap)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. ggplot(by_year) + geom_line(aes(x = year, y = meanGDPc, color = continent)) + scale_y_log10() 3.7 Bar plots To make a bar plot, we use geom_col(). Note that the x argument of aes needs to be a categorical variable for a bar plot to make sense. Here's a simple example: by_continent &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarize(meanLifeExp = mean(lifeExp)) ggplot(by_continent) + geom_col(aes(x = continent, y = meanLifeExp)) Sometimes we want to turn a bar plot, or some other kind of plot, on its side. This can be particularly helpful if the x-axis labels are very long. To do this, simply add + coord_flip() to your ggplot command, for example: ggplot(by_continent) + geom_col(aes(x = continent, y = meanLifeExp)) + coord_flip() 3.7.1 Exercise Make a collection of bar plots faceted by year that compare mean GDP per capita across countries in a given year. Orient your plots so it's easy to read the continent labels. Show Solution by_continent &lt;- gapminder %&gt;% group_by(continent) %&gt;% summarize(meanGDPc = mean(gdpPercap)) ggplot(by_year) + geom_col(aes(x = continent, y = meanGDPc)) + facet_wrap(~ year) + coord_flip() 3.8 Histograms To make a ggplot2 histogram, we use the function geom_histogram(). Recall from Econ 103 that a histogram summarizes a single variable at a time by forming non-overlapping bins of equal width and calculating the fraction of observations in each bin. If we choose a different width for the bins, we'll get a different histogram. Here's an example of two different bin widths: gapminder_2007 &lt;- gapminder %&gt;% filter(year == 2007) ggplot(gapminder_2007) + geom_histogram(aes(x = lifeExp), binwidth = 5) ggplot(gapminder_2007) + geom_histogram(aes(x = lifeExp), binwidth = 1) 3.8.1 Exercise All of the examples we've seen that use ggplot besides histograms have involved specifying both x and y within aes(). Why are histograms different? Show Solution This is because histograms only depict a single variable while the other plots we've made show two variables at once. What happens if you don't specify a bin width in either of my two examples? Show Solution If you don't specify a bin width, ggplot2 will pick one for you and possibly give you a warning suggesting that you pick a better bin width manually. Make a histogram of GDP per capita in 1977. Play around with different bin widths until you find one that gives a good summary of the data. Show Solution There's no obvious right answer for the bin width, but here's one possibility: gapminder1977 &lt;- gapminder %&gt;% filter(year == 1977) ggplot(gapminder_1977) + geom_histogram(aes(x = gdpPercap), binwidth = 5000) Repeat 3. but put GDP per capita on the log scale. Show Solution You'll need a much smaller bin width when using the log scale, for example: ggplot(gapminder_1977) + geom_histogram(aes(x = gdpPercap), binwidth = 0.2) + scale_x_log10() Compare and contrast the two different histograms you've made. Show Solution No right answer: it's a discussion question! But the idea is to see how taking logs gets rid of the huge positive skewness in GDP per capita. 3.9 Boxplots The final kind of ggplot we'll learn how to produce is a boxplot, a visualization of the five-number summary of a variable: minimum, 25th percentile, median, 75th percentile, and maximum. To make a boxplot in ggplot we use the function geom_boxplot(), for example: ggplot(gapminder_2007) + geom_boxplot(aes(x = continent, y = lifeExp)) Compared to histograms, boxplots provide less detail but allow us to easily compare across groups. 3.9.1 Exercise What is the meaning of the little \"dots\" that appear in the boxplot above? Use a Google search to find out what they are and how they are computed. Show Solution They are outliers: ggplot considers any observation that is more than 1.5 times the interquartile range away from the \"box\" to be an outlier, and adds a point to indicate it. Use faceting to construct a collection of boxplots, each of which compares log GDP per capita across continents in a given year. Turn your boxplots sideways to make it easier to read the continent labels. Show Solution ggplot(gapminder) + geom_boxplot(aes(x = continent, y = gdpPercap)) + facet_wrap(~ year) + scale_y_log10() + coord_flip() + ggtitle(&#39;GDP per Capita by Continent: 1952-2007&#39;) Use a Google search to find out how to add a title to a ggplot. Use it to add a title to the plot you created in 2. Show Solution Use ggtitle('YOUR TITLE HERE') as I did in my solution to 2. above. "],["predictive-regression-part-i.html", "Lesson 4 Predictive Regression Part I 4.1 Introduction 4.2 The Least Squares Problem 4.3 Linear Regression with lm() 4.4 Getting More from lm() 4.5 Summarizing The Ouput of lm() 4.6 Using broom() to get what you need from lm() 4.7 Dummy Variables with lm() 4.8 Fun with R Formulas 4.9 Exercises", " Lesson 4 Predictive Regression Part I 4.1 Introduction This is the first of two lessons that will teach you how to implement and interpret predictive linear regression in R. For the moment we won't worry about causality and we won't talk about heteroskedasticity or autocorrelation. In this first lesson, we'll introduce the basics using a simple dataset that you can download from my website and display as follows: library(readr) kids &lt;- read_csv(&quot;http://ditraglia.com/econ103/child_test_data.csv&quot;) kids ## # A tibble: 434 × 4 ## kid.score mom.hs mom.iq mom.age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 65 1 121. 27 ## 2 98 1 89.4 25 ## 3 85 1 115. 27 ## 4 83 1 99.4 25 ## 5 115 1 92.7 27 ## 6 98 0 108. 18 ## 7 69 1 139. 20 ## 8 106 1 125. 23 ## 9 102 1 81.6 24 ## 10 95 1 95.1 19 ## # … with 424 more rows Each row of the tibble kids contains information on a three-year old child. The first column gives the child's test score at age three, while the remaining columns provide information about each child's mother: kid.score child test score at age 3 mom.age age of mother at birth of child mom.hs mother completed high school? (1 = yes) mom.iq mother's IQ score The columns kid.score gives the child's test score at age three. The remaining columns describe the child's mother: mom.age is mother's age at the birth of the child, mom.hs is a dummy variable that equals one the mother completed high school, and mom.iq is the mother's IQ score. Our main goal will be to predict a child's test score based on mother characteristics. But stay alert: in some of the exercises I may be a bit devious and ask you to predict something else! Exercise Rename the columns to replace each instance of a \".\" with an underscore: \"_\". Summary stats for the columns. library(dplyr) kids &lt;- kids %&gt;% rename(kid_score = kid.score, mom_hs = mom.hs, mom_iq = mom.iq, mom_age = mom.age) summary(kids) ## kid_score mom_hs mom_iq mom_age ## Min. : 20.0 Min. :0.0000 Min. : 71.04 Min. :17.00 ## 1st Qu.: 74.0 1st Qu.:1.0000 1st Qu.: 88.66 1st Qu.:21.00 ## Median : 90.0 Median :1.0000 Median : 97.92 Median :23.00 ## Mean : 86.8 Mean :0.7857 Mean :100.00 Mean :22.79 ## 3rd Qu.:102.0 3rd Qu.:1.0000 3rd Qu.:110.27 3rd Qu.:25.00 ## Max. :144.0 Max. :1.0000 Max. :138.89 Max. :29.00 4.2 The Least Squares Problem Suppose we observe a dataset with \\(n\\) observations \\((Y_i, X_i)\\) where \\(Y_i\\) is an outcome variable for person \\(i\\)--the thing we want to predict--and \\(X_i\\) is a vector of \\(p\\) predictor variables--the things we'll use to make our prediction. In the kids dataset, our outcome is kid_score and our predictors are mom_hs, mom_age, and mom_iq. Our goal is to build a model of the form \\(X&#39;\\beta = \\sum_{j=1}^p \\beta_j X_{j}\\) that we can use to predict \\(Y\\) for a person who is not in our dataset. The constants \\(\\beta_j\\) are called coefficients and a model of this form is called a linear model because the \\(\\beta_j\\) enter linearly: they're not raised to any powers etc. Ordinary least squares (OLS) uses the observed data to find the coefficients \\(\\widehat{\\beta}\\) that solve the least squares problem \\[ \\underset{\\beta}{\\text{minimize}} \\sum_{i=1}^n (Y_i - X_i&#39;\\beta)^2. \\] In case you were wondering \"but wait, where's the intercept?\" I should point out that some people prefer to write \\((Y_i - \\beta_0 - X_i&#39; \\beta)\\) rather than \\((Y_i - X_i&#39;\\beta)\\). To allow an intercept using my notation, simply treat the first element of my \\(X_i\\) vector as a \\(1\\) and the first element of my \\(\\beta\\) vector as the intercept. 4.3 Linear Regression with lm() The R function lm(), short for linear model, solves the least squares problem. Its basic syntax is lm([formula], [dataframe]) where [formula] is an R formula--an object that describes the regression we want to run--and [dataframe] is the name of a data frame containing our \\(X\\) and \\(Y\\) observations, e.g. kids. R formulas can be a bit confusing when you first encounter them, so I'll explain the details in stages. For the moment, there are two symbols you need to learn: ~ and + The tilde symbol ~ is used to separate the \"left hand side\" and \"right hand side\" of a formula: the outcome goes on the left of the ~ and the predictors go on the right. For example, to regress kid_score on mom_iq we use the command lm(kid_score ~ mom_iq, kids) ## ## Call: ## lm(formula = kid_score ~ mom_iq, data = kids) ## ## Coefficients: ## (Intercept) mom_iq ## 25.80 0.61 This tells R: \"please solve the least squares problem to predict kid_score using mom_iq based on the data contained in kids.\" Notice that R includes an intercept in the regression automatically. This is a good default, because it seldom makes sense to run a regression without an intercept. When you want to run a regression with multiple right-hand side predictors, use the plus sign + to separate them. For example, to regress kid_score on mom_iq and mom_ageuse the command lm(kid_score ~ mom_iq + mom_age, kids) ## ## Call: ## lm(formula = kid_score ~ mom_iq + mom_age, data = kids) ## ## Coefficients: ## (Intercept) mom_iq mom_age ## 17.5962 0.6036 0.3881 Exercise What's the correct interpretation of the regression coefficients from the regression lm(kid_score ~ mom_iq, kids)? How about in lm(kid_score ~ mom_iq, kids)? Exercise Run a linear regression to predict mom_hs using kid_score and mom_iq. lm(mom_hs ~ kid_score + mom_iq, kids) ## ## Call: ## lm(formula = mom_hs ~ kid_score + mom_iq, data = kids) ## ## Coefficients: ## (Intercept) kid_score mom_iq ## -0.060135 0.002775 0.006050 4.4 Getting More from lm() If we simply run lm as above, R will display only the estimated regression coefficients and the command that we used to run the regression: Call. To get more information, we need to store the results of our regression using the assignment operator &lt;- for example: reg1 &lt;- lm(kid_score ~ mom_iq, kids) If you run the preceding line of code in the R console, it won't produce any output. But if you check your R environment after running it, you'll see a new List object: reg1. To see what's inside this list, we can use the command str: str(reg1) ## List of 12 ## $ coefficients : Named num [1:2] 25.8 0.61 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;mom_iq&quot; ## $ residuals : Named num [1:434] -34.68 17.69 -11.22 -3.46 32.63 ... ## ..- attr(*, &quot;names&quot;)= chr [1:434] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ effects : Named num [1:434] -1808.22 190.39 -8.77 -1.96 33.73 ... ## ..- attr(*, &quot;names&quot;)= chr [1:434] &quot;(Intercept)&quot; &quot;mom_iq&quot; &quot;&quot; &quot;&quot; ... ## $ rank : int 2 ## $ fitted.values: Named num [1:434] 99.7 80.3 96.2 86.5 82.4 ... ## ..- attr(*, &quot;names&quot;)= chr [1:434] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .... Don't panic: you don't need to know what all of these list elements are. The important thing to understand is that lm returns a list from which we can extract important information about the regression we have run. To extract the regression coefficient estimates, we use the function coefficients() or coef() for short coef(reg1) ## (Intercept) mom_iq ## 25.7997778 0.6099746 To extract the regression residuals, we use the function residuals() or resid() for short resid(reg1) ## 1 2 3 4 5 6 ## -34.67839049 17.69174662 -11.21717291 -3.46152907 32.62769741 6.38284487 ## 7 8 9 10 11 12 ## -41.52104074 3.86488149 26.41438662 11.20806784 11.17050590 -25.66178773 ## 13 14 15 16 17 18 ## 3.93517580 -17.40659730 14.87699469 10.74760539 6.40273692 5.13172163 ## 19 20 21 22 23 24 ## -2.44440289 15.87116678 9.04396998 11.90180808 14.47664178 0.24807074 ## 25 26 27 28 29 30 ## 13.66974891 -4.06297022 -14.03359486 -7.52559318 2.18354609 2.47533381 .... To extract the fitted values i.e. \\(\\hat{Y}_i \\equiv X_i&#39;\\hat{\\beta}\\), the predicted values of, we use fitted.values fitted.values(reg1) ## 1 2 3 4 5 6 7 8 ## 99.67839 80.30825 96.21717 86.46153 82.37230 91.61716 110.52104 102.13512 ## 9 10 11 12 13 14 15 16 ## 75.58561 83.79193 79.82949 83.66179 80.06482 95.40660 87.12301 99.25239 ## 17 18 19 20 21 22 23 24 ## 95.59726 93.86828 107.44440 85.12883 92.95603 103.09819 85.52336 86.75193 ## 25 26 27 28 29 30 31 32 ## 85.33025 100.06297 86.03359 85.52559 74.81645 95.52467 92.37138 87.90567 ## 33 34 35 36 37 38 39 40 ## 97.75549 92.06345 84.67978 82.44896 84.29520 91.07649 78.98774 80.30825 .... Exercise Plot a histogram of the residuals from reg1 using ggplot with a bin width of 5. Is there anything noteworthy about this plot? Calculate the residuals \"by hand\" by subtracting the fitted values from reg1 from the column kid_score in kids. Use the R function all.equal to check that this gives the same result as resid(). As long as you include an intercept in your regression, the residuals will sum to zero. Verify that this is true (up to machine precision!) of the residuals from reg1 By construction, the regression residuals are uncorrelated with any predictors included in the regression. Verify that this holds (up to machine precision!) for reg1. Solution There seems to be a bit of left skewness in the residuals. library(ggplot2) ggplot() + geom_histogram(aes(x = resid(reg1)), binwidth = 5) They give exactly the same result: all.equal(resid(reg1), kids$kid_score - fitted.values(reg1)) ## [1] TRUE Close enough! sum(resid(reg1)) ## [1] 1.056155e-12 cor(resid(reg1), kids$mom_iq) ## [1] 3.121525e-16 4.5 Summarizing The Ouput of lm() To view the \"usual\" summary of regression output, we use the summary() function: summary(reg1) ## ## Call: ## lm(formula = kid_score ~ mom_iq, data = kids) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56.753 -12.074 2.217 11.710 47.691 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.79978 5.91741 4.36 1.63e-05 *** ## mom_iq 0.60997 0.05852 10.42 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.27 on 432 degrees of freedom ## Multiple R-squared: 0.201, Adjusted R-squared: 0.1991 ## F-statistic: 108.6 on 1 and 432 DF, p-value: &lt; 2.2e-16 Among other things, summary shows us the coefficient estimates and associated standard errors for each regressor. It also displays the t-value (Estimate / SE) and associated p-value for a test of the null hypothesis \\(H_0\\colon \\beta = 0\\) versus \\(H_1\\colon \\beta \\neq 0\\). Farther down in the output, summary provides the residual standard error, the R-squared, and the F-statistic and associated p-value for a test of the null hypothesis that all regression coefficients except for the intercept are zero.17 Health warning: by default, lm() computes standard errors and p-values under the classical regression assumptions. In particular, unless you explicitly tell R to do otherwise, it will assume that the regression errors \\(\\varepsilon_i \\equiv Y_i - X_i&#39; \\beta\\) are homoskedastic, and iid. If you're not quite sure what this means, or if you're worried that I'm sweeping important details under the rug, fear not: we'll revisit this in a later lesson. For the moment, let me offer you the following mantra, paraphrasing the wisdom of my favorite professor from grad school: You can always run a [predictive] linear regression; it's inference that requires assumptions. Exercise Use the kids tibble to run a regression that uses kid_score and mom_hs to predict mom_iq. Store your results in an object called reg_reverse and then display a summary of the regression results. reg_reverse &lt;- lm(mom_iq ~ mom_hs + kid_score, kids) summary(reg_reverse) ## ## Call: ## lm(formula = mom_iq ~ mom_hs + kid_score, data = kids) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.046 -10.412 -1.762 8.839 42.714 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 68.86638 2.82454 24.381 &lt; 2e-16 *** ## mom_hs 6.82821 1.58450 4.309 2.03e-05 *** ## kid_score 0.29688 0.03189 9.309 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.16 on 431 degrees of freedom ## Multiple R-squared: 0.234, Adjusted R-squared: 0.2304 ## F-statistic: 65.82 on 2 and 431 DF, p-value: &lt; 2.2e-16 4.6 Using broom() to get what you need from lm() We saw above that lm() returns a list. It turns out that summary(), when applied to an lm() object, also returns a list: str(summary(reg1)) ## List of 11 ## $ call : language lm(formula = kid_score ~ mom_iq, data = kids) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language kid_score ~ mom_iq ## .. ..- attr(*, &quot;variables&quot;)= language list(kid_score, mom_iq) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;kid_score&quot; &quot;mom_iq&quot; ## .. .. .. ..$ : chr &quot;mom_iq&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;mom_iq&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 .... In principle, this gives us a way of extracting particular pieces of information from a table of regression output generated by summary(). For example, if you carefully examine the output of str(summary(reg1)) you'll find a named list element called r.squared. By accessing this element, you can pluck out the R-squared from summary(reg1) as follows: summary(reg1)$r.squared ## [1] 0.2009512 Similarly, you could extract F-statistics and associated degrees of freedom by accessing You could extract the information That wasn't so bad! But now suppose you wanted to extract the estimates, standard errors, and p-values from reg1. While it's possible to do this by poring over the output of str(summary(reg1)), there's a much easier way. The broom package provides some extremely useful functions for extracting regression output. Best of all, the same tools apply to models that we'll meet in later lessons. Use tidy() to create a tibble containing regression estimates, standard errors, t-statistics, and p-values e.g. library(broom) tidy(reg1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 25.8 5.92 4.36 1.63e- 5 ## 2 mom_iq 0.610 0.0585 10.4 7.66e-23 Use glance() to create a tibble that summarizes various measures of model fit: glance(reg1) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.201 0.199 18.3 109. 7.66e-23 1 -1876. 3757. 3769. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Finally, use augment() to create a tibble that merges the tibble you used to run your regression with the corresponding regression fitted values, residuals, etc. augment(reg1, kids) ## # A tibble: 434 × 10 ## kid_score mom_hs mom_iq mom_age .fitted .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 65 1 121. 27 99.7 -34.7 0.00688 18.2 0.0126 ## 2 98 1 89.4 25 80.3 17.7 0.00347 18.3 0.00164 ## 3 85 1 115. 27 96.2 -11.2 0.00475 18.3 0.000905 ## 4 83 1 99.4 25 86.5 -3.46 0.00231 18.3 0.0000416 ## 5 115 1 92.7 27 82.4 32.6 0.00284 18.2 0.00456 ## 6 98 0 108. 18 91.6 6.38 0.00295 18.3 0.000181 ## 7 69 1 139. 20 111. -41.5 0.0178 18.2 0.0478 ## 8 106 1 125. 23 102. 3.86 0.00879 18.3 0.000200 ## 9 102 1 81.6 24 75.6 26.4 0.00577 18.2 0.00611 ## 10 95 1 95.1 19 83.8 11.2 0.00255 18.3 0.000483 ## # … with 424 more rows, and 1 more variable: .std.resid &lt;dbl&gt; Notice that augment() uses a dot \".\" to begin the name of any column that it merges. This avoids potential clashes with columns you already have in your dataset. After all, you'd never start a column name with a dot would you? Exercise To complete these exercises, you may need to consult the help files for tidy.lm(), glance.lm(), and augment.lm() from the broom package. These will tell you how the functions tidy(), glance(), and augment() behave when you apply them to an lm() object. Use dplyr and tidy() to display the regression estimate, standard error, t-statistic, and p-value for the predictor kid_score in reg_reverse from above. reg_reverse %&gt;% tidy() %&gt;% filter(term == &#39;kid_score&#39;) ## # A tibble: 1 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 kid_score 0.297 0.0319 9.31 6.61e-19 Use ggplot() and augment() to make a scatterplot with the fitted values from reg_reverse on the horizontal axis and mom_iq on the vertical axis. Use geom_abline() to add a 45-degree line to your plot augment(reg_reverse, kids) %&gt;% ggplot(aes(x = .fitted, y = mom_iq)) + geom_point() + geom_abline(intercept = 0, slope = 1) Continuing from the preceding exercise, run a regression of mom_iq on the fitted values from reg_reverse and display the estimated regression coefficients. Compare the R-squared of this regression to that of reg_reverse. Explain your results. kids_augmented &lt;- augment(reg_reverse, kids) reg_y_vs_fitted &lt;- lm(mom_iq ~ .fitted, kids_augmented) tidy(reg_y_vs_fitted) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.96e-13 8.73 2.25e-14 1.00e+ 0 ## 2 .fitted 1.00e+ 0 0.0871 1.15e+ 1 7.85e-27 glance(reg_reverse) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.234 0.230 13.2 65.8 1.14e-25 2 -1733. 3474. 3490. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; glance(reg_y_vs_fitted) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.234 0.232 13.1 132. 7.85e-27 1 -1733. 3472. 3484. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; 4.7 Dummy Variables with lm() The column mom_hs in kids is a dummy variable, also known as a binary variable. It equals 1 if a child's mother graduated from college and 0 otherwise. For this reason, the coefficient on mom_hs in the following regression tells us the difference of mean test scores between kids whose mothers graduated from college and those whose mothers did not, while the intercept tells us the mean of kid_score for children whose mothers didn't graduate from high school: lm(kid_score ~ mom_hs, kids) ## ## Call: ## lm(formula = kid_score ~ mom_hs, data = kids) ## ## Coefficients: ## (Intercept) mom_hs ## 77.55 11.77 Although it's represented using the numerical values 0 and 1, mom_hs doesn't actually encode quantitative information. The numerical values are just shorthand for two different categories: mom_hs is a categorical variable. To keep from getting confused, it's good practice to make categorical variables obvious by storing them as character or factor data. Here I create a new column, mom_education, that stores the same information as mom_hs as a factor: kids &lt;- kids %&gt;% mutate(mom_education = if_else(mom_hs == 1, &#39;High School&#39;, &#39;No High School&#39;)) %&gt;% mutate(mom_education = factor(mom_education, levels = unique(mom_education))) The column mom_education is a factor, R's built-in representation of a categorical variable. So what happens if we include mom_education in our regression in place of mom_hs? lm(kid_score ~ mom_education, kids) ## ## Call: ## lm(formula = kid_score ~ mom_education, data = kids) ## ## Coefficients: ## (Intercept) mom_educationNo High School ## 89.32 -11.77 Wait a minute; now the estimate is negative! We can't run a regression that includes an intercept and a coefficient for each level of a dummy variable--this is the dummy variable trap!--so R has excluded one of them. Rather capriciously, lm() has chosen to treat High School as the omitted category. We can override this behavior by using fct_relevel() from the forcats package. The following code tells R that we want 'No High School' to be the first ordered factor level, the level that lm() treats as the omitted category by default: library(forcats) kids &lt;- kids %&gt;% mutate(mom_education = fct_relevel(mom_education, &#39;No High School&#39;)) lm(kid_score ~ mom_education, kids) ## ## Call: ## lm(formula = kid_score ~ mom_education, data = kids) ## ## Coefficients: ## (Intercept) mom_educationHigh School ## 77.55 11.77 Explain about categorical variables that take on more than two values! Exercise Create a categorical variable with three levels perhaps using mom_age? Then see what happens This is worth putting in bold: you never have to explicitly construct dummy variables in R. The lm() function will construct them for you. 4.8 Fun with R Formulas It's time to learn some more about R formulas. But before we do, you may ask \"why bother?\" It's true that you run just about any regression you need using nothing more complicated than + and ~ as introduced above. I know, because I did this for the better part of a decade! But a key goal of this book is showing you how to work smarter rather than harder, both to make your own life easier and help others replicate your work. If you ever plan to fit more than a handful of models with more than a handful of variables, it's worth your time to learn about formulas. You've already met the special symbols ~ and + explained in the following table. In the next few sub-sections, I'll walk you through the others: ., -, 1, :, *, ^, and I(). Symbol Purpose Example In Words ~ separate LHS and RHS of formula y ~ x regress y on x + add variable to a formula y ~ x + z regress y on x and z . denotes \"everything else\" y ~ . regress y on all other variables in a data frame - remove variable from a formula y ~ . - x regress y on all other variables except z 1 denotes intercept y ~ x - 1 regress y on x without an intercept : construct interaction term y ~ x + z + x:z regress y on x, z, and the product x times z * shorthand for levels plus interaction y ~ x * z regress y on x, z, and the product x times z ^ higher order interactions y ~ (x + z + w)^3 regress y on x, z, w, all two-way interactions, and the three-way interactions I() \"as-is\" - override special meanings of other symbols from this table y ~ x + I(x^2) regress y on x and x squared 4.8.1 \"Everything Else\" -- The Dot . Sometimes all you want to do is run a regression of one variable on everything else. If you have lots of predictors, typing out all of their names, each separated by a + sign, is painful and error-prone. Fortunately there's a shortcut: the dot . lm(kid_score ~ ., kids) ## ## Call: ## lm(formula = kid_score ~ ., data = kids) ## ## Coefficients: ## (Intercept) mom_hs mom_iq ## 20.9847 5.6472 0.5625 ## mom_age mom_educationHigh School ## 0.2248 NA This command tells R to regress kid_score on everything else in kids. We'll encounter the dot in many guises later in this lesson and elsewhere. Wherever you see it, replace it mentally with the word \"everything\" and you'll never be confused. The rest will be clear from context. 4.8.2 Removing Predictors with - Suppose you wanted to regress kid_score on mom_hs and mom_iq but not mom_age. One way to do this is by writing out the formula explicitly: lm(kid_score ~ mom_hs + mom_iq, kids) ## ## Call: ## lm(formula = kid_score ~ mom_hs + mom_iq, data = kids) ## ## Coefficients: ## (Intercept) mom_hs mom_iq ## 25.7315 5.9501 0.5639 In this example, it's not too difficult to specify the formula we want by typing it out in full. But what if we had dozens of predictors and only wanted to exclude one or two? This is where the minus sign - comes in handy. Think of + as saying \"add me to the regression\" and - as saying \"remove me from the regression.\" By combining - with . we can tell R to \"add everything, then remove mom_age\" as follows: lm(kid_score ~ . - mom_age, kids) ## ## Call: ## lm(formula = kid_score ~ . - mom_age, data = kids) ## ## Coefficients: ## (Intercept) mom_hs mom_iq ## 25.7315 5.9501 0.5639 ## mom_educationHigh School ## NA This use of - is very similar to what you've seen in the select() function from dplyr. And as in dplyr, we can use it to remove more than one variable. In this example, it's silly but just to show you that it works as expected: lm(kid_score ~ . - mom_age - mom_hs, kids) ## ## Call: ## lm(formula = kid_score ~ . - mom_age - mom_hs, data = kids) ## ## Coefficients: ## (Intercept) mom_iq mom_educationHigh School ## 25.7315 0.5639 5.9501 4.8.3 The Intercept: 1 It almost always makes sense to include an intercept when you run a linear regression. Without one, we're forced to predict that \\(Y\\) will be zero when \\(X\\) is zero. Because this is usually a bad idea, lm() includes an intercept by default: lm(kid_score ~ mom_iq, kids) ## ## Call: ## lm(formula = kid_score ~ mom_iq, data = kids) ## ## Coefficients: ## (Intercept) mom_iq ## 25.80 0.61 In some special cases, however, we may have a reason to run a regression without an intercept. R's formula syntax denotes the intercept by 1. Armed with this knowledge, we can remove it from our regression using - as introduced above: lm(kid_score ~ mom_iq - 1, kids) ## ## Call: ## lm(formula = kid_score ~ mom_iq - 1, data = kids) ## ## Coefficients: ## mom_iq ## 0.8623 Another situation in which we may wish to remove the intercept is when running a regression with a categorical variable. As mentioned above, we can't include an intercept and a coefficient for each value of a categorical variable in our regression: this is the dummy variable trap. We either have to drop one level of the categorical variable (the baseline or omitted category) or drop the intercept. Above we saw how to choose which category to omit. But another option is to drop the intercept. In the first regression, the intercept equals the mean of kid_score for the omitted category mom_education == \"No High School\" while the intercept gives the difference of means: lm(kid_score ~ mom_education, kids) ## ## Call: ## lm(formula = kid_score ~ mom_education, data = kids) ## ## Coefficients: ## (Intercept) mom_educationHigh School ## 77.55 11.77 In the second, we obtain the mean of kid_score for each group: lm(kid_score ~ mom_education - 1, kids) ## ## Call: ## lm(formula = kid_score ~ mom_education - 1, data = kids) ## ## Coefficients: ## mom_educationNo High School mom_educationHigh School ## 77.55 89.32 kids %&gt;% group_by(mom_education) %&gt;% summarize(mean(kid_score)) ## # A tibble: 2 × 2 ## mom_education `mean(kid_score)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 No High School 77.5 ## 2 High School 89.3 Exercise #4 What do you get if you run the regression lm(kid_score ~ 1, kids)? Explain. Solution This is a regression with only an intercept, so it calculates the sample mean of kid_score lm(kid_score ~ 1, kids) ## ## Call: ## lm(formula = kid_score ~ 1, data = kids) ## ## Coefficients: ## (Intercept) ## 86.8 kids %&gt;% summarize(mean(kid_score)) ## # A tibble: 1 × 1 ## `mean(kid_score)` ## &lt;dbl&gt; ## 1 86.8 If you're still not convinced, a quick calculation shows that \\(\\widehat{\\alpha} = \\bar{y}\\) minimizes \\(\\sum_{i=1}^n (y_i - \\alpha)^2\\) over \\(\\alpha\\), where \\(\\bar{y} \\equiv \\frac{1}{n}\\sum_{i=1}^n y_i\\). 4.8.4 Adding Interactions With :, *, and ^ Point out that * and ^ are really just helpful shorthand: you can do everything with : and +. There are many different ways to specify the same formula in R, but some are easier to type and some are easier to understand that others. 4.8.5 Transforming Outcomes and Predictors What if you wanted to regress the logarithm of kid_score on mom_age and mom_age^2? One way to do this is by creating a new data frame: new_kids &lt;- kids %&gt;% mutate(log_kid_score = log(kid_score), mom_age_sq = mom_age^2) lm(log_kid_score ~ mom_age + mom_age_sq, new_kids) ## ## Call: ## lm(formula = log_kid_score ~ mom_age + mom_age_sq, data = new_kids) ## ## Coefficients: ## (Intercept) mom_age mom_age_sq ## 4.4586761 -0.0109575 0.0004214 It worked! But that required an awful lot of typing. What's more, I had to clutter up my R environment with another data frame: new_kids. A more elegant approach uses R's formula syntax to do all the heavy lifting. First I'll show you the syntax and then I'll explain it: lm(log(kid_score) ~ mom_age + I(mom_age^2), kids) ## ## Call: ## lm(formula = log(kid_score) ~ mom_age + I(mom_age^2), data = kids) ## ## Coefficients: ## (Intercept) mom_age I(mom_age^2) ## 4.4586761 -0.0109575 0.0004214 The key point here is that we can use functions within an R function. When lm() encounters log(kid_score) ~ mom_age + I(mom_age^2) it looks at the data frame kids, and then parses the formula to construct all the variables that it needs to run the regession. There's no need for us to construct and store these in advance: R can do everything for us. The only awkward part is the function I(). What on earth is that doing in our formula? Formulas have their own special syntax: a + inside a formula doesn't denote addition and a . doesn't indicate a decimal point. To override the special meanings of these symbols, Every programming language has words and characters with special meanings. If you try to construct a variable called +TRUE^2, R won't let you: +, TRUE and ^ have special meanings in R. If, for whatever strange reason, you insist upon having a variable called +TRUE^2, you have to tell R that it shouldn't interpret these symbols in the usual name. You can do this by wrapping them in backticks, as follows: `+TRUE^2` &lt;- 6 `+TRUE^2` ## [1] 6 The role of I() inside an R formula is identical to that of back ticks in the creation of variable names 4.8.6 Formulas are objects Shorten this. Don't get into too much detail, but point them to various functions for computing on and updating formulas. Say why this might be helpful: you want to run a linear probability model and Probit using the same LHS and RHS variables. The first thing you need to know about formulas is that they're objects. This means we can store them, pass them into functions, and write code to modify them. Above we supplied kid_score ~ mom_iq as the first argument to lm(). Now let's try something different: we'll store this formula, give it a name, and then pass it to lm() little_formula &lt;- kid_score ~ mom_iq str(little_formula) ## Class &#39;formula&#39; language kid_score ~ mom_iq ## ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; lm(little_formula, kids) ## ## Call: ## lm(formula = little_formula, data = kids) ## ## Coefficients: ## (Intercept) mom_iq ## 25.80 0.61 So what? We got exactly the same results above with less typing! Before it becomes clear why storing a formula could come in handy, we need to learn some additional syntax. big_formula &lt;- kid_score ~ mom_iq + mom_hs + mom_age 4.9 Exercises Look for interaction between mom_hs and mom_iq, interpret results, make some plots If you're rusty on the F-test, this may help.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
