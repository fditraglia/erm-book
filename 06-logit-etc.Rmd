# Logistic Regression and Friends

This chapter is about regression models for *binary outcomes*, models in which our outcome of interest $Y$ takes on one of two mutually exclusive values: yes/no, smoker/non-smoker, default/repay, etc. By convention we code one of the two possibilities as a "success," assigning it the value `1`, and the other a "failure," assigning it the value `0`. Here's a simple and entertaining example. I recently asked my MPhil students at Oxford to play a game called "two truths and a lie."^[This example was devised by [Andrew Gelman](http://www.stat.columbia.edu/~gelman/research/published/truths_paper.pdf). For more in-class examples and experiments, I highly recommend [Teaching Statistics: A Bag of Tricks](http://www.stat.columbia.edu/~gelman/bag-of-tricks/).] The class divided into groups of four, and we then took it in turns to make three statements to the other members of our groups: two true statements, and one lie. The remaining three group members were tasked with determining which statement was a lie and assigning an overall "certainty score" ranging from zero (just guessing) to ten (completely certain). So how accurate are these subjective certainty scores? This question amounts to asking how the probability of *guessing correctly*, $Y = 1$, varies with the certainty score, $X$. 

Our task in this lesson will be propose, interpret, and estimate regression models for the probability that $Y= 1$ given $X$, where $X$ is a vector of one or more predictor variables. While $Y$ is binary, $X$ could be continuous, discrete, or a mix of the two. Models like this *can* be used for causal inference, but for now we will scrupulously avoid using any causal language. Our problem for the moment is simply to predict $Y$. A natural question is "why not just use linear regression?" Indeed this is one possibility, as we'll discuss below. The fundamental problem is that lines have the same slope *everywhere*: they keep going up or down forever at the same rate. Probabilities, on the other hand, are bounded between zero and one. If we want a regression model that is guaranteed to make predictions in this range, it *can't* be linear. 

Our main focus in this lesson will be *logistic regression*, far and away the most popular model for binary outcomes. Towards the end, we'll briefly discuss *probit regression.* Each of these is an example of an *index model,* a model of the form $P(Y=1|X) = G(X'\beta)$ where $G(\cdot)$ is known function. Roughly speaking, the idea is to stick our usual linear regression predictor $X'\beta$ inside a function $G(\cdot)$ that makes predictions on the right scale. For logistic regression $G(z) = \texttt{plogis}(z)$ the [standard logistic CDF](https://en.wikipedia.org/wiki/Logistic_distribution); for probit regression $G(z) = \texttt{pnorm}(z)$, the standard normal CDF. Setting $G(\cdot)$ equal to a CDF ensures that our predictions will indeed lie between zero and one!

Compared to the usual treatment of logistic regression in econometrics, we will devote *much more time* to understanding the meaning of the model and much less time to technical details. This is because I assume that your econometrics course likely took the *opposite approach*. If you're hankering for more technical details, or if you want to brush up on the basic details of logistic regression, you can view my [MPhil teaching materials here](https://www.economictricks.com/). 

## Understanding the Logistic Regression Model

From a purely probabilistic perspective, there are two equivalent ways of thinking about logistic regression:
$$
P(Y=1|X) = \texttt{plogis}(X'\beta) \quad \iff \quad \log \left[\frac{P(Y=1|X)}{P(Y=0|X)}\right] = X'\beta.
$$
The first of these expresses the *probability* that $Y=1$ given $X$ as a nonlinear function of data and parameters; the second expresses the *log odds* that $Y=1$ given $X$ as a linear function of data and parameters.^[When I write log in this book, unless I explicitly say otherwise this is the *natural log*, following R's usage: `log()` defaults to natural log unless you specify `base = [YOUR PREFERRED BASE HERE]`.] This means that we can either think of logit regression as an index model for $P(Y=1|X)$ or as a linear regression model for the *log odds*. Many people, even those with extensive training in probability, find odds to be a slightly strange concept. In fact odds, and especially log odds, are very natural.^[Pun intended!] If you'll permit a short digression, I'd like to take this opportunity to change your mind about odds. 

### Odds aren't so odd!

Forget about regression for the moment and consider some event $A$ with probability $p$ of occurring. Then we say that the **odds** of $A$ are $p/(1 - p)$. For example, if $p = 1/3$ then the event $A$ is equivalent to drawing a red ball from an urn that contains one red and two blue balls: the probability gives the *ratio of red balls to total balls*. The odds of $A$, on the other hand, equal $1/2$: odds give the *ratio of red balls to blue balls*. Viewed from this perspective, there's nothing inherently more intuitive about probabilities compared to odds. We're simply more familiar with probabilities. 

But perhaps you're still not convinced. Here's an example that you've surely seen before:

> One in a hundred women has breast cancer $(B)$. If you have breast cancer, there is a 95\% chance that you will test positive $(T)$; if you do not have breast cancer $(B^C)$, there is a 2\% chance that you will nonetheless test positive $(T)$. We know nothing about Alice other than the fact that she tested positive. How likely is it that she has breast cancer?

It's easy enough to solve this problem using Bayes' Theorem, as long as you have pen and paper handy: 
$$
\begin{aligned}
P(B | T) &= \frac{P(T|B)P(B)}{P(T)} = \frac{P(T|B)P(B)}{P(T|B)P(B) + P(T|B^C)P(B^C)}\\
&= \frac{0.95 \times 0.01}{0.95 \times 0.01 + 0.02 \times 0.99} \approx 0.32.
\end{aligned}
$$
But what if I asked you how the result would change if only one in a thousand women had breast cancer? What if I changed the [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) of the test from 95\% to 99\% or the [specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) from 98% to 95\%? If you're anything like me, you would struggle to do these calculations in your head. That's because $P(B|T)$ is a *highly non-linear* function of $P(B)$, $P(T|B)$, and $P(T|B^C)$.

In contrast, working with odds makes this problem a snap. The key point is that $P(B|T)$ and $P(B^C|T)$ have the same denominator, namely $P(T)$:
$$
P(B | T) = \frac{P(T|B)P(B)}{P(T)}, \quad
P(B^C | T) = \frac{P(T|B^C)P(B^C)}{P(T)}
$$
Notice that $P(T)$ was the "complicated" term in $P(B|T)$; the numerator was simple. Since the odds of $B$ given $T$ is defined as the ratio of $P(B|T)$ to $P(B^C|T)$, the denominator cancels and we're left with 
$$
\text{Odds}(B|T) \equiv \frac{P(B|T)}{P(B^C|T)} = \frac{P(T|B)}{P(T|B^C)} \times \frac{P(B)}{P(B^C)}.
$$
In other words, the *posterior odds* of $B$ equal the *likelihood ratio*, $P(T|B)/P(T|B^C)$, multiplied by the *prior odds* of $B$, $P(B)/P(B^C)$:
$$
\text{Posterior Odds} = \text{(Likelihood Ratio)} \times \text{(Prior Odds)}.
$$
Now we can easily solve the original problem in our head. The prior odds are 1/99 while the likelihood ratio is 95/2. Rounding these to 0.01 and 50 respectively, we find that the posterior odds are around 1/2. This mean's that Alice's chance of having breast cancer is roughly equivalent to the chance of drawing a red ball from an urn with one red and two blue balls. There's no need to convert this back to a probability since we can already answer the question: it's considerably more likely that Alice *does not* have breast cancer. But if you insist, odds of 1/2 give a probability of 1/3, so in spite of rounding and calculating in our heads we're within 0.3\% of the exact answer!

Repeat after me: **odds are on a multiplicative scale**. This is their key virtue and the reason why they make it so easy to explore variations on the original problem. If one in a thousand women has breast cancer, the prior odds become 1/999 so we simply divide our previous result by 10, giving posterior odds of around 1/20. If we instead changed the sensitivity from 95\% to 99\% and the specificity from 98\% to 95\%, then the likelihood ratio would change from $95/2 \approx 50$ to $99/5 \approx 20$. It's also easy to work out the effect of conditioning on additional sources of information. To compute the posterior odds after a *second* positive test result with the same sensitivity and specificity as the first and conducted independently of it, simply multiply by the likelihood ratio a *second time*. This is worth repeating: each positive test result has the effect of multiplying the prior odds by the *same factor*.  

Since odds are on a multiplicative scale, **log odds are on an additive scale** 
$$
\log(\text{Posterior Odds}) = \log\text{(Likelihood Ratio)} + \log\text{(Prior Odds)}.
$$
Base 2 logarithms are simplest for computers, natural logarithms are simplest for mathematics, and base 10 logarithms are simplest for human beings. So for a moment let's think in base 10. A good diagnostic test might have a sensitivity and specificity of around 90\%; an better one might have values of 99\%; an excellent one might have values of around 99.9\%. These translate into likelihood ratios of 9, 99, and 999 respectively. On a log base ten scale these are approximately 1, 2, and 3. A common disease might have a prevalence of 10\%; a rarer one 1\%; and an even rarer one 0.1\%. Expressed as prior odds, these values become  1/9, 1/99, and 1/999 or -1, -2, and -3 on the log base 10 scale. The log posterior odds are simply the *sum* of these two numbers. For example, a sensitivity and specificity of 10\% and a prevalence of 1\% gives log odds of approximately $1 - 2 = -1$. This means that the odds of having the disease are around $1/10$. A second positive result multiplies the odds by the likelihood ratio a second time. On the log scale this means adding 1, yielding log odds of 0 and odds of 1/2. This kind of reasoning is extremely helpful for quick, back-of-the-envelope calculations about how much to change our views in the light of new evidence. It's also *extremely* helpful for understanding logistic regression. But before returning to our regularly-scheduled programming, here are a few exercises to test your understanding.

### Exercise
1. I haven't given you a closed-form expression for `plogis()`. Use the log-odds representation of logit regression to work out an explicit formula, then create a function called `myplogis()` that implements it in R. Check that your function gives the same results as the "real" `plogis()` function on a grid of 400 equally spaced points between -4 and 4. Then plot your function on the same range.
`r hide('Show Solution')`
Let $p \equiv P(Y=1|X)$. We are given two expressions: $p = \texttt{plogis}(X'\beta)$ and $\log[p/(1-p)] = X'\beta$. Exponentiating both sides and re-arranging gives $\exp(-X'\beta) = (1-p)/p$. Solving for $p$, we obtain $p = 1/[1 + \exp(-X'\beta)]$. But since $p = \texttt{plogis}(X'\beta)$ this means that $\texttt{plogis}(z) = 1/[1 + \exp(-z)]$. Equivalently, we could multiply the numerator and denominator by $\exp(z)$ to write $\texttt{plogis}(z) = \exp(z)/[1 + \exp(z)]$.
```{r}
myplogis <- function(z) {
  1 / (1 + exp(-z))
}
z_seq <- seq(from = -4, to = 4, length.out = 400)
all.equal(myplogis(z_seq), plogis(z_seq))
library(ggplot2)
qplot(z_seq, myplogis(z_seq), geom = 'line')
```
`r unhide()`
2. Exercise: probabilities are between 0 and 1. What about odds? What about log odds? 
`r hide('Show Solution')`
Odds are between 0 and $+\infty$. Since $p\in [0,1]$ the ratio $p/(1 - p)$ cannot be negative, but we can make it arbitrarily large by taking $p$ close to one. Log odds are the log of something that is between zero and positive infinity, so they are between $-\infty$ and $+\infty$.
`r unhide()`
3. If the probability of an event is $1/2$, what are the odds? What about the log odds? What does this tell us about the qualitative interpretation of odds and log odds?
`r hide('Show Solution')`
A probability of 1/2 corresponds to odds of $(1/2) / (1 - 1/2) = 1$ and log odds of $\log(1) = 0$. Thus, odds of one mean "as likely as not," as do log odds of zero. 
`r unhide()`
4. Generalizing the Bayes rule example from above, let $\pi$ be the prior probability of breast cancer, $\text{sens}$ be the sensitivity of the test and $\text{spec}$ be the specificity of the test. Suppose that we observe the results of $n$ tests of which $n^+$ are positive the rest are negative. Write down an expression for the odds that Alice has breast cancer, assuming that each test result is statistically independent of the others conditional on a person's true breast cancer status. (Unconditionally, the tests *definitely* aren't independent!) 
`r hide('Show Solution')`
FILL IN LATER!
`r unhide()`

### Interpreting a Simple Logit Regression Model

To summarize our digression and exercises from above: probabilities are mathematically inconvenient because they're bounded above and below. This is the reason for the non-linearity in Bayes' Theorem expressed on the probability scale. In contrast, odds are simpler: they're *multiplicative*, bounded below by zero, and unbounded above. Odds of one mean $A$ and $A^C$ are equally likely. Log odds are even simpler: they're additive and unbounded. Log odds of zero mean that $A$ and $A^C$ are equally likely. Now it's time to use this knowledge to help us understand logistic regression. Our goal is to understand how changing $x$ affects $P(Y=1|X=x)$ under the model.

Let's start with the simplest possible example: $p(x) \equiv P(Y=1|X=x) = \texttt{plogis}(\alpha + \beta x)$ where $X$ is a scalar. What is the derivative of $p(x)$ with respect to $x$? By the chain rule,
$$
\frac{d}{dx} \texttt{plogis}(\alpha + \beta x) = \beta \times \texttt{dlogis}(\alpha + \beta x) 
$$
where $\texttt{dlogis}(\cdot)$ is the standard logistic density. Since $\texttt{dlogis}(\cdot)$ is a density it can't be negative, so the derivative has the same sign as $\beta$. Its magnitude, however, depends on the value of $x$ at which we evaluate it *and* on the value of $\alpha$. On the *probability scale*, partial effects for logit regression are *non-linear*. Because it's a CDF, $\texttt{plogis}(\cdot)$ eventually "flattens out." Increasing $x$ by one unit therefore cannot have the same effect on $P(Y=1|X=x)$ when the baseline value of $x$ as it does when the baseline value is high. 

As above, everything's easier with odds. As you'll show in the exercise below,
$$
\frac{\texttt{plogis}(z)}{1 - \texttt{plogis}(z)} = \exp(z) \implies
\text{Odds}(Y=1|X=x) = \exp(\alpha + \beta x)
$$
Therefore, increasing $x$ by one unit is equivalent to multiplying the *odds* that $Y=1$ by $\exp(\beta)$. This holds true *regardless* of the values of $\alpha$ or $x$. Log odds are even simpler since $\text{log Odds}(Y=1|x) = \alpha + \beta x$. Notice that we've observed the same phenomenon here as in our Bayes' rule example from above. This is partly down to the magic of odds, and partly due to the *specific structure of the logit model*. These same results wouldn't go through for probit regression or other index models: the function $\texttt{plogis}(\cdot)$ is *extremely* mathematically convenient.

### Exercise
1. Divide by four rule
2. Average partial effect versus partial effect at average
3. Something with $x$ having a mean of zero and thinking about different changes from there. Different values of $\alpha$ etc. Think about relating this to the prior in the Bayes' rule calculation.
4. Something with odds *ratios*.



## Simulating Data from a Logistic Regression

There are many different ways to simulate draws from a logistic regression. Here are two. One possibility is to simulate Bernoulli draws with probabilities determined by `plogis()` and $X_i$, namely
$$
Y_i|X_i \sim \text{indep. Bernoulli}(p_i), \quad p_i \equiv \texttt{plogis}(X_i'\beta).
$$
Another is to generate a *latent continuous variable* $y^*_i$ from a linear regression with logistic errors, and then transform the result into a binary observed outcome $y_i$, in particular
$$
 y_i^* = X_i'\beta + \epsilon_i, \quad
 y_i = \left\{ \begin{array}{cc}
 1 & \mbox{if } y^*_i > 0\\
 0 & \mbox{if } y^*_i \leq 0\\
 \end{array}\right., \quad
 \epsilon_i \sim \mbox{ iid Logistic}(0,1). 
$$
To give us some data to play with, I'll use the second approach. In an exercise below, you'll be asked to supply code for the first approach.

The R function `rlogis()` creates iid draws from the logistic distribution. If we only specify one argument, `rlogis()` assumes that this is the number of random draws that we wish to make, and sets the values of its *location* and *scale* parameters to 0 and 1, respectively. This is what we want, since these parameters correspond to the Logistic$(0,1)$ distribution that appears in the latent data formulation from above. Using `rlogis()`, we can simulate data from a logistic regression model as follows: 
```{r}
set.seed(1234)
n <- 500
alpha <- 0.5
beta <- 1
x <- rnorm(n, mean = 1.5, sd = 2)
ystar <- alpha + beta * x + rlogis(n)
y <- 1 * (ystar > 0)
mydat <- data.frame(x, y)
```
In this case I've chosen to draw `x` from a normal distribution with mean `1.5` and standard deviation `2`. Given the values of `alpha` and `beta` that I've chosen, this means that $Y=1$ is considerably more likely than not: 
```{r}
mean(y)
```


### Exercise
My simulation code from above employed the latent variable approach: rather than using the function `plogis()` I generated standard logistic draws using `rlogis()` to form an unobserved continuous outcome variable `ystar`. Write code that simulates from the *same* logistic regression model *without* generating any random logistic draws or creating a latent continuous variable `ystar`.
```{r, webex.hide = 'Show Solution'}
# These lines are identical to those from above so we get the same x-values:
set.seed(1234)
n <- 500
alpha <- 0.5
beta <- 1
x <- rnorm(n, mean = 1.5, sd = 2)
# Here's the only thing that changes:
y <- rbinom(n, size = 1, prob = plogis(alpha + beta * x))
mydat2 <- data.frame(x, y)
```


## Running a Logistic Regression in R
Now we're reading to use the simulated dataset `mydat` to carry out logistic regression. Perhaps this strikes you as a silly idea. We generated the data so we *know* the true values of $\beta_0$ and $\beta_1$.  Why bother carrying out logistic regression to *estimate* them?
There are two answers to this question. First, this is only an example: don't be so picky!
Second, it can be extremely valuable to work with simulated data to check whether our statistical methods are working correctly. If we *know* for sure that the data came from a logistic regression model, then our logistic regression estimates should be close to the truth. If they're not, then something is wrong with our computer code.

The R function `glm` can be used to carry out logistic regression. The name of this function is an acronym for *generalized linear model*. Generalized linear models (GLMs) are exactly what their name says, a *generalization* of linear regression. GLMs include logistic regression as a special case. To tell `glm` that we want to carry out a logistic regression, we need to specify `family = binomial(link = 'logit')`. Otherwise the syntax is practically identical to that of `lm`. We specify a *formula*, `y ~ x`, and indicate a dataframe in which R should look to find `y` and `x`:
```{r}
lreg <- glm(y ~ x, mydat, family = binomial(link = 'logit'))
summary(lreg)
```
Notice that the output of `summary` when applied to a `glm` object is a little different from what we've seen for `lm` objects. For now, let's focus on what's the same. We still obtain the estimates of each of the coefficients in our model, along with standard errors, test statistics, and p-values. We can use this information to carry out statistical inference exactly as we do with linear regression: R has already done all the hard work for us by calculating the standard errors. As you'll see in the following exercises, the functions from the `broom` and `modelsummary` packages that you learned in our lessons on linear regression work just as well with `glm` objects as they do with `lm` objects. 

### Exercise
1. Construct approximate 95\% confidence intervals for the parameters $\beta_0$ and $\beta_1$ based on the logistic regression output from above. Do your confidence intervals include the true parameter values that we used to simulate the data?
`r hide('Show Solution')`
The confidence interval for the regression intercept is approximately `r round(coef(lreg)[1], 2)` $\pm$ `r round(2 * sqrt(summary(lreg)$cov.scaled[1,1]), 2)` which includes the true value: $\alpha=$ `r alpha`. 
Similarly, the confidence interval for the regression slope is `r round(coef(lreg)[2], 2)` $\pm$  `r round(2 * sqrt(summary(lreg)$cov.scaled[2,2]), 2)` which includes the true value: $\beta =$ `r beta`.
`r unhide()`
2. Interpret the estimated slope coefficient from `lreg`. 
`r hide('Show Solution')`
Consider two people: Alice has a value of $x$ that is one unit higher than Bob's value of $x$. Our model predicts that the log odds of $Y = 1$ are 0.96 higher for Alice than for Bob. Equivalently, we predict that Alice's odds of $Y=1$ are a multiplicative factor of $\exp(0.96) \approx 2.6$ larger than Bob's odds. Whether this is a large or a small difference measured on the *probability scale* depends on Alice's specific value of $x$.
`r unhide()`
3. Try using `coef()` with `lreg`. What do you get? Does it work as expected? Now try the `broom` functions `tidy()` and `glance()` and long with the function `modelsummary()` from the `modelsummary` package. What do you get?
```{r, webex.hide = 'Show Solution'}
# They work as expected!
library(broom)
library(modelsummary)
tidy(lreg)
glance(lreg)
modelsummary(lreg)
```

4. As we discussed above, $\beta$ is *not* the partial derivative of $\texttt{plogis}(\alpha + \beta x)$ with respect to $x$. 
    (a) Use the "divide by 4" rule to calculate the *maximum* possible marginal effect of $x$ on the predicted probability that $Y = 1$ using the results of `lreg`.
    (b) Calculate the marginal effect of $x$ on the predicted probability that $Y= 1$ at evaluated at the sample mean value of $X$. 
    (c) Compare your answers to (a) and (b)
```{r, webex.hide = 'Show Solution'}
# Divide by 4 rule
alpha_hat <- coef(lreg)[1]
beta_hat <- coef(lreg)[2]
beta_hat / 4

# Marginal effect at average x
linear_predictor <- alpha_hat + beta_hat * mean(x)
beta_hat * exp(linear_predictor) / (1 + exp(linear_predictor))^2
```

## Predicted Probabilities for Logistic Regression 

As you saw in the preceding exercises, many of the functions that we've already learned to use with `lm` objects generalize immediately to `glm` objects. One place were we need to be a bit careful is when we want to make *predictions* based on a fitted generalized linear model. The `predict()` function works for `glm` objects, but there's a slight wrinkle. If we want to calculate the predicted *probability* that $Y_i = 1$, we need to specify the argument `type = 'response'`. For example, we can calculate the predicted probability that $y_i = 1$ given that $X_i = 0$ as follows:
```{r}
predict(lreg, newdata = data.frame(x = 0), type = 'response')
```
Similarly, we can calculate the predicted probability that $y_i = 1$ given that $X_i$ equals the *sample mean* of $X$ as follows:
```{r}
predict(lreg, newdata = data.frame(x = mean(x)), type = 'response')
```
If we don't specify anything for `newdata`, then predict will give us the predicted probabilities for the *observed* values of $X$, exactly 
```{r}
p_hat <- predict(lreg, type = 'response')
head(p_hat)
```


### Exercise
1. Use the results of `lreg` to calculate the predicted probability that $Y_i = 1$ when: (i) $X_i = 0$ and (ii) $X_i = \bar{X}$ *without using* `predict`. Check that your results match those calculated using `predict` above.
```{r, webex.hide = 'Show Solution'}
alpha_hat <- coef(lreg)[1]
beta_hat <- coef(lreg)[2]
plogis(alpha_hat)
plogis(alpha_hat + beta_hat * mean(x))
```
2. You may recall the function `augment()` from the `broom` package that we used in an earlier lesson on linear regression. Read the help file for `augment.glm()` from the `broom` package. This explains the behavior of `augment()` when applied to a `glm` object rather than an `lm` object. How can we use `augment()` to calculate the same predictions as `predict(lreg, type = 'response')` and append them to `mydat`?
```{r, webex.hide = 'Show Solution'}
augment(lreg, mydat, type.predict = 'response')
```

## Plotting a Logistic Regression
We can plot a logistic regression function using a method very similar to the one we used to plot a linear regression:
```{r}
library(ggplot2)
ggplot(mydat, aes(x, y)) +
  stat_smooth(method='glm', 
              method.args = list(family = "binomial"),
              formula = y ~ x) 
```

To add the datapoints, we just add `geom_point()`
```{r}
library(ggplot2)
ggplot(mydat, aes(x, y)) +
  stat_smooth(method='glm', method.args = list(family = "binomial"),
              formula = y ~ x) + 
  geom_point()
```

This doesn't look very nice!
That's because there are only *two* possible $y$-values meaning that the observations will overlap substantially.
A helpful way to distinguish them visually is to add a bit of random noise to the points so they no longer overlap.
This is called *jittering* and `ggplot2` will do it for us if we replace `geom_point()` with `geom_jitter()`
```{r}
library(ggplot2)
ggplot(mydat, aes(x, y)) +
  stat_smooth(method='glm', method.args = list(family = "binomial"),
              formula = y ~ x) + 
  geom_jitter()
```

That's a bit *too much* random noise in the $y$-dimension.
We can control the amount of jittering by specifying `width` and `height` arguments to `geom_jitter` as follows
```{r}
library(ggplot2)
ggplot(mydat, aes(x, y)) +
  stat_smooth(method='glm', method.args = list(family = "binomial"),
              formula = y ~ x) + 
  geom_jitter(width = 0.5, height = 0.1)
```

From this plot it is easy to tell that there are many more observations with $Y = 1$ than $Y = 0$, something that was not at all clear from the plot using `geom_point()`.

### Exercise
Data from the "two truths and a lie" experiment described at the beginning of these lesson are available from `two-truths-and-a-lie-2022-cleaned.csv` in the `data` directory of my website: `https://ditraglia.com/data/`. 

1. Read the dataset directly into R from the web, storing it in a tibble called `two_truths`.
```{r, webex.hide = 'Show Solution', message = FALSE}
library(tidyverse)
data_url <- 'https://ditraglia.com/data/two-truths-and-a-lie-2022-cleaned.csv'
two_truths <- read_csv(data_url)
```
2. Run a logistic regression that predicts `guessed_right` based on `certainty`. Make a nicely-formatted table of regression results using `modelsummary()` and comment briefly on your findings.
`r hide('Show Solution')`
The estimated coefficient for `certainty` is *negative*! This means that on average we predict more wrong guesses as students' subjective certainty in their guesses increases. But notice that our estimate is fairly noisy: the coefficient estimate is around -0.2 and the standard error is around 0.16. We haven't found any compelling evidence that the relationship runs in either direction. Maybe the 2023 Core ERM students will be more obliging!  
```{r}
two_truths_reg <- glm(guessed_right ~ certainty, family = binomial(link = 'logit'),
                      data = two_truths)
library(modelsummary)
modelsummary(two_truths_reg)
```
`r unhide()`
3. Use `ggplot()` to depict the regression from part 2, adding jittering to make the raw data clearly visible. You may need to adjust the `width` and `height` parameters of `geom_jitter()`. 
```{r, message = FALSE, webex.hide = 'Show Solution'}
two_truths %>%
  ggplot(aes(x = certainty, y = guessed_right)) +
  stat_smooth(method = 'glm', method.args = list(family = 'binomial'),
              formula = y ~ x) +
  geom_jitter(width = 0.1, height = 0.05)
```


## Binary Outcome: Example 15.1 from Wooldridge (2010)
Mroz (1987, Econometrica) 

```{r, message = FALSE}
# Load packages for robust standard errors (install them first!)
library(sandwich)
library(lmtest)

# Load the data from the wooldridge package (install it first!)
library(wooldridge)

# View the names of the columns
names(mroz)
```

### Description of Variables: `mroz`

```{r}
# Specify the linear index
labor_model <- inlf ~ nwifeinc + educ + exper + expersq + age +
  kidslt6 + kidsge6
```

- `inlf` equals 1 if in labor force, 1975
- `nwifeinc` non-wife income in $1000
- `educ` years of schooling
- `exper` actual labor market experience
- `expersq` square of `exper`
- `age` woman's age in years
- `kidslt6` number of kids < 6 years
- `kidsge6` number of kids 6-18

### Fit LPM, Logit, and Probit 
```{r}
lpm <- lm(labor_model, data = mroz)
logit <- glm(labor_model, family = binomial(link = 'logit'), 
             data = mroz)
probit <- glm(labor_model, family = binomial(link = 'probit'), 
              data = mroz)
```

### LPM Results - Robust SE
```{r}
coeftest(lpm, vcov. = vcovHC, type = 'HC0')
```

### Logistic Regression Results
```{r}
coeftest(logit)
```

### Probit Regression Results
```{r}
coeftest(probit)
```
### Calculating Pseudo-$R^2$ 
```{r}
# Fit models with only an intercept
model0 <- inlf ~ 1
logit0 <- glm(model0, family = binomial(link = 'logit'), data = mroz)
probit0 <- glm(model0, family = binomial(link = 'probit'), data = mroz)

# Pseudo R-squared for Logit
1 - logLik(logit) / logLik(logit0)

# Pseudo R-squared for Probit
1 - logLik(probit) / logLik(probit0)
```

### Calculating Average Partial Effects
```{r}
# Average of g(x'beta_hat) where g is dlogis
# (predict defaults to the scale of x'beta_hat)
logit_APE_factor <- mean(dlogis(predict(logit))) 
logit_APE_factor

# Average of g(x'beta_hat) where g is dnorm 
# (predict defaults to the scale of x'beta_hat)
probit_APE_factor <- mean(dnorm(predict(probit)))
probit_APE_factor 
```

### Comparison of APEs - LPM, Logit & Probit
```{r}
# Extract estimated coefficients, excluding the first (the constant)
lpm_est <- coefficients(lpm)[-1]
logit_est <- coefficients(logit)[-1]
probit_est <- coefficients(probit)[-1]

# Rescale the logit and probit estimates to obtain APEs
cbind(lpm = lpm_est, logit_APE = logit_APE_factor * logit_est, 
      probit_APE = probit_APE_factor * probit_est)
```

## Addendum: Odds and Ends about Risk
![](https://imgs.xkcd.com/comics/spacecraft_debris_odds_ratio.png)

Logistic regression is often used to talk about *risk*. For example: what is the risk that a baby born to a mother with characteristics $X$ will have a low birthweight? How does this risk vary across mothers with different characteristics? There are many different ways to pose this question, and it's important not to mix them up. Confusion on this score is common, so it's worth taking a moment to distinguish three concepts: **risk differences**, **relative risk**, and **odds ratios**. 

Consider two mothers: one with characteristics $x_1$ and another with characteristics $x_2$. If we take $x_1$ as our "baseline," then the **risk difference** is
$$
\begin{aligned}
\text{Risk Difference} &\equiv P(Y=1|X=x_2) - P(Y=1|X=x_1)\\
&= \texttt{plogis}(\alpha + x_2'\beta) - \texttt{plogis}(\alpha + x_1'\beta)
\end{aligned}
$$
This is simply the difference of probabilities: it tells us how much higher or lower the probability of having a low birthweight baby is for a mother with $X=x_2$ compared to $X=x_1$. In contrast, the **relative risk** is a ratio:
$$
\text{Relative Risk} \equiv \frac{P(Y=1|X=x_2)}{P(Y=1|X=x_1)} = \frac{\texttt{plogis}(\alpha + x_2'\beta)}{\texttt{plogis}(\alpha + x_1'\beta)}.
$$
This tells us how many *times* more or less likely a mother with $X = x_2$ is to have a low birthweight baby compared to a mother with $X = x_1$. The **odds ratio** is yet *another* ratio:
$$
\begin{aligned}
\text{Odds Ratio} &\equiv \frac{P(Y=1|X=x_2)/P(Y=0|X=x_2)}{P(Y=1|X=x_1)/P(Y=1|X=x_1)} = \frac{\exp(\alpha + x_2'\beta)}{\exp(\alpha + x_1'\beta)}\\
&= \exp\left\{ (x_2 - x_1)'\beta\right\}.
\end{aligned}
$$
Each of these measures is a perfectly reasonable answer to the question "how does the risk vary across the two mothers?" But each is also on a completely different scale: in general they are *not directly comparable*. Before asking if a number is big or small, we first have to be clear about what is being measured.

People often confuse odds ratios with relative risk. This is tempting for three reasons. First, they're both ratios. Second, when the event $\{Y=1\}$ is *extremely rare*, the two measures give similar results.^[In epidemiology this is called the "rare disease assumption."] Third, when the two measures are not comparable, the odds ratio is often quite a lot larger, and therefore sounds more impressive! Consider two examples. In the first, $P(Y=1|X=x_2) = 0.1$ while $P(Y=1|X=x_1)= 0.05$, so the relative risk is $2$ and the odds ratio is $2.11$. In the second, $P(Y=1|X=x_2)= 0.8$ and $P(Y=1|X=x_1)=0.4$ so the relative risk remains $2$ while the odds ratio shoots up to $6$. Again, there's nothing wrong with odds ratios: they're simply not equivalent to relative risk. In the second example it would be a gross exaggeration to claim that "setting $X = x_2$ makes $Y=1$ six times more likely!" It is the *odds* that increase by a factor of six, not the probability.

Even in cases where the odds ratio and relative risk are similar, there's still room for confusion. Suppose I tell you that going from $X=x_2$ to $X = x_1$ increases your mortality risk: the relative risk and odds ratio are both around 2. Should this worry you? It depends. Increasing your probability of death from 0.000005\% to 0.00001\% gives a relative risk and odds ratio of 2, but pales in comparison to the danger of driving a car. When the baseline probability is low, the relative risk and odds ratio can be high despite the risks involved being negligible.

### Exercise
Explain the cartoon at the beginning of this section.
