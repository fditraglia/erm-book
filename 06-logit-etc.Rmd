# Logistic Regression and Friends

This chapter is about regression models for *binary outcomes*, models in which our outcome of interest $Y$ takes on one of two mutually exclusive values: yes/no, smoker/non-smoker, default/repay, etc. By convention we code one of the two possibilities as a "success," assigning it the value `1`, and the other a "failure," assigning it the value `0`. Here's a simple and entertaining example. I recently asked my MPhil students at Oxford to play a game called "two truths and a lie."^[This example was devised by [Andrew Gelman](http://www.stat.columbia.edu/~gelman/research/published/truths_paper.pdf). For more in-class examples and experiments, I highly recommend [Teaching Statistics: A Bag of Tricks](http://www.stat.columbia.edu/~gelman/bag-of-tricks/).] The class divided into groups of four, and we then took it in turns to make three statements to the other members of our groups: two true statements, and one lie. The remaining three group members were tasked with determining which statement was a lie and assigning an overall "certainty score" ranging from zero (just guessing) to ten (completely certain). So how accurate are these subjective certainty scores? This question amounts to asking how the probability of *guessing correctly*, $Y = 1$, varies with the certainty score, $X$. 

Our task in this lesson will be propose, interpret, and estimate regression models for the probability that $Y= 1$ given $X$, where $X$ is a vector of one or more predictor variables. While $Y$ is binary, $X$ could be continuous, discrete, or a mix of the two. Models like this *can* be used for causal inference, but for now we will scrupulously avoid using any causal language. Our problem for the moment is simply to predict $Y$. A natural question is "why not just use linear regression?" Indeed this is one possibility, as we'll discuss below. The fundamental problem is that lines have the same slope *everywhere*: they keep going up or down forever at the same rate. Probabilities, on the other hand, are bounded between zero and one. If we want a regression model that is guaranteed to make predictions in this range, it *can't* be linear. 

Our main focus in this lesson will be *logistic regression*, far and away the most popular model for binary outcomes. Towards the end, we'll briefly discuss *probit regression.* Each of these is an example of an *index model,* a model of the form $P(Y=1|X) = G(X'\beta)$ where $G(\cdot)$ is known function. Roughly speaking, the idea is to stick our usual linear regression predictor $X'\beta$ inside a function $G(\cdot)$ that makes predictions on the right scale. For logistic regression $G(z) = \texttt{plogis}(z)$ the [standard logistic CDF](https://en.wikipedia.org/wiki/Logistic_distribution); for probit regression $G(z) = \texttt{pnorm}(z)$, the standard normal CDF. Setting $G(\cdot)$ equal to a CDF ensures that our predictions will indeed lie between zero and one!

Compared to the usual treatment of logistic regression in econometrics, we will devote *much more time* to understanding the meaning of the model and much less time to technical details. This is because I assume that your econometrics course likely took the *opposite approach*. If you're hankering for more technical details, you can view my [MPhil teaching materials here](https://www.economictricks.com/). 

## Understanding the Logistic Regression Model

From a purely probabilistic perspective, there are two equivalent ways of thinking about logistic regression:
$$
P(Y=1|X) = \texttt{plogis}(X'\beta) \quad \iff \quad \log \left[\frac{P(Y=1|X)}{1 - P(Y=1|X)}\right] = X'\beta.
$$
The first of these expresses the *probability* that $Y=1$ given $X$ as a nonlinear function of data and parameters; the second expresses the *log odds* that $Y=1$ given $X$ as a linear function of data and parameters.^[When I write log in this book, unless I explicitly say otherwise this is the *natural log*, following R's usage: `log()` defaults to natural log unless you specify `base = [YOUR PREFERRED BASE HERE]`.] This means that we can either think of logit regression as an index model for $P(Y=1|X)$ or as a linear regression model for the *log odds*. 

So what on earth are odds and why should we care? Many people, even those with extensive training in probability, find odds to be a slightly strange concept. In fact odds, and especially log odds, are very natural.^[Pun intended!] Forget about regression for the moment and consider some event $A$ with probability $p$ of occurring. Then we say that the *odds* of $A$ are $p/(1 - p)$. For example, if $p = 1/3$ then the event $A$ is equivalent to drawing a red ball from an urn that contains one red and two blue balls: the probability is the *ratio of red balls to total balls*. The odds of $A$, on the other hand, are $(1/3) / (2/3) = 1/2$: the odds is the *ratio of red balls to blue balls*. Viewed from this perspective, there's nothing inherently more intuitive about probabilities compared to odds; we're just more used to them. 

But perhaps you're still not convinced. Here's an example that you've surely seen before:
> One in a hundred women has breast cancer ($B$). If you have breast cancer, there is a 99\% chance that your test result will be positive ($T$); if you do not have breast cancer, there is a 5\% chance that your test result will nevertheless be positive ($T$). We know nothing about Alice other than the fact that she tested positive. How likely is it that she has breast cancer?

The usual approach to this problem is simple enough with pen and paper:
$$
\begin{aligned}
P(B | T) &= \frac{P(T|B)P(B)}{P(T)} = \frac{P(T|B)P(B)}{P(T|B)P(B) + P(T|B^C)P(B^C)}\\
&= \frac{0.99 \times 0.01}{0.99 \times 0.01 + 0.05 \times 0.99} = 1/6
\end{aligned}
$$

**Maybe it's worth putting in a little Bayes' rule example here? Expressed in terms of odds? Make the point about multiplicative versus additive.**

### Exercise
1. I haven't given you a closed-form expression for `plogis()`. Use the log-odds representation of logit regression to work out an explicit formula. Make your own version of this function `myplogis()` and check that it gives the same results on a grid of 100 equally spaced points `z` from `-4` to `4`.
2. Exercise: probabilities are between 0 and 1. What about odds? What about log odds? Probability of 1/2 gives odds of what? Log odds of what?


The log odds of $A$ are simply the natural logarithm of this quantity: $\log(p) - \log(1 - p)$.  


- Probabilities are nasty because they're bounded on both sides; odds are simpler because they're *multiplicative* and only bounded below; log odds are *even simpler* because they're *additive* and unbounded. Example with `plogis(a + b* x)`. What is the derivative wrt `x`? It's a bit of a pain. What about derivative of the *odds* wrt `x`? Much simpler! What about derivative of the *log odds*? Even simpler still! Need to be able to think about moving back-and-forth between these scales.
- Example: derive the *divide by four* rule.
- Also can think about average partial effects.



## Odds and Ends about Risk
![](https://imgs.xkcd.com/comics/spacecraft_debris_odds_ratio.png)



## The Latent Data Formulation of Logistic Regression
Another way to think about logistic regression is via the following *generative model*:
\[
 y_i^* = X_i'\beta + \epsilon_i, \quad
 y_i = \left\{ \begin{array}{cc}
 1 & \mbox{if } y^*_i > 0\\
 0 & \mbox{if } y^*_i \leq 0\\
 \end{array}\right., \quad
 \epsilon_i \sim \mbox{ iid Logistic}(0,1) 
\]
where the Logistic$(0,1)$ distribution has CDF $\Lambda(z) = e^z/(1 + e^z)$ and pdf $\lambda(z) = e^z/(1 + e^z)^2$.
The expressions $\Lambda$ and $\lambda$ should look familiar, since we worked with them above.
We call this a generative model because it tells us how to *generate* the observations $y_i$ using the regressors $X_i$.
If we want to *simulate* data from a logistic regression model, the latent data formulation gives us a convenient way to do so.

The idea behind the latent data formulation is that a continuous *unobserved* random variable $y_i^*$ determines whether the *observed* binary random variable $y_i$ is zero or one.
The term *latent* is just a synonym for unobserved.
While this may seem odd, in specific examples we can often give $y_i^*$ a meaningful interpretation.
For example, suppose that $y_i=1$ if person $i$ voted for Hilary Clinton in the 2016 presidential election and $X_i$ contains demographic information, e.g.\ income, education, race, sex, and age.
The latent variable $y_i^*$ can be viewed as a measure of person $i$'s *strength of preference* for Hilary Clinton relative to Donald Trump.
If $y^*_i$ is large and positive, person $i$ strongly prefers Clinton; if $y_i^*$ is large and negative, person $i$ strongly prefers Trump; if $y_i^* = 0$, person $i$ is indifferent.

### Exercise \#3
(a) Show that $\lambda(z)$ is symmetric about zero, i.e. $\lambda(z) = \lambda(-z)$.
(b) Show that the latent data formulation implies $\mathbb{P}(y_i = 1) = \Lambda(X_i'\beta)$. Hint: if $Z$ is a continuous RV with a pdf that is symmetric about zero, then $\mathbb{P}(-Z<c) = \mathbb{P}(Z\leq c)$.

## Solution to Exercise \#3
<!-- ANS_START -->
(a) Expand the denominator, and then multiply by $\exp(-2z)/\exp(-2z)$, yielding
\[
\lambda(x) = \frac{\exp(z)}{[1 + \exp(z)]^2} = \frac{\exp(z)}{1 + 2\exp(z) + \exp(2z)} = \frac{\exp(-z)}{\exp(-2z) + 2\exp(-z) + 1} = \frac{\exp(-z)}{[1 + \exp(-z)]^2} = \lambda(-z)
\]
(b) $\mathbb{P}(y_i = 1) = \mathbb{P}(y_i^* > 0) = \mathbb{P}(X_i'\beta + \epsilon_i > 0) = \mathbb{P}(-\epsilon_i < X_i'\beta) = \mathbb{P}(-\epsilon_i \leq X_i'\beta) = \Lambda(X_i'\beta)$
<!-- ANS_END -->


## Part II - Logistic Regression in R
Now we'll take a quick look at how to carry out logistic regression in R using a simulated dataset.
In Thursday's lab you'll use what you learn in this part to study a real-world example.

## Simulating Data from a Logistic Regression
The R function `rlogis` creates iid draws from the logistic distribution.
If we only specify one argument, `rlogis` assumes that this is the number of random draws that we wish to make, and sets the values of its *location* and *scale* parameters to 0 and 1, respectively.
This is what we want, since these parameters correspond to the Logistic$(0,1)$ distribution that appears in the latent data formulation from above.
Using `rlogis`, we can simulate data from a logistic regression model as follows: 
```{r}
set.seed(1234)
n <- 500
b0 <- 0.5
b1 <- 1
x <- rnorm(n, mean = 1.5, sd = 2)
ystar <- b0 + b1 * x + rlogis(n)
y <- 1 * (ystar > 0)
mydat <- data.frame(x, y)
```

### Running a Logistic Regression in R
We can now run a logistic regression use the simulated dataset `mydat` to carry out logistic regression.
Note that in a certain sense this is silly: we generated the data so we *know* the true values of $\beta_0$ and $\beta_1$. 
Why bother carrying out logistic regression to *estimate* them?
There are two answers to this question.
First, this is only an example: don't be so picky!
Second, it can be extremely valuable to work with simulated data to check whether our statistical methods are working correctly.
If we *know* for sure that the data came from a logistic regression model, then our logistic regression estimates should be close to the truth. 
If they're not, then something is wrong with our computer code.

The R function `glm` can be used to carry out logistic regression.
The name of this function is an acronym for *generalized linear model*.
Generalized linear models (GLMs) are exactly what their name says, a *generalization* of linear regression.
GLMs include logistic regression as a special case.
To tell `glm` that we want to carry out a logistic regression, we need to specify `family = binomial(link = 'logit')`.
Otherwise the syntax is practically identical to that of `lm`.
We specify a *formula*, `y ~ x`, and indicate a dataframe in which R should look to find `y` and `x`:
```{r}
lreg <- glm(y ~ x, mydat, family = binomial(link = 'logit'))
summary(lreg)
```
Notice that the output of `summary` when applied to a `glm` object is a little different from what we've seen for `lm` objects.
But let's focus on what's the same.
We still obtain the estimates of each of the coefficients in our model, along with standard errors, test statistics, and p-values.
We can use this information to carry out statistical inference exactly as we do with linear regression: R has already done all the hard work for us by calculating the standard errors.

### Exercise \#4
Construct approximate 95\% confidence intervals for the parameters $\beta_0$ and $\beta_1$ based on the logistic regression output from above.
Do your confidence intervals include the true parameter values that we used to simulate the data?

## Solution to Exercise \#4
<!-- ANS_START -->
The confidence interval for the regression intercept is `r round(coef(lreg)[1], 2)` $\pm$ `r round(2 * sqrt(summary(lreg)$cov.scaled[1,1]), 2)` which includes the true value: $\beta_0=$ `r b0`. 
Similarly, the confidence interval for the regression slope is `r round(coef(lreg)[2], 2)` $\pm$  `r round(2 * sqrt(summary(lreg)$cov.scaled[2,2]), 2)` which includes the true value: $\beta_1 =$ `r b1`.
<!-- ANS_END -->

## Predicted Probabilities for Logistic Regression 
Many of the functions we used with `lm` also work with `glm`.
For example, to extract the coefficients from a generalized linear model, we can use the command `coef`:
```{r}
coef(lreg)
```
We can also use the function `predict` to calculated the predicted probability that $y = 1$ given particular values of the predictors $X_i$. 
There's just one slight wrinkle here: we need to make sure to specify `type = 'response'` to indicate to R that we want the predicted *probabilities*.
For example, we can calculate the predicted probability that $y_i = 1$ given that $X_i = 0$ as follows:
```{r}
predict(lreg, newdata = data.frame(x = 0), type = 'response')
```
Similarly, we can calculate the predicted probability that $y_i = 1$ given that $X_i$ equals the *sample mean* of $X$ as follows:
```{r}
predict(lreg, newdata = data.frame(x = mean(x)), type = 'response')
```
If we don't specify anything for `newdata`, then predict will give us the predicted probabilities for the *observed* values of $X$:
```{r}
phat <- predict(lreg, type = 'response')
head(phat)
```

### Exercise \#5
(a) Write an R function called `Lambda` that calculates the value of $e^z/(1 + e^z)$.
(b) Using your function from part (a) and the results of `lreg`, calculate the predicted probability that $y_i = 1$ when: (i) $X_i = 0$ and (ii) $X_i = \bar{X}$ *without using* `predict`. Check that your results match those calculated using `predict` above.

### Solution to Exercise \#5

<!-- ANS_START -->
```{r}
Lambda <- function(x) {
  1 / (1 + exp(-x))
}
bhat_0 <- coef(lreg)[1]
bhat_1 <- coef(lreg)[2]
Lambda(bhat_0)
Lambda(bhat_0 + bhat_1 * mean(x))
```
<!-- ANS_END -->

## Calculating Marginal Effects
As we discussed above, $\beta_j$ is *not* the partial derivative of $P(X)$ with respect to $X_j$.
But since we have a formula for this partial derivative, we can calculate it for any value of $X$.
In the following exercise, you will compare the exact calculation to the approximation given by the "divide by 4" rule.

### Exercise \#6
(a) Use the "divide by 4" rule to calculate the *maximum* possible effect of $X$ on the predicted probability that $y_i = 1$ using the results of `lreg`.
(b) Calculate the effect of $X$ on the predicted probability that $y_i= 1$ at $X_i = \bar{X}$.
(c) Compare your answers to (a) and (b)

### Solution to Exercise \#6
<!-- ANS_START -->
```{r}
# Divide by 4 rule
bhat_1 / 4

# Marginal effect at average x
linear_predictor <- bhat_0 + bhat_1 * mean(x)
bhat_1 * exp(linear_predictor) / (1 + exp(linear_predictor))^2
```
<!-- ANS_END -->

### Plotting a Logistic Regression
We can plot a logistic regression function using a method very similar to the one we used to plot a linear regression:
```{r}
library(ggplot2)
ggplot(mydat, aes(x, y)) +
  stat_smooth(method='glm', 
              method.args = list(family = "binomial"),
              formula = y ~ x) 
```

To add the datapoints, we just add `geom_point()`
```{r}
library(ggplot2)
ggplot(mydat, aes(x, y)) +
  stat_smooth(method='glm', method.args = list(family = "binomial"),
              formula = y ~ x) + 
  geom_point()
```

This doesn't look very nice!
That's because there are only *two* possible $y$-values meaning that the observations will overlap substantially.
A helpful way to distinguish them visually is to add a bit of random noise to the points so they no longer overlap.
This is called *jittering* and `ggplot2` will do it for us if we replace `geom_point()` with `geom_jitter()`
```{r}
library(ggplot2)
ggplot(mydat, aes(x, y)) +
  stat_smooth(method='glm', method.args = list(family = "binomial"),
              formula = y ~ x) + 
  geom_jitter()
```

That's a bit *too much* random noise in the $y$-dimension.
We can control the amount of jittering by specifying `width` and `height` arguments to `geom_jitter` as follows
```{r}
library(ggplot2)
ggplot(mydat, aes(x, y)) +
  stat_smooth(method='glm', method.args = list(family = "binomial"),
              formula = y ~ x) + 
  geom_jitter(width = 0.5, height = 0.1)
```

From this plot it is easy to tell that there are many more observations with $y = 1$ than $y = 0$, something that was not at all clear from the plot using `geom_point()`.


## Alternatives
